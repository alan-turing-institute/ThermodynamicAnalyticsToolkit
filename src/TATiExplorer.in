#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import argparse
from collections import deque
import logging
import numpy as np
import tensorflow as tf
import time
import scipy.spatial.distance as scidist

from TATi.common import add_common_options_to_parser, add_data_options_to_parser, \
        add_model_options_to_parser, add_prior_options_to_parser, add_sampler_options_to_parser, \
        get_filename_from_fullpath, get_trajectory_header,\
        react_to_common_options, react_to_sampler_options, \
        setup_csv_file, str2bool
from TATi.exploration.explorer import Explorer
from TATi.exploration.trajectorydatacontainer import TrajectoryDataContainer
from TATi.exploration.trajectoryjobid import TrajectoryJobId
from TATi.models.model import model
from TATi.runtime.runtime import runtime
from TATi.TrajectoryAnalyser import compute_diffusion_maps

from multiprocessing.managers import BaseManager

class MyManager(BaseManager):
    pass

MyManager.register('network_model', model)
MyManager.register('list', list)
MyManager.register('TrajectoryDataContainer', TrajectoryDataContainer)
MyManager.register('TrajectoryJobId', TrajectoryJobId)

FLAGS = None

def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    parser = argparse.ArgumentParser()

    add_common_options_to_parser(parser)
    add_data_options_to_parser(parser)
    add_model_options_to_parser(parser)
    add_prior_options_to_parser(parser)
    add_sampler_options_to_parser(parser)

    # please adhere to alphabetical ordering
    parser.add_argument('--cornerpoints_file', type=str, default=None,
        help='Filename to write found corner points to')
    parser.add_argument('--diffusion_map_method', type=str, default='vanilla',
        help='Method to use for computing the diffusion map: pydiffmap, vanilla or TMDMap')
    parser.add_argument('--max_exploration_steps', type=int, default=2,
        help='Maximum number of exploration steps')
    parser.add_argument('--max_legs', type=int, default=100,
        help='Maximum number of legs per trajectory')
    parser.add_argument('--minima_file', type=str, default=None,
        help='Filename to write found minima to')
    parser.add_argument('--number_of_eigenvalues', type=int, default=4,
        help='How many largest eigenvalues to compute')
    parser.add_argument('--number_processes', type=int, default=0,
        help='Whether to activate parallel mode (unequal 0) and how many processes to use then')
    parser.add_argument('--number_of_parallel_trajectories', type=int, default=5,
        help='Number of trajectories to run in parallel, i.e. number of points maximally apart in diffusion distance')
    parser.add_argument('--number_pruning', type=int, default=0,
        help='Number of pruning stages through metropolis criterion after end of trajectory')
    parser.add_argument('--optimizer', type=str, default="GradientDescent",
        help='Choose the optimizer to use for sampling: GradientDescent')
    parser.add_argument('--use_reweighting', type=str2bool, default=False,
        help='Use reweighting of the kernel matrix of diffusion maps by the target distribution.')

    return parser.parse_known_args()


def main(_):
    rt = runtime(FLAGS)
    time_zero = time.process_time()

    # setup neural network
    if FLAGS.number_processes == 0:
        network_model = model(FLAGS)
        manager=None
    else:
        manager = MyManager()
        manager.start()
        network_model = manager.network_model(FLAGS)

    time_init_network_zero = time.process_time()
    # prepare for both sampling and training
    network_model.init_network(FLAGS.restore_model, setup="sample")
    network_model.init_network(FLAGS.restore_model, setup="train")
    network_model.init_input_pipeline()
    rt.set_init_network_time(time.process_time() - time_init_network_zero)

    # prevent writing of output files during leg sampling
    network_model.set_config_map("do_write_averages_file", False)
    network_model.set_config_map("do_write_run_file", False)
    network_model.set_config_map("do_write_trajectory_file", False)

    # 1. run initially just one trajectory
    explorer = Explorer(parameters=FLAGS,
                        max_legs=FLAGS.max_legs,
                        use_processes=FLAGS.number_processes,
                        number_pruning=FLAGS.number_pruning,
                        manager=manager)

    # add a uses_ids list
    if FLAGS.number_processes == 0:
        used_ids = []
    else:
        used_ids = manager.list()
    explorer.add_used_data_ids_list(used_ids)

    # launch worker processes initially
    if FLAGS.number_processes > 0:
        explorer.queue.start_processes(network_model, FLAGS)

    print("Creating starting trajectory.")
    # a. add three legs to queue
    explorer.spawn_starting_trajectory(network_model, FLAGS.number_of_parallel_trajectories)

    print("Starting multiple explorations from starting trajectory.")
    # 2. with the initial trajectory done and analyzed,
    #    find maximally separate points and sample from these
    cornerpoints = []
    exploration_step = 0
    while exploration_step < FLAGS.max_exploration_steps:
        explorer.run_all_jobs(network_model, FLAGS)

        # a. combine all trajectories
        steps, parameters, losses = explorer.combine_sampled_trajectories()

        # b. perform diffusion map analysis for eigenvectors
        idx_corner = explorer.get_corner_points(parameters, losses, FLAGS, FLAGS.number_of_parallel_trajectories)

        # d. spawn new trajectories from these points
        cornerpoints.append( explorer.spawn_corner_trajectories(steps, parameters, losses, idx_corner, network_model) )

        exploration_step += 1

    rt.set_train_network_time(time.process_time() - rt.time_init_network)

    # stop worker processes finally
    if FLAGS.number_processes > 0:
        explorer.queue.stop_processes()

    # 3. write final set of values
    data_container = explorer.queue.get_data_container()

    # write away averages time information
    if FLAGS.averages_file is not None:
        averages_writer, averages_file = setup_csv_file(FLAGS.averages_file, network_model.get_averages_header(setup="sample"))
        for current_id in data_container.get_ids():
            data_object = data_container.get_data(current_id)
            if data_object.type == "sample":
                averages_lines_per_leg = data_object.averages_lines
                for leg_nr in range(len(averages_lines_per_leg)):
                    averages_lines = averages_lines_per_leg[leg_nr]
                    for row in range(len(averages_lines.index)):
                        averages_line = averages_lines.iloc[row,:]
                        averages_line[0] = current_id
                        averages_writer.writerow(averages_line)
        averages_file.close()


    # write away run info
    if FLAGS.run_file is not None:
        run_header = network_model.get_sample_header()
        run_writer, run_file = setup_csv_file(FLAGS.run_file, run_header)
        for current_id in data_container.get_ids():
            data_object = data_container.get_data(current_id)
            if data_object.type == "sample":
                run_lines_per_leg = data_object.run_lines
                for leg_nr in range(len(run_lines_per_leg)):
                    run_lines = run_lines_per_leg[leg_nr]
                    for row in range(len(run_lines.index)):
                        run_line = run_lines.iloc[row,:]
                        run_line[0] = current_id
                        run_writer.writerow(run_line)
        run_file.close()

    trajectory_header = get_trajectory_header(
        network_model.get_total_weight_dof(),
        network_model.get_total_bias_dof())

    # write away trajectory information
    if FLAGS.trajectory_file is not None:
        trajectory_writer, trajectory_file = \
            setup_csv_file(FLAGS.trajectory_file, trajectory_header)
        for current_id in data_container.get_ids():
            data_object = data_container.get_data(current_id)
            if data_object.type == "sample":
                trajectory_lines_per_leg = data_object.trajectory_lines
                for leg_nr in range(len(trajectory_lines_per_leg)):
                    trajectory_lines = trajectory_lines_per_leg[leg_nr]
                    for row in range(len(trajectory_lines.index)):
                        trajectory_line = trajectory_lines.iloc[row,:]
                        trajectory_line[0] = current_id
                        trajectory_writer.writerow(trajectory_line)
        trajectory_file.close()

    # write minima to file
    if FLAGS.minima_file is not None:
        header = trajectory_header[:]
        header.insert(header.index("loss")+1, "gradient")
        if FLAGS.do_hessians:
            insert_index = header.index("gradient")+1
            header.insert(insert_index, "num_negative_ev")
            header.insert(insert_index, "num_positive_ev")
        minima_writer, minima_file = setup_csv_file(
            FLAGS.minima_file, header)
        for current_id in data_container.get_ids():
            data_object = data_container.get_data(current_id)
            if data_object.type == "train":
                step = data_object.steps[-1]
                minima_parameters = data_object.parameters[-1]
                loss = data_object.losses[-1]
                gradient = data_object.gradients[-1]
                minima_line = [current_id, step, loss, gradient]
                if FLAGS.do_hessians:
                    hessian_ev = data_object.hessian_eigenvalues[-1]
                    num_negative_ev = (0 > hessian_ev).sum()
                    num_positive_ev = (0 < hessian_ev).sum()
                    minima_line.extend([num_positive_ev, num_negative_ev])
                minima_line.extend(np.asarray(minima_parameters))
                minima_writer.writerow(minima_line)
        minima_file.close()

    # write corner points to file
    if FLAGS.cornerpoints_file is not None:
        header = trajectory_header[:]
        cornerpoints_writer, cornerpoints_file = setup_csv_file(
            FLAGS.cornerpoints_file, header)
        for id in range(len(cornerpoints)):
            for row in range(len(cornerpoints[id])):
                print(cornerpoints[id][row][0])
                print(cornerpoints[id][row][1])
                print(np.asarray(cornerpoints[id][row][2]))
                cornerpoints_line = [id, cornerpoints[id][row][0], cornerpoints[id][row][1]]
                cornerpoints_line.extend(np.asarray(cornerpoints[id][row][2]))
                print(cornerpoints_line)
                cornerpoints_writer.writerow(cornerpoints_line)
        cornerpoints_file.close()

    network_model.finish()

    rt.set_overall_time(time.process_time() - time_zero)

if __name__ == '__main__':
    # setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.WARNING)

    FLAGS, unparsed = parse_parameters()

    react_to_common_options(FLAGS, unparsed)
    react_to_sampler_options(FLAGS, unparsed)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
