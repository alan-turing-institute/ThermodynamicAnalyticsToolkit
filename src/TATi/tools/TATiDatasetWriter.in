#!/usr/bin/env @PYTHON@
#
# This a command-line version of some of the features found under
# http://playground.tensorflow.org/
# aimed at comparing steepest descent optimizer with a bayesian
# sampling approach.
#
# (C) Frederik Heber 2017-12-01

import sys

sys.path.insert(1, '@pythondir@')

import argparse
import csv
import logging
import numpy as np

from TATi.datasets.classificationdatasets import ClassificationDatasets as DatasetGenerator
from TATi.options.commandlineoptions import react_generally_to_options

FLAGS = None


if __name__ == '__main__':
    # setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.WARNING)

    parser = argparse.ArgumentParser()
    # please adhere to alphabetical ordering
    parser.add_argument('--data_type', type=int, default=DatasetGenerator.SPIRAL,
        help='Which data set to use: (0) two circles, (1) squares, (2) two clusters, (3) spiral.')
    parser.add_argument('--dimension', type=int, default=10,
        help='Number P of samples (Y^i,X^i)^P_{i=1} to generate for the desired dataset type.')
    parser.add_argument('--noise', type=float, default=0.,
        help='Amount of noise in [0,1] to use.')
    parser.add_argument('--seed', type=int, default=None,
        help='Seed to use for random number generators.')
    parser.add_argument('--train_data_files', type=str, nargs='+', default=[],
        help='training CSV file name.')
    parser.add_argument('--train_test_ratio', type=float, default=0.5,
        help='ratio in [0,1] to split dataset into training and test part.')
    parser.add_argument('--test_data_files', type=str, nargs='+', default=[],
        help='testing CSV file name.')
    parser.add_argument('--verbose', '-v', action='count',
        help='Level of verbosity during compare')
    parser.add_argument('--version', action="store_true",
        help='Gives version information')
    FLAGS, unparsed = parser.parse_known_args()

    react_generally_to_options(FLAGS, unparsed)

    # init random: None will use random seed
    if FLAGS.seed is not None:
        np.random.seed(FLAGS.seed)

    # get number of files
    if len(FLAGS.train_data_files) != 0:
        number_files = len(FLAGS.train_data_files)
    elif len(FLAGS.test_data_files) != 0:
        number_files = len(FLAGS.test_data_files)
    else:
        logging.critial("Neither test nor train output filenames specified!")
        sys.exit(255)

    # check for same number of files
    if (len(FLAGS.train_data_files) != 0 and len(FLAGS.test_data_files) != 0) \
        and (len(FLAGS.train_data_files) !=len(FLAGS.test_data_files)):
        logging.critical("The same number of test and train files need to be specified.")
        sys.exit(255)

    # check dimension
    if FLAGS.dimension % number_files != 0:
        logging.warning("Truncating dimension to multiple of number of input files.")
    FLAGS.dimension = int(FLAGS.dimension/number_files)

    print("Generating input data")
    dataset_generator=DatasetGenerator()
    xs, ys = dataset_generator.generate(
        dimension=FLAGS.dimension*number_files,
        noise=FLAGS.noise,
        data_type=FLAGS.data_type)

    randomize = np.arange(len(xs))
    #print("Randomized set is "+str(randomize))
    np.random.shuffle(randomize)
    xs[:] = np.array(xs)[randomize]
    ys[:] = np.array(ys)[randomize]

    slice_ratio = int(FLAGS.dimension*FLAGS.train_test_ratio)
    other_slice_ratio = int(FLAGS.dimension*(1.-FLAGS.train_test_ratio))
    slice_index = slice_ratio*number_files

    if len(FLAGS.train_data_files) != 0:
        for l in range(number_files):
            logging.info("Writing to training file "+FLAGS.train_data_files[l])
            start_index = l*slice_ratio
            end_index = (l+1)*slice_ratio
            with open(FLAGS.train_data_files[l], 'w', newline='') as train_data_file:
                csv_writer = csv.writer(train_data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
                csv_writer.writerow(['x1', 'x2', 'label'])
                for x,y in zip(xs[start_index:end_index], ys[start_index:end_index]):
                    csv_writer.writerow([x[0], x[1], y[0]])
                train_data_file.close()

    if len(FLAGS.test_data_files) != 0:
        for l in range(number_files):
            logging.info("Writing to test file "+FLAGS.test_data_files[l])
            start_index = slice_index+l*other_slice_ratio
            end_index = slice_index+(l+1)*other_slice_ratio
            with open(FLAGS.test_data_files[l], 'w', newline='') as test_data_file:
                csv_writer = csv.writer(test_data_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
                csv_writer.writerow(['x1', 'x2', 'label'])
                for x,y in zip(xs[start_index:end_index], ys[start_index:end_index]):
                    csv_writer.writerow([x[0], x[1], y[0]])
                test_data_file.close()
