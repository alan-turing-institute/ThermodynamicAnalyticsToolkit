#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import argparse
import csv
import logging
import math
import numpy as np
import pandas as pd
import tensorflow as tf

from TATi.datasets.classificationdatasets import ClassificationDatasets

from TATi.common import file_length, \
        get_list_from_string, setup_csv_file
from TATi.models.model import model
from TATi.models.neuralnet_parameters import neuralnet_parameters
from TATi.options.commandlineoptions import CommandlineOptions

options = CommandlineOptions()

output_width=8
output_precision=8


def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    options.add_data_options_to_parser()
    options.add_model_options_to_parser()

    options._add_option_cmd('--csv_file', type=str, default=None,
        help='CSV file name to output sampled values to.')
    options._add_option_cmd('--interval_biases', type=str, nargs='+', default=[],
        help='Min and max value for each bias.')
    options._add_option_cmd('--interval_center_file', type=str, default=None,
        help='CSV file with step column and one for each parameter, e.g. w4 and b3.')
    options._add_option_cmd('--interval_center_step', type=int, default=None,
        help='Giving the step to use of the interval_center_file, i.e. its row')
    options._add_option_cmd('--exclude_parameters', type=str, nargs='+', default=[],
        help='List of biases, e.g. b0, and weights, e.g. w3, to exclude from sampling.')
    options._add_option_cmd('--inter_ops_threads', type=int, default=1,
        help='Sets the number of threads to split up ops in between. NOTE: This hurts reproducibility to some extent because of parallelism.')
    options._add_option_cmd('--interval_weights', type=str, nargs='+', default=[],
        help='Min and max value for each weight.')
    options._add_option_cmd('--intra_ops_threads', type=int, default=None,
        help='Sets the number of threads to use within an op, i.e. Eigen threads for linear algebra routines.')
    options._add_option_cmd('--restore_model', type=str, default=None,
        help='Restore model (weights and biases) from a file.')
    options._add_option_cmd('--samples_biases', type=int, default=None,
        help='Number of samples to take per bias interval')
    options._add_option_cmd('--samples_weights', type=int, default=None,
        help='Number of samples to take per weight interval')
    options._add_option_cmd('--verbose', '-v', action='count',
        help='Level of verbosity during compare')
    options._add_option_cmd('--version', '-V', action="store_true",
        help='Gives version information')

    return options.parse()


def countStringInCollection(_string, _collection, _sess):
    count = 0
    for var in _collection:
        if _string in var.name:
            size = _sess.run(tf.size(var))
            #print("Comparing "+_string+" with "+var.name+" of size "+str(size))
            count += size
    return count


def getDegreesFromLayerName(name, _sess):
    return countStringInCollection(name, tf.get_collection(tf.GraphKeys.WEIGHTS), _sess), \
        countStringInCollection(name, tf.get_collection(tf.GraphKeys.BIASES), _sess),

def getNumberFromParameter(key, name):
    start = name.index(key)+1
    return int(name[start:])

def create_linspace_grid(coords_size, coords_start, interval, exclude_parameters, coordname, num_samples):
    linspace = []
    len_grid = []
    for i in range(coords_size):
        interval_start = float(interval[0])
        interval_end = float(interval[1])

        keyname = coordname+str(i)
        if keyname in exclude_parameters:
            interval_start = coords_start[i]
            interval_end = interval_start

        interval_length = interval_end - interval_start

        if (num_samples > 0) and (interval_length > 0.):
            linspace.append(np.arange(0,num_samples+1)*interval_length/float(
                    num_samples)+interval_start)
            len_grid.append(num_samples+1)
        else:
            linspace.append(np.arange(0,1)+(interval_start+interval_end)/2.)
            len_grid.append(1)
    assert( len(linspace) == coords_size )
    assert( len(len_grid) == coords_size )
    #print(linspace)
    #print(len_grid)

    return linspace, len_grid


def main(_):
    # add options which model requires
    options.add("number_walkers")
    options.number_walkers = 1
    options.add("max_steps")
    options.max_steps = 1

    network_model = model(options)
    network_model.init_network(options.restore_model, setup=None)

    nn_weights = network_model.weights[0]
    nn_biases = network_model.biases[0]

    weights_vals = nn_weights.create_flat_vector()
    biases_vals = nn_biases.create_flat_vector()

    options.exclude_parameters = get_list_from_string(options.exclude_parameters)

    # get parse_parameters_file length or number of sampled points
    if options.parse_parameters_file is not None and \
            ((options.parse_steps is None) or (len(options.parse_steps) == 0)):
        options.max_steps = file_length(options.parse_parameters_file)-1
    elif options.directions_file is not None:
        # parse directions and compute max_steps from there
        directions = pd.read_csv(options.directions_file, sep=",", header=0)
        column_starts = ["weight0", "w0", "bias0", "b0", "c0", "coord0"]
        weight_name = None
        for i in range(len(column_starts)):
            try:
                weight_name = column_starts[i]
                weight_start_index = directions.columns.get_loc(weight_name)
                break
            except KeyError:
                pass
        if weight_name is None:
            print("Could not determine start column for weights and/or biases.")
            sys.exit(127)
        degrees=list(directions.columns[weight_start_index:].values)
        if len(degrees) != weights_vals.size+biases_vals.size:
           logging.critical("Directions file contains only "+str(len(degrees)) \
                +", while there are "+str(weights_vals)+" weights and "+ \
                str(biases_vals.size)+" biases.")
        coords_size = len(directions.index)
        directions = directions.iloc[:,weight_start_index:]
        print(directions)
        directions = directions.values
        for i in range(len(directions)):
            directions[i] *= 1./np.linalg.norm(directions[i])
        print(directions)
        options.max_steps = math.pow(options.samples_weights+1, coords_size)
    else:
        weight_degrees = weights_vals.size
        bias_degrees = biases_vals.size

        excluded = 0
        if (len(options.exclude_parameters) > 0):
            # subtract every degree to exclude
            for val in options.exclude_parameters:
                if "b" in val and (getNumberFromParameter("b",val) < biases_vals.size):
                    weight_degrees -= 1
                    excluded += 1
            for val in options.exclude_parameters:
                if "w" in val and (getNumberFromParameter("w",val) < weights_vals.size):
                    bias_degrees -= 1
                    excluded += 1

        print("Excluded "+str(excluded)+" parameters from max_steps sampling calculation.")

        options.max_steps = math.pow(options.samples_weights+1, weight_degrees)
        options.max_steps *= math.pow(options.samples_biases+1, bias_degrees)
        options.max_steps = math.ceil(options.max_steps)


    print("There are "+str(options.max_steps)+" points to sample.")
    network_model.reset_parameters(options)

    network_model.init_input_pipeline()
    network_model.reset_dataset()
    print(options)

    loss = network_model.nn[0].get_list_of_nodes(["loss"])
    acc = network_model.nn[0].get_list_of_nodes(["accuracy"])
    sess = network_model.sess

    ## set up output csv file
    header = ["step", "loss", "accuracy"]
    if options.directions_file is not None:
        for i in range(coords_size):
            header.append("c"+str(i))
    else:
        for i in range(weights_vals.size):
            header.append("w"+str(i))
        for i in range(biases_vals.size):
            header.append("b"+str(i))

    if options.csv_file is not None:
        csv_writer, csv_file = setup_csv_file(options.csv_file, header)

    current_step = 0

    ## function to evaluate the loss

    def evaluate_loss(coords_eval):
        # get next batch of data
        features, labels = network_model.input_pipeline.next_batch(sess)

        # place in feed dict
        feed_dict = {
            network_model.xinput: features,
            network_model.nn[0].placeholder_nodes["y_"]: labels
        }

        loss_eval, acc_eval = sess.run([loss, acc], feed_dict=feed_dict)

        #print("Loss and accuraccy at the given parameters w("+str(weights_eval)+" b("
        #      +str(biases_eval)+") is "+str(loss_eval[0])+" and "+str(acc_eval[0]))

        if options.csv_file is not None:
            print_row = [current_step] \
                        + ['{:{width}.{precision}e}'.format(loss_eval[0], width=output_width,
                                                            precision=output_precision)] \
                        + ['{:{width}.{precision}e}'.format(acc_eval[0], width=output_width,
                                                            precision=output_precision)] \
                        + ['{:{width}.{precision}e}'.format(coord, width=output_width,
                                                            precision=output_precision)
                                                            for coord in coords_eval[:]]
            csv_writer.writerow(print_row)

        return loss_eval[0]


    if (options.parse_parameters_file is not None) and \
            options.parse_steps is not None and (len(options.parse_steps) != 1):

        # parse csv file
        trajectories = pd.read_csv(options.parse_parameters_file, sep=',', header=0)
        for rownr in trajectories.index:
            weights_eval, biases_eval = network_model.assign_weights_and_biases_from_dataframe(
                df_parameters=trajectories,
                rownr=rownr,
                do_check=True
            )
            #loss_check = trajectories.loc[rownr, ['loss']].values[0]
            if (weights_eval.size != 0) and (biases_eval.size != 0):
                coord_eval = np.concatenate([weights_eval,biases_eval])
                loss_check = evaluate_loss(coord_eval)
            elif weights_eval.size != 0:
                loss_check = evaluate_loss(weights_eval)
            elif biases_eval.size != 0:
                loss_check = evaluate_loss(biases_eval)
            else:
                raise ValueError("There are no degrees of freedom")
            #assert( math.fabs( loss_eval - float(loss_check) ) < 1e-6 )

            current_step += 1

    elif options.directions_file is not None:

        print(options.parse_steps[0])
        trajectories = pd.read_csv(options.parse_parameters_file, sep=',', header=0)
        rownr = trajectories.index[trajectories.loc[:,'step'].values == options.parse_steps[0]]
        print(rownr)
        weights_eval, biases_eval = network_model.assign_weights_and_biases_from_dataframe(
            df_parameters=trajectories,
            rownr=rownr[0],
            do_check=True
        )

        weights_start = network_model.weights[0].evaluate(network_model.sess)
        biases_start = network_model.biases[0].evaluate(network_model.sess)
        print(weights_start)
        print(biases_start)

        coords_index_grid = np.zeros(coords_size, dtype=int)

        coords_start = np.concatenate([weights_start, biases_start])
        coords_linspace, coords_len_grid = \
            create_linspace_grid(coords_size, coords_start,
                options.interval_weights, options.exclude_parameters, "w",
                options.samples_weights)

        def check_end():
            isend = True
            for i in range(coords_size):
                if coords_index_grid[i] != coords_len_grid[i]-1:
                    isend = False
                    break
            return isend

        def next_index():
            for i in range(coords_size):
                if coords_index_grid[i] != coords_len_grid[i]-1:
                    coords_index_grid[i] += 1
                    for j in range(i):
                        coords_index_grid[j]=0
                    break

        is_at_end = False
        while not is_at_end:
            vals = [coords_linspace[i][ coords_index_grid[i] ] for i in range(coords_size)]

            weights_vals = weights_start.copy()
            biases_vals = biases_start.copy()
            for i in range(coords_size):
                # set the parameters for the direction
                temp = np.multiply(directions[i], vals[i])
                print(temp)
                weights_vals += temp[:weights_vals.size]
                biases_vals += temp[weights_vals.size:]

            weights_eval, biases_eval = network_model.assign_weights_and_biases( \
                weights_vals, biases_vals, do_check=True)
            evaluate_loss(vals)

            current_step += 1

            is_at_end = check_end()

            next_index()

    else:

        # evaluate weights and biases to obtain interval centers from parsed file
        weights_eval = nn_weights.evaluate(network_model.sess)
        biases_eval = nn_biases.evaluate(network_model.sess)

        weights_index_grid = np.zeros(weights_vals.size, dtype=int)
        biases_index_grid = np.zeros(biases_vals.size, dtype=int)

        weights_linspace, weights_len_grid = \
            create_linspace_grid(weights_vals.size, weights_eval,
            options.interval_weights, options.exclude_parameters, "w",
            options.samples_weights)
        biases_linspace, biases_len_grid = \
            create_linspace_grid(biases_vals.size, biases_eval,
            options.interval_biases, options.exclude_parameters, "b",
            options.samples_biases)

	### functions to iterate over weights and biases

        def check_end():
            isend = True
            for i in range(weights_vals.size):
                if weights_index_grid[i] != weights_len_grid[i]-1:
                    isend = False
                    break
            for i in range(biases_vals.size):
                if biases_index_grid[i] != biases_len_grid[i]-1:
                    isend = False
                    break
            return isend
        
        def next_index():
            incremented = False
            for i in range(weights_vals.size):
                if weights_index_grid[i] != weights_len_grid[i]-1:
                    weights_index_grid[i] += 1
                    incremented = True
                    for j in range(i):
                        weights_index_grid[j]=0
                    break
            if not incremented:
                for i in range(biases_vals.size):
                    if biases_index_grid[i] != biases_len_grid[i]-1:
                        biases_index_grid[i] += 1
                        for j in range(i):
                            biases_index_grid[j]=0
                        for j in range(weights_vals.size):
                            weights_index_grid[j]=0
                        break

        is_at_end = False
        while not is_at_end:
            # set the parameters

            weights_vals[:] = [weights_linspace[i][ weights_index_grid[i] ] for i in range(weights_vals.size)]
            biases_vals[:] = [biases_linspace[i][ biases_index_grid[i] ] for i in range(biases_vals.size)]

            weights_eval, biases_eval = network_model.assign_weights_and_biases( \
                weights_vals, biases_vals, do_check=True)
            coord_eval = np.concatenate([weights_eval,biases_eval]) \
                if (weights_eval.size != 0) and (biases_eval.size != 0) else \
                (weights_eval if weights_eval.size != 0 else biases_eval)
            evaluate_loss(coord_eval)

            for i in range(weights_eval.size):
                #print("Comparing "+str(weights_eval[i])+" against "+str(weights_vals[i]))
                assert( math.fabs(weights_eval[i] - weights_vals[i]) < 1e-6 )
            for i in range(biases_eval.size):
                #print("Comparing "+str(biases_eval[i])+" against "+str(biases_vals[i]))
                assert( math.fabs(biases_eval[i] - biases_vals[i]) < 1e-6 )

            current_step += 1

            is_at_end = check_end()

            next_index()

    if options.csv_file is not None:
        csv_file.close()

if __name__ == '__main__':
    # setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.WARNING)

    unparsed = parse_parameters()

    print(options)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

