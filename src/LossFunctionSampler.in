#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import tensorflow as tf

import numpy as np

import argparse
import math

import csv

from DataDrivenSampler.datasets.classificationdatasets import ClassificationDatasets

from DataDrivenSampler.common import add_common_options_to_parser, add_data_options_to_parser, \
        add_model_options_to_parser, get_filename_from_fullpath, react_to_common_options, \
        setup_csv_file
from DataDrivenSampler.models.model import model
from DataDrivenSampler.version import get_package_version, get_build_hash

FLAGS = None

def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    parser = argparse.ArgumentParser()
    # please adhere to alphabetical ordering

    add_data_options_to_parser(parser)
    add_model_options_to_parser(parser)

    parser.add_argument('--csv_file', type=str, default=None,
        help='CSV file name to output sampled values to.')
    parser.add_argument('--interval_biases', type=str, nargs='+', default=[],
        help='Min and max value for each bias.')
    parser.add_argument('--inter_ops_threads', type=int, default=1,
        help='Sets the number of threads to split up ops in between. NOTE: This hurts reproducibility to some extent because of parallelism.')
    parser.add_argument('--interval_weights', type=str, nargs='+', default=[],
        help='Min and max value for each weight.')
    parser.add_argument('--intra_ops_threads', type=int, default=None,
        help='Sets the number of threads to use within an op, i.e. Eigen threads for linear algebra routines.')
    parser.add_argument('--restore_model', type=str, default=None,
        help='Restore model (weights and biases) from a file.')
    parser.add_argument('--samples_biases', type=int, default=None,
        help='Number of samples to take per bias interval')
    parser.add_argument('--samples_weights', type=int, default=None,
        help='Number of samples to take per weight interval')
    parser.add_argument('--trajectory_file', type=str,default=None,
        help='Input trajectory file')
    parser.add_argument('--version', '-V', action="store_true",
        help='Gives version information')
    return parser.parse_known_args()

def main(_):
    network_model = model(FLAGS)

    network_model.init_network(FLAGS.restore_model, setup=None)

    sess = network_model.sess
    weights = tf.get_collection(tf.GraphKeys.WEIGHTS)
    length_weights = weights[0].get_shape()[0]
    weights_placeholder = tf.placeholder(shape=weights[0].get_shape(), dtype=weights[0].dtype.base_dtype)
    set_weights_t = weights[0].assign(weights_placeholder)
    biases = tf.get_collection(tf.GraphKeys.BIASES)
    length_biases = shape=biases[0].get_shape()[0]
    biases_placeholder = tf.placeholder(shape=biases[0].get_shape(), dtype=biases[0].dtype.base_dtype)
    set_biases_t = biases[0].assign(biases_placeholder)
    loss = network_model.nn.get_list_of_nodes(["loss"])

    weights_vals = np.zeros([length_weights, 1])
    biases_vals = np.zeros([length_biases])

    ## set up output csv file
    header = []
    for i in range(length_weights):
        header.append("w"+str(i))
    for i in range(length_biases):
        header.append("b"+str(i))
    header.append("loss")

    if FLAGS.csv_file is not None:
        csv_writer, csv_file = setup_csv_file(FLAGS.csv_file, header)

    ## function to evaluate the loss

    def evaluate_loss():
        batch_xs, batch_ys = network_model.ds.next_batch(FLAGS.batch_size)
        feed_dict = {
            network_model.xinput: batch_xs,
            network_model.nn.placeholder_nodes["y_"]: batch_ys
        }

        sess.run([set_weights_t, set_biases_t], feed_dict={
            weights_placeholder: weights_vals,
            biases_placeholder: biases_vals
        })

        # evaluate the loss
        loss_eval = sess.run(loss, feed_dict=feed_dict)
        #print("Loss at the given parameters w("+str(w1)+","+str(w2)+"), b("
        #      +str(b)+") is "+str(loss_eval[0]))
	
        # get the weights and biases to check against what we set
        weights_eval, biases_eval = sess.run(
            [network_model.nn.get("weights"), network_model.nn.get("biases")],
            feed_dict=feed_dict)
        for i in range(length_weights):
            #print("Comparing "+str(weights_eval[i][0])+" against "+str(weights_vals[i,0]))
            assert( math.fabs(weights_eval[i][0] - weights_vals[i,0]) < 1e-6 )
        for i in range(length_biases):
            #print("Comparing "+str(biases_eval[i])+" against "+str(biases_vals[i]))
            assert( math.fabs(biases_eval[i] - biases_vals[i]) < 1e-6 )

        if FLAGS.csv_file is not None:
            print_row = []
            print_row.extend(weights_vals[:,0])
            print_row.extend(biases_vals[:])
            print_row.append(loss_eval[0])
            csv_writer.writerow(print_row)

        return loss_eval[0]

    if FLAGS.trajectory_file is None:
        weights_interval_start = float(FLAGS.interval_weights[0])
        weights_interval_end = float(FLAGS.interval_weights[1])
        weights_interval_length = weights_interval_end - weights_interval_start
        biases_interval_start = float(FLAGS.interval_biases[0])
        biases_interval_end = float(FLAGS.interval_biases[1])
        biases_interval_length = biases_interval_end - biases_interval_start

        weights_linspace = np.arange(0,FLAGS.samples_weights+1)*weights_interval_length/float(
                FLAGS.samples_weights+1)+weights_interval_start
        biases_linspace = np.arange(0,FLAGS.samples_biases+1)*biases_interval_length/float(
                FLAGS.samples_biases+1)+biases_interval_start
        weights_index_grid = np.zeros(length_weights, dtype=int)
        weights_len_grid = FLAGS.samples_weights+1
        biases_index_grid = np.zeros(length_biases, dtype=int)
        biases_len_grid = FLAGS.samples_biases+1

	### functions to iterate over weights and biases

        def check_end():
            isend = True
            for i in range(length_weights):
                if weights_index_grid[i] != weights_len_grid-1:
                    isend = False
                    break
            for i in range(length_biases):
                if biases_index_grid[i] != biases_len_grid-1:
                    isend = False
                    break
            return isend
        
            def next_index():
                incremented = False
                for i in range(length_weights):
                    if weights_index_grid[i] != weights_len_grid-1:
                        weights_index_grid[i] += 1
                        incremented = True
                        for j in range(i):
                            weights_index_grid[j]=0
                        break
                if not incremented:
                    for i in range(length_biases):
                        if biases_index_grid[i] != biases_len_grid-1:
                            biases_index_grid[i] += 1
                            for j in range(i):
                                biases_index_grid[j]=0
                            for j in range(length_weights):
                                weights_index_grid[j]=0
                            break
        is_at_end = False
        while not is_at_end:
            # set the parameters

            weights_vals[:,0] = [weights_linspace[ weights_index_grid[i] ] for i in range(length_weights)]
            biases_vals[:] = [biases_linspace[ biases_index_grid[i] ] for i in range(length_biases)]

            evaluate_loss()

            is_at_end = check_end()

            next_index()
    else:
        # parse csv file
        with open(FLAGS.trajectory_file, 'r') as csv_file:
            csvreader = csv.reader(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
            # go through each row and use values
            next(csvreader) # skip header
            row_length = length_weights+length_biases+2 # initially, there is step and loss
            for row in csvreader:
                print("length of row is "+str(len(row))+" and expected length is "+str(row_length))
                assert( len(row) >= row_length)
                weights_vals[:length_weights,0] = row[2:(length_weights+2)]
                biases_vals[:length_biases] = row[(length_weights+2):row_length]
                loss_eval = evaluate_loss()
                #assert( math.fabs( loss_eval - float(row[1]) ) < 1e-6 )

    network_model.finish()

    if FLAGS.csv_file is not None:
        csv_file.close()

if __name__ == '__main__':
    FLAGS, unparsed = parse_parameters()

    react_to_common_options(FLAGS, unparsed)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

