#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import argparse
import csv
import logging
import math
import numpy as np
import tensorflow as tf

from DataDrivenSampler.datasets.classificationdatasets import ClassificationDatasets

from DataDrivenSampler.common import add_common_options_to_parser, add_data_options_to_parser, \
        add_model_options_to_parser, file_length, get_filename_from_fullpath, \
        get_list_from_string, react_to_common_options, setup_csv_file
from DataDrivenSampler.models.model import model
from DataDrivenSampler.models.neuralnet_parameters import neuralnet_parameters
from DataDrivenSampler.version import get_package_version, get_build_hash

FLAGS = None

output_width=8
output_precision=8


def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    parser = argparse.ArgumentParser()
    # please adhere to alphabetical ordering

    add_data_options_to_parser(parser)
    add_model_options_to_parser(parser)

    parser.add_argument('--csv_file', type=str, default=None,
        help='CSV file name to output sampled values to.')
    parser.add_argument('--interval_biases', type=str, nargs='+', default=[],
        help='Min and max value for each bias.')
    parser.add_argument('--exclude_parameters', type=str, nargs='+', default=[],
        help='List of biases, e.g. b0, and weights, e.g. w3, to exclude from sampling.')
    parser.add_argument('--inter_ops_threads', type=int, default=1,
        help='Sets the number of threads to split up ops in between. NOTE: This hurts reproducibility to some extent because of parallelism.')
    parser.add_argument('--interval_weights', type=str, nargs='+', default=[],
        help='Min and max value for each weight.')
    parser.add_argument('--intra_ops_threads', type=int, default=None,
        help='Sets the number of threads to use within an op, i.e. Eigen threads for linear algebra routines.')
    parser.add_argument('--restore_model', type=str, default=None,
        help='Restore model (weights and biases) from a file.')
    parser.add_argument('--samples_biases', type=int, default=None,
        help='Number of samples to take per bias interval')
    parser.add_argument('--samples_weights', type=int, default=None,
        help='Number of samples to take per weight interval')
    parser.add_argument('--trajectory_file', type=str,default=None,
        help='Input trajectory file')
    parser.add_argument('--verbose', '-v', action='count',
        help='Level of verbosity during compare')
    parser.add_argument('--version', '-V', action="store_true",
        help='Gives version information')
    return parser.parse_known_args()


def countStringInCollection(_string, _collection, _sess):
    count = 0
    for var in _collection:
        if _string in var.name:
            size = _sess.run(tf.size(var))
            print("Comparing "+_string+" with "+var.name+" of size "+str(size))
            count += size
    return count


def getDegreesFromLayerName(name, _sess):
    return countStringInCollection(name, tf.get_collection(tf.GraphKeys.WEIGHTS), _sess), \
        countStringInCollection(name, tf.get_collection(tf.GraphKeys.BIASES), _sess),

def getNumberFromParameter(key, name):
    start = name.index(key)+1
    return int(name[start:])

def main(_):
    network_model = model(FLAGS)
    network_model.init_network(FLAGS.restore_model, setup=None)

    nn_weights = network_model.weights
    nn_biases = network_model.biases

    weights_vals = nn_weights.create_flat_vector()
    biases_vals = nn_biases.create_flat_vector()

    # get trajectory length or number of sampled points
    if FLAGS.trajectory_file is not None:
        FLAGS.max_steps = file_length(FLAGS.trajectory_file)-1
    else:
        weight_degrees = weights_vals.size
        bias_degrees = biases_vals.size

        excluded = 0
        if (len(FLAGS.exclude_parameters) > 0):
            # subtract every degree to exclude
            for val in FLAGS.exclude_parameters:
                if "b" in val and (getNumberFromParameter("b",val) < biases_vals.size):
                    weight_degrees -= 1
                    excluded += 1
            for val in FLAGS.exclude_parameters:
                if "w" in val and (getNumberFromParameter("w",val) < weights_vals.size):
                    bias_degrees -= 1
                    excluded += 1

        print("Excluded "+str(excluded)+" parameters from max_steps sampling calculation.")

        FLAGS.max_steps = math.pow(FLAGS.samples_weights+1, weight_degrees)
        FLAGS.max_steps *= math.pow(FLAGS.samples_biases+1, bias_degrees)
        FLAGS.max_steps = math.ceil(FLAGS.max_steps)


    print("There are "+str(FLAGS.max_steps)+" points to sample.")
    network_model.reset_parameters(FLAGS)

    network_model.init_input_pipeline()
    print(FLAGS)


    loss = network_model.nn.get_list_of_nodes(["loss"])
    sess = network_model.sess

    ## set up output csv file
    header = ["step", "loss"]
    for i in range(weights_vals.size):
        header.append("w"+str(i))
    for i in range(biases_vals.size):
        header.append("b"+str(i))

    if FLAGS.csv_file is not None:
        csv_writer, csv_file = setup_csv_file(FLAGS.csv_file, header)

    current_step = 0

    ## function to evaluate the loss

    def evaluate_loss():
        # get next batch of data
        features, labels = network_model.input_pipeline.next_batch(sess)

        # place in feed dict
        feed_dict = {
            network_model.xinput: features,
            network_model.nn.placeholder_nodes["y_"]: labels
        }

        nn_weights.assign(sess, weights_vals)
        nn_biases.assign(sess, biases_vals)

        loss_eval = sess.run(loss, feed_dict=feed_dict)
        #print("Loss at the given parameters w("+str(w1)+","+str(w2)+"), b("
        #      +str(b)+") is "+str(loss_eval[0]))

        # get the weights and biases to check against what we set
        weights_eval = nn_weights.evaluate(sess)
        biases_eval = nn_biases.evaluate(sess)

        for i in range(weights_eval.size):
            #print("Comparing "+str(weights_eval[i])+" against "+str(weights_vals[i]))
            assert( math.fabs(weights_eval[i] - weights_vals[i]) < 1e-6 )
        for i in range(biases_eval.size):
            #print("Comparing "+str(biases_eval[i])+" against "+str(biases_vals[i]))
            assert( math.fabs(biases_eval[i] - biases_vals[i]) < 1e-6 )

        if FLAGS.csv_file is not None:
            print_row = [current_step] \
                        + ['{:{width}.{precision}e}'.format(loss_eval[0], width=output_width,
                                                            precision=output_precision)] \
                        + ['{:{width}.{precision}e}'.format(weight, width=output_width,
                                                            precision=output_precision)
                                                            for weight in weights_vals[:]] \
                        + ['{:{width}.{precision}e}'.format(bias, width=output_width,
                                                            precision=output_precision)
                                                            for bias in biases_vals[:]]
            csv_writer.writerow(print_row)

        return loss_eval[0]

    if FLAGS.trajectory_file is None:

        weights_index_grid = np.zeros(weights_vals.size, dtype=int)
        biases_index_grid = np.zeros(biases_vals.size, dtype=int)

        weights_linspace = []
        weights_len_grid = []
        for i in range(weights_vals.size):
            weights_interval_start = float(FLAGS.interval_weights[0])
            weights_interval_end = float(FLAGS.interval_weights[1])

            keyname = "w"+str(i)
            if keyname in FLAGS.exclude_parameters:
                weights_interval_length = 0.
            else:
                weights_interval_length = weights_interval_end - weights_interval_start

            if (FLAGS.samples_weights > 0) and (weights_interval_length > 0.):
                weights_linspace.append(np.arange(0,FLAGS.samples_weights+1)*weights_interval_length/float(
                        FLAGS.samples_weights)+weights_interval_start)
                weights_len_grid.append(FLAGS.samples_weights+1)
            else:
                weights_linspace.append(np.arange(0,1)+(weights_interval_start+weights_interval_end)/2.)
                weights_len_grid.append(1)
        assert( len(weights_linspace) == weights_vals.size )
        assert( len(weights_len_grid) == weights_vals.size )
        print(weights_linspace)
        print(weights_len_grid)

        biases_linspace = []
        biases_len_grid = []
        for i in range(biases_vals.size):
            biases_interval_start = float(FLAGS.interval_biases[0])
            biases_interval_end = float(FLAGS.interval_biases[1])

            keyname = "b"+str(i)
            if keyname in FLAGS.exclude_parameters:
                biases_interval_length = 0.
            else:
                biases_interval_length = biases_interval_end - biases_interval_start

            if (FLAGS.samples_biases > 0) and (biases_interval_length > 0.):
                biases_linspace.append(np.arange(0,FLAGS.samples_biases+1)*biases_interval_length/float(
                        FLAGS.samples_biases)+biases_interval_start)
                biases_len_grid.append(FLAGS.samples_biases+1)
            else:
                biases_linspace.append(np.arange(0,1)+(biases_interval_start+biases_interval_end)/2.)
                biases_len_grid.append(1)
        assert( len(biases_linspace) == biases_vals.size )
        assert( len(biases_len_grid) == biases_vals.size )
        print(biases_linspace)
        print(biases_len_grid)

	### functions to iterate over weights and biases

        def check_end():
            isend = True
            for i in range(weights_vals.size):
                if weights_index_grid[i] != weights_len_grid[i]-1:
                    isend = False
                    break
            for i in range(biases_vals.size):
                if biases_index_grid[i] != biases_len_grid[i]-1:
                    isend = False
                    break
            return isend
        
        def next_index():
            incremented = False
            for i in range(weights_vals.size):
                if weights_index_grid[i] != weights_len_grid[i]-1:
                    weights_index_grid[i] += 1
                    incremented = True
                    for j in range(i):
                        weights_index_grid[j]=0
                    break
            if not incremented:
                for i in range(biases_vals.size):
                    if biases_index_grid[i] != biases_len_grid[i]-1:
                        biases_index_grid[i] += 1
                        for j in range(i):
                            biases_index_grid[j]=0
                        for j in range(weights_vals.size):
                            weights_index_grid[j]=0
                        break

        is_at_end = False
        while not is_at_end:
            # set the parameters

            weights_vals[:] = [weights_linspace[i][ weights_index_grid[i] ] for i in range(weights_vals.size)]
            biases_vals[:] = [biases_linspace[i][ biases_index_grid[i] ] for i in range(biases_vals.size)]

            evaluate_loss()

            current_step += 1

            is_at_end = check_end()

            next_index()
    else:
        # parse csv file
        with open(FLAGS.trajectory_file, 'r') as csv_file:
            csvreader = csv.reader(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
            # go through each row and use values
            next(csvreader) # skip header
            row_length = weights_vals.size+biases_vals.size+2 # initially, there is step and loss
            for row in csvreader:
                #print("length of row is "+str(len(row))+" and expected length is "+str(row_length))
                assert( len(row) >= row_length)
                step = row[0]
                loss_check = row[0]
                weights_vals[:weights_vals.size] = row[2:(weights_vals.size+2)]
                biases_vals[:biases_vals.size] = row[(weights_vals.size+2):row_length]
                loss_eval = evaluate_loss()
                #assert( math.fabs( loss_eval - float(loss_check) ) < 1e-6 )

                current_step += 1

    network_model.finish()

    if FLAGS.csv_file is not None:
        csv_file.close()

if __name__ == '__main__':
    # setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.WARNING)

    FLAGS, unparsed = parse_parameters()

    react_to_common_options(FLAGS, unparsed)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

