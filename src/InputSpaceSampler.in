#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import argparse
import csv
import logging
import math
import numpy as np
import pandas as pd
import tensorflow as tf


from DataDrivenSampler.datasets.classificationdatasets import ClassificationDatasets

from DataDrivenSampler.common import add_common_options_to_parser, add_data_options_to_parser, \
        add_model_options_to_parser, get_filename_from_fullpath, get_list_from_string, \
        react_to_common_options, setup_csv_file
from DataDrivenSampler.models.model import model
from DataDrivenSampler.models.neuralnet_parameters import neuralnet_parameters
from DataDrivenSampler.version import get_package_version, get_build_hash

FLAGS = None

output_width=8
output_precision=8


def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    parser = argparse.ArgumentParser()
    # please adhere to alphabetical ordering

    add_data_options_to_parser(parser)
    add_model_options_to_parser(parser)

    parser.add_argument('--csv_file', type=str, default=None,
        help='CSV file name to output sampled values to.')
    parser.add_argument('--inter_ops_threads', type=int, default=1,
        help='Sets the number of threads to split up ops in between. NOTE: This hurts reproducibility to some extent because of parallelism.')
    parser.add_argument('--interval_input', type=str, nargs='+', default=[],
        help='Min and max value for each weight.')
    parser.add_argument('--intra_ops_threads', type=int, default=None,
        help='Sets the number of threads to use within an op, i.e. Eigen threads for linear algebra routines.')
    parser.add_argument('--restore_model', type=str, default=None,
        help='Restore model (input and biases) from a file.')
    parser.add_argument('--samples_input', type=int, default=None,
        help='Number of samples to take per weight interval')
    parser.add_argument('--steps', type=int, nargs='+', default=[],
        help='At which steps of the loaded trajectory to sample the input space.')
    parser.add_argument('--trajectory_file', type=str, default=None,
        help='Input trajectory file')
    parser.add_argument('--verbose', '-v', action='count',
        help='Level of verbosity during compare')
    parser.add_argument('--version', '-V', action="store_true",
        help='Gives version information')
    return parser.parse_known_args()

def main(_):

    # create the data set
    input_dimension = FLAGS.input_dimension

    # sample space
    input_interval_start = float(FLAGS.interval_input[0])
    input_interval_end = float(FLAGS.interval_input[1])
    input_interval_length = input_interval_end - input_interval_start

    input_linspace = np.arange(0,FLAGS.samples_input+1)*input_interval_length/float(
            FLAGS.samples_input)+input_interval_start
    input_index_grid = np.zeros(input_dimension, dtype=int)
    input_len_grid = FLAGS.samples_input+1

    input_vals = np.zeros(input_dimension, dtype=float)

    input_total_vals = math.pow(FLAGS.samples_input+1, input_dimension)

### functions to iterate over input and biases

    def check_end():
        isend = True
        for i in range(input_dimension):
            if input_index_grid[i] != input_len_grid-1:
                isend = False
                break
        return isend

    def next_index():
        for i in range(input_dimension):
            if input_index_grid[i] != input_len_grid-1:
                input_index_grid[i] += 1
                for j in range(i):
                    input_index_grid[j]=0
                break


    ## set up output csv file
    header = [] #["i"]
    for i in range(input_dimension):
        header.append("x"+str(i+1))
    header.append("label")

    csv_writer, csv_file = setup_csv_file(FLAGS.batch_data_files[0], header)
    current_step = 0
    is_at_end = False
    while not is_at_end:
        # set the parameters

        input_vals[:] = [input_linspace[ input_index_grid[i] ] for i in range(input_dimension)]

        print_row=[] # [current_step]
        print_row.extend(np.asarray(input_vals))
        print_row.append(0)
        csv_writer.writerow(print_row)

        current_step += 1

        is_at_end = check_end()

        next_index()

    csv_file.flush()
    csv_file.close()

    FLAGS.batch_size=1
    FLAGS.max_steps=input_total_vals
    network_model = model(FLAGS)
    network_model.init_network(FLAGS.restore_model, setup=None)
    network_model.init_input_pipeline()

    y = network_model.nn.get_list_of_nodes(["y"])
    sess = network_model.sess

    nn_weights = network_model.weights
    nn_biases = network_model.biases

    weights_vals = nn_weights.create_flat_vector()
    biases_vals = nn_biases.create_flat_vector()

    if FLAGS.csv_file is not None:
        header = ["i"]
        for i in range(input_dimension):
            header.append("x"+str(i+1))
        header.append("label")
        csv_writer, csv_file = setup_csv_file(FLAGS.csv_file, header)

    def assign_parameters():
        nn_weights.assign(sess, weights_vals)
        nn_biases.assign(sess, biases_vals)

        # get the input and biases to check against what we set
        weights_eval = nn_weights.evaluate(sess)
        biases_eval = nn_biases.evaluate(sess)

        print("Evaluating at weights "+str(weights_eval[0:10])+", biases "+str(biases_eval[0:10]))

    ## function to evaluate the loss

    def evaluate_label(current_step):
        # get next batch of data
        features, _ = network_model.input_pipeline.next_batch(sess, auto_reset=False)
        print("batch_data is "+str(features))

        # place in feed dict
        feed_dict = {
            network_model.xinput: features,
        }

        print(np.asarray(features[0]))

        label = network_model.sess.run(
            network_model.nn.placeholder_nodes["y"],
            feed_dict=feed_dict)

        if FLAGS.csv_file is not None:
            print_row = [current_step] \
                        + ['{:{width}.{precision}e}'.format(feature, width=output_width,
                                                            precision=output_precision)
                                                            for feature in np.asarray(features[0])] \
                        + ['{:{width}.{precision}e}'.format(label, width=output_width,
                                                            precision=output_precision)
                                                            for label in np.asarray(label[0])]
            csv_writer.writerow(print_row)

    if FLAGS.trajectory_file is None:
        print("Need trajectory file that contains weights and biases per step.")
        sys.exit(255)
    else:
        # parse csv file
        trajectories = pd.read_csv(FLAGS.trajectory_file, sep=',', header=0)
        for step in FLAGS.steps:
            if step in trajectories.loc[:,['step']].values:
                parameters = {}
                rownr = np.where(trajectories.loc[:,['step']].values==step)[0]
                for keyname in trajectories.columns:
                    if  (keyname[1] >= "0" and keyname[1] <= "9"):
                        if ("w" == keyname[0]):
                            fullname = "weight"
                        elif "b" == keyname[0]:
                            fullname = "bias"
                        else:
                            # not a parameter column
                            continue
                        fullname += keyname[1:]
                        parameters[fullname] = trajectories.loc[rownr, [keyname]].values[0]
                    else:
                        if ("weight" in keyname) or ("bias" in keyname):
                            parameters[keyname] = trajectories.loc[rownr, [keyname]].values[0]
                print("Got for step "+str(step)+":"+str(parameters))

                loss_check = trajectories.loc[rownr, ['loss']].values[0]
                weights_vals[:weights_vals.size] = [parameters[key] for key in sorted(parameters.keys()) if "w" in key]
                biases_vals[:biases_vals.size] = [parameters[key] for key in sorted(parameters.keys()) if "b" in key]
                assign_parameters()

                current_step = 0
                while True:
                    try:
                        evaluate_label(current_step)
                        current_step += 1
                    except tf.errors.OutOfRangeError:
                        print("End of dataset.")
                        break

    network_model.finish()

    if FLAGS.csv_file is not None:
        csv_file.close()

if __name__ == '__main__':
    # setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.WARNING)

    FLAGS, unparsed = parse_parameters()

    react_to_common_options(FLAGS, unparsed)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)

