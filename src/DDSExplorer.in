#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import argparse
import logging
import numpy as np
import tensorflow as tf
import time

import scipy.spatial.distance as scidist

from DataDrivenSampler.common import add_common_options_to_parser, add_data_options_to_parser, \
        add_model_options_to_parser, add_prior_options_to_parser, add_sampler_options_to_parser, \
        get_filename_from_fullpath, get_trajectory_header,\
        react_to_common_options, react_to_sampler_options, \
        setup_csv_file
from DataDrivenSampler.exploration.explorer import Explorer
from DataDrivenSampler.models.model import model
from DataDrivenSampler.runtime.runtime import runtime
from DataDrivenSampler.TrajectoryAnalyser import compute_diffusion_maps

FLAGS = None

def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    parser = argparse.ArgumentParser()

    add_common_options_to_parser(parser)
    add_data_options_to_parser(parser)
    add_model_options_to_parser(parser)
    add_prior_options_to_parser(parser)
    add_sampler_options_to_parser(parser)

    # please adhere to alphabetical ordering
    parser.add_argument('--cornerpoints_file', type=str, default=None,
        help='Filename to write found corner points to')
    parser.add_argument('--diffusion_map_method', type=str, default='vanilla',
        help='Method to use for computing the diffusion map: pydiffmap, vanilla or TMDMap')
    parser.add_argument('--max_exploration_steps', type=int, default=2,
        help='Maximum number of exploration steps')
    parser.add_argument('--max_legs', type=int, default=100,
        help='Maximum number of legs per trajectory')
    parser.add_argument('--minima_file', type=str, default=None,
        help='Filename to write found minima to')
    parser.add_argument('--number_of_eigenvalues', type=int, default=4,
        help='How many largest eigenvalues to compute')
    parser.add_argument('--number_of_parallel_trajectories', type=int, default=5,
        help='Number of trajectories to run in parallel, i.e. number of points maximally apart in diffusion distance')
    parser.add_argument('--number_pruning', type=int, default=0,
        help='Number of pruning stages through metropolis criterion after end of trajectory')
    parser.add_argument('--optimizer', type=str, default="GradientDescent",
        help='Choose the optimizer to use for sampling: GradientDescent')
    parser.add_argument('--use_reweighting', type=bool, default=False,
        help='Use reweighting of the kernel matrix of diffusion maps by the target distribution.')

    return parser.parse_known_args()


def main(_):
    rt = runtime(FLAGS)
    time_zero = time.process_time()

    # setup neural network
    network_model = model(FLAGS)
    time_init_network_zero = time.process_time()
    # prepare for both sampling and training
    network_model.init_network(FLAGS.restore_model, setup="sample")
    network_model.init_network(FLAGS.restore_model, setup="train")
    rt.set_init_network_time(time.process_time() - time_init_network_zero)

    # prevent writing of output files during leg sampling
    network_model.config_map["do_write_run_file"] = False
    network_model.config_map["do_write_trajectory_file"] = False

    # 1. run initially just one trajectory
    explorer = Explorer(max_legs=FLAGS.max_legs,
                        number_pruning=FLAGS.number_pruning)

    print("Creating starting trajectory.")
    # a. add three legs to queue
    explorer.spawn_starting_trajectory(network_model)

    # b. continue until queue has run dry
    explorer.run_all_jobs(network_model, FLAGS)

    print("Starting multiple explorations from starting trajectory.")
    # 2. with the initial trajectory done and analyzed,
    #    find maximally separate points and sample from these
    cornerpoints = []
    exploration_step = 1
    while exploration_step < FLAGS.max_exploration_steps:
        # a. combine all trajectories
        steps, parameters, losses = explorer.combine_trajectories()

        # b. perform diffusion map analysis for eigenvectors
        idx_corner = explorer.get_corner_points(parameters, losses, FLAGS, FLAGS.number_of_parallel_trajectories)

        # d. spawn new trajectories from these points
        cornerpoints.append( explorer.spawn_corner_trajectories(steps, parameters, losses, idx_corner, network_model) )

        # d. run all trajectories till terminated
        explorer.run_all_jobs(network_model, FLAGS)

        exploration_step += 1

    rt.set_train_network_time(time.process_time() - rt.time_init_network)

    # 3. write final set of values

    # write away run time information
    if network_model.config_map["csv_file"] is not None:
        network_model.config_map["do_write_run_file"] = True
        for current_id in explorer.container.data.keys():
            run_lines_per_leg = explorer.container.data[current_id].run_lines
            for leg_nr in range(len(run_lines_per_leg)):
                run_lines = run_lines_per_leg[leg_nr]
                for row in range(len(run_lines.index)):
                    run_line = run_lines.iloc[row,:]
                    run_line[0] = current_id
                    network_model.run_writer.writerow(run_line)


    # write away trajectory information
    if network_model.config_map["trajectory_file"] is not None:
        network_model.config_map["do_write_trajectory_file"] = True
        for current_id in explorer.container.data.keys():
            trajectory_lines_per_leg = explorer.container.data[current_id].trajectory_lines
            for leg_nr in range(len(trajectory_lines_per_leg)):
                trajectory_lines = trajectory_lines_per_leg[leg_nr]
                for row in range(len(trajectory_lines.index)):
                    trajectory_line = trajectory_lines.iloc[row,:]
                    trajectory_line[0] = current_id
                    network_model.trajectory_writer.writerow(trajectory_line)

    # write minima to file
    if FLAGS.minima_file is not None:
        header = get_trajectory_header(
            network_model.weights.get_total_dof(),
            network_model.biases.get_total_dof())
        minima_writer, minima_file = setup_csv_file(
            FLAGS.minima_file, header)
        for current_id in explorer.container.data.keys():
            minima = explorer.container.data[current_id].local_minima
            losses =  explorer.container.data[current_id].loss_at_minima
            for row in range(len(minima)):
                minima_line = [current_id, row, losses[row]]
                minima_line.extend(np.asarray(minima[row]))
                minima_writer.writerow(minima_line)
        minima_file.close()

    # write corner points to file
    if FLAGS.cornerpoints_file is not None:
        header = get_trajectory_header(
            network_model.weights.get_total_dof(),
            network_model.biases.get_total_dof())
        cornerpoints_writer, cornerpoints_file = setup_csv_file(
            FLAGS.cornerpoints_file, header)
        for id in range(len(cornerpoints)):
            for row in range(len(cornerpoints[id])):
                print(cornerpoints[id][row][0])
                print(cornerpoints[id][row][1])
                print(np.asarray(cornerpoints[id][row][2]))
                cornerpoints_line = [id, cornerpoints[id][row][0], cornerpoints[id][row][1]]
                cornerpoints_line.extend(np.asarray(cornerpoints[id][row][2]))
                print(cornerpoints_line)
                cornerpoints_writer.writerow(cornerpoints_line)
        cornerpoints_file.close()

    network_model.finish()

    rt.set_overall_time(time.process_time() - time_zero)

if __name__ == '__main__':
    # setup logging
    logging.basicConfig(stream=sys.stdout, level=logging.WARNING)

    FLAGS, unparsed = parse_parameters()

    react_to_common_options(FLAGS, unparsed)
    react_to_sampler_options(FLAGS, unparsed)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
