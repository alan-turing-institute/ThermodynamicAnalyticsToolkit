#!/usr/bin/env @PYTHON@

import sys, getopt
sys.path.insert(1, '@pythondir@')

import tensorflow as tf
import numpy as np

import argparse
import time

import scipy.spatial.distance as scidist

from DataDrivenSampler.common import add_common_options_to_parser, add_data_options_to_parser, \
        add_model_options_to_parser, add_prior_options_to_parser, add_sampler_options_to_parser, \
        get_filename_from_fullpath, get_trajectory_header, \
        react_to_common_options, react_to_sampler_options, \
        setup_csv_file
from DataDrivenSampler.exploration.trajectorydatacontainer import TrajectoryDataContainer
from DataDrivenSampler.exploration.trajectoryqueue import TrajectoryQueue
from DataDrivenSampler.models.model import model
from DataDrivenSampler.runtime.runtime import runtime
from DataDrivenSampler.TrajectoryAnalyser import compute_diffusion_maps

FLAGS = None

INITIAL_LEGS = 3 # how many legs to run without convergence stop check

def parse_parameters():
    """ Sets up the argument parser for parsing command line parameters into dictionary

    :return: dictionary with parameter names as keys, unrecognized parameters
    """
    parser = argparse.ArgumentParser()

    add_common_options_to_parser(parser)
    add_data_options_to_parser(parser)
    add_model_options_to_parser(parser)
    add_prior_options_to_parser(parser)
    add_sampler_options_to_parser(parser)

    # please adhere to alphabetical ordering
    parser.add_argument('--cornerpoints_file', type=str, default=None,
        help='Filename to write found corner points to')
    parser.add_argument('--diffusion_map_method', type=str, default='vanilla',
        help='Method to use for computing the diffusion map: pydiffmap, vanilla or TMDMap')
    parser.add_argument('--max_exploration_steps', type=int, default=2,
        help='Maximum number of exploration steps')
    parser.add_argument('--max_legs', type=int, default=100,
        help='Maximum number of legs per trajectory')
    parser.add_argument('--minima_file', type=str, default=None,
        help='Filename to write found minima to')
    parser.add_argument('--number_of_eigenvalues', type=int, default=4,
        help='How many largest eigenvalues to compute')
    parser.add_argument('--number_of_parallel_trajectories', type=int, default=5,
        help='Number of trajectories to run in parallel, i.e. number of points maximally apart in diffusion distance')
    parser.add_argument('--number_pruning', type=int, default=0,
        help='Number of pruning stages through metropolis criterion after end of trajectory')
    parser.add_argument('--optimizer', type=str, default="GradientDescent",
        help='Choose the optimizer to use for sampling: GradientDescent')
    parser.add_argument('--use_reweighting', type=bool, default=False,
        help='Use reweighting of the kernel matrix of diffusion maps by the target distribution.')

    return parser.parse_known_args()


def find_corner_points(dmap_eigenvectors, number_corner_points):
    """ Finds corner points given the diffusion map eigenvectors of a trajectory.

    :param dmap_eigenvectors: diffusion map eigenvector matrix
    :param number_corner_points: desired number of corner points
    :return: indices of the corner points with respect to trajectory
    """
    # select a random point and compute distances to it
    select_first = "dominant_eigenmode" # "random"
    if select_first == "random":
        m = np.shape(dmap_eigenvectors)[0]
        idx_corner = np.random.randint(m)

        dist = scidist.cdist(dmap_eigenvectors[[idx_corner],:], dmap_eigenvectors)[0]
        idx_corner = [np.argmax(dist)]

    elif select_first == "dominant_eigenmode":
        # find first cornerstone as maximum on dominant eigenvector
        idx_corner = [np.argmax(dmap_eigenvectors[:,0])]
    else:
        assert( False )

    # print('idx_corner ')
    # print(idx_corner)
    # iteration to find the other cornerstones
    for k in np.arange(1, number_corner_points):
        # update minimum distance to existing cornerstones
        if(k>1):
            dist = np.minimum(dist, scidist.cdist(dmap_eigenvectors[[idx_corner[-1]],:], dmap_eigenvectors)[0])
        else:
            dist = scidist.cdist(dmap_eigenvectors[idx_corner,:], dmap_eigenvectors)[0]
        # select new cornerstone
        idx_corner.append(np.argmax(dist))

    return idx_corner


def main(_):
    rt = runtime(FLAGS)
    time_zero = time.process_time()

    # setup neural network
    network_model = model(FLAGS)
    time_init_network_zero = time.process_time()
    # prepare for both sampling and training
    network_model.init_network(FLAGS.restore_model, setup="sample")
    network_model.init_network(FLAGS.restore_model, setup="train")
    rt.set_init_network_time(time.process_time() - time_init_network_zero)

    # prevent writing of output files during leg sampling
    network_model.config_map["do_write_run_file"] = False
    network_model.config_map["do_write_trajectory_file"] = False

    # 1. run initially just one trajectory
    container = TrajectoryDataContainer()
    queue = TrajectoryQueue(container, FLAGS.number_pruning)

    print("Creating starting trajectory.")
    # a. add three legs to queue
    for i in range(1,INITIAL_LEGS+1):
        current_id = container.add_empty_data()
        queue.add_run_job(
            data_id=current_id,
            network_model=network_model,
            initial_step=0,
            parameters=None,
            continue_flag=(i == INITIAL_LEGS))

    # b. continue until queue has run dry
    while not queue.is_empty():
        queue.run_next_job(network_model, FLAGS)

    print("Starting multiple explorations from starting trajectory.")
    # 2. with the initial trajectory done and analyzed,
    #    find maximally separate points and sample from these
    cornerpoints = []
    exploration_step = 1
    while exploration_step < FLAGS.max_exploration_steps:
        # a. combine all trajectories
        steps = []
        parameters = []
        losses = []
        for id in container.data.keys():
            steps.extend( container.data[id].steps )
            parameters.extend( container.data[id].parameters )
            losses.extend( container.data[id].losses )

        # b. perform diffusion map analysis for eigenvectors
        try:
            dmap_eigenvectors, dmap_eigenvalues, dmap_kernel = compute_diffusion_maps( \
                traj=parameters, \
                beta=FLAGS.inverse_temperature, \
                loss=losses, \
                nrOfFirstEigenVectors=FLAGS.number_of_eigenvalues, \
                method=FLAGS.diffusion_map_method,
                use_reweighting=FLAGS.use_reweighting)
        except scipy.sparse.linalg.eigen.arpack.ArpackNoConvergence:
            print("ERROR: Vectors were non-convergent.")
            dmap_eigenvectors = np.zeros( (np.shape(parameters)[0], FLAGS.number_of_eigenvalues) )
            dmap_eigenvalues = np.zeros( (FLAGS.number_of_eigenvalues) )
            dmap_kernel = np.zeros( (np.shape(parameters)[0], np.shape(trajectory)[0]) )
            # override landmarks to skip computation
            FLAGS.landmarks = 0
        print("Global diffusion map eigenvalues: "+str(dmap_eigenvalues))

        # c. find number of points maximally apart
        idx_corner = find_corner_points(
                dmap_eigenvectors, FLAGS.number_of_parallel_trajectories)

        # d. spawn new trajectories from these points
        for i in range(FLAGS.number_of_parallel_trajectories):
            print("Current corner point is "+str(parameters[idx_corner[i]]))
            current_id = container.add_empty_data()
            data_object = container.data[current_id]
            data_object.steps[:] = [steps[idx_corner[i]]]
            data_object.parameters[:] = [parameters[idx_corner[i]]]
            data_object.losses[:] = [losses[idx_corner[i]]]
            data_object.gradients[:] = [1]

            queue.add_run_job(
                data_id=current_id,
                network_model=network_model,
                initial_step=container.data[current_id].steps[-1],
                parameters=container.data[current_id].parameters[-1],
                continue_flag=True)
            cornerpoints.append( [data_object.steps[-1], data_object.losses[-1], data_object.parameters[-1]] )

        # d. run all trajectories till terminated
        while not queue.is_empty():
            queue.run_next_job(network_model, FLAGS)

        exploration_step += 1

    rt.set_train_network_time(time.process_time() - rt.time_init_network)

    # 3. write final set of values

    # write away run time information
    if network_model.config_map["csv_file"] is not None:
        network_model.config_map["do_write_run_file"] = True
        for current_id in container.data.keys():
            run_lines_per_leg = container.data[current_id].run_lines
            for leg_nr in range(len(run_lines_per_leg)):
                run_lines = run_lines_per_leg[leg_nr]
                for row in range(len(run_lines.index)):
                    run_line = run_lines.iloc[row,:]
                    run_line[0] = current_id
                    network_model.run_writer.writerow(run_line)


    # write away trajectory information
    if network_model.config_map["trajectory_file"] is not None:
        network_model.config_map["do_write_trajectory_file"] = True
        for current_id in container.data.keys():
            trajectory_lines_per_leg = container.data[current_id].trajectory_lines
            for leg_nr in range(len(trajectory_lines_per_leg)):
                trajectory_lines = trajectory_lines_per_leg[leg_nr]
                for row in range(len(trajectory_lines.index)):
                    trajectory_line = trajectory_lines.iloc[row,:]
                    trajectory_line[0] = current_id
                    network_model.trajectory_writer.writerow(trajectory_line)

    # write minima to file
    if FLAGS.minima_file is not None:
        header = get_trajectory_header(
            network_model.weights.get_total_dof(),
            network_model.biases.get_total_dof())
        minima_writer, minima_file = setup_csv_file(
            FLAGS.minima_file, header)
        for current_id in explorer.container.data.keys():
            minima = explorer.container.data[current_id].local_minima
            losses =  explorer.container.data[current_id].loss_at_minima
            for row in range(len(minima)):
                minima_line = [current_id, row, losses[row]]
                minima_line.extend(np.asarray(minima[row]))
                minima_writer.writerow(minima_line)
        minima_file.close()

    # write corner points to file
    if FLAGS.cornerpoints_file is not None:
        header = get_trajectory_header(
            network_model.weights.get_total_dof(),
            network_model.biases.get_total_dof())
        cornerpoints_writer, cornerpoints_file = setup_csv_file(
            FLAGS.cornerpoints_file, header)
        for id in range(len(cornerpoints)):
            for row in range(len(cornerpoints[id])):
                print(cornerpoints[id][row][0])
                print(cornerpoints[id][row][1])
                print(np.asarray(cornerpoints[id][row][2]))
                cornerpoints_line = [id, cornerpoints[id][row][0], cornerpoints[id][row][1]]
                cornerpoints_line.extend(np.asarray(cornerpoints[id][row][2]))
                print(cornerpoints_line)
                cornerpoints_writer.writerow(cornerpoints_line)
        cornerpoints_file.close()

    network_model.finish()

    rt.set_overall_time(time.process_time() - time_zero)

if __name__ == '__main__':
    FLAGS, unparsed = parse_parameters()

    react_to_common_options(FLAGS, unparsed)
    react_to_sampler_options(FLAGS, unparsed)

    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
