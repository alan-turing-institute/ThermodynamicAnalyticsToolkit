<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="AsciiDoc 8.6.9">
<meta name="description" content="TATi is a software suite based on tensorflow that brings enhanced sampling methods based on Langevin Dynamics and Hamiltonian Dynamics to neural network training.">
<meta name="keywords" content="neural networks, loss, loss manifold, sampling, exploration">
<title>Thermodynamic Analytics Toolkit (TATi)</title>
<style type="text/css">
/* Shared CSS for AsciiDoc xhtml11 and html5 backends */

/* Default font. */
body {
  font-family: Georgia,serif;
}

/* Title font. */
h1, h2, h3, h4, h5, h6,
div.title, caption.title,
thead, p.table.header,
#toctitle,
#author, #revnumber, #revdate, #revremark,
#footer {
  font-family: Arial,Helvetica,sans-serif;
}

body {
  margin: 1em 5% 1em 5%;
}

a {
  color: blue;
  text-decoration: underline;
}
a:visited {
  color: fuchsia;
}

em {
  font-style: italic;
  color: navy;
}

strong {
  font-weight: bold;
  color: #083194;
}

h1, h2, h3, h4, h5, h6 {
  color: #527bbd;
  margin-top: 1.2em;
  margin-bottom: 0.5em;
  line-height: 1.3;
}

h1, h2, h3 {
  border-bottom: 2px solid silver;
}
h2 {
  padding-top: 0.5em;
}
h3 {
  float: left;
}
h3 + * {
  clear: left;
}
h5 {
  font-size: 1.0em;
}

div.sectionbody {
  margin-left: 0;
}

hr {
  border: 1px solid silver;
}

p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

ul, ol, li > p {
  margin-top: 0;
}
ul > li     { color: #aaa; }
ul > li > * { color: black; }

.monospaced, code, pre {
  font-family: "Courier New", Courier, monospace;
  font-size: inherit;
  color: navy;
  padding: 0;
  margin: 0;
}
pre {
  white-space: pre-wrap;
}

#author {
  color: #527bbd;
  font-weight: bold;
  font-size: 1.1em;
}
#email {
}
#revnumber, #revdate, #revremark {
}

#footer {
  font-size: small;
  border-top: 2px solid silver;
  padding-top: 0.5em;
  margin-top: 4.0em;
}
#footer-text {
  float: left;
  padding-bottom: 0.5em;
}
#footer-badges {
  float: right;
  padding-bottom: 0.5em;
}

#preamble {
  margin-top: 1.5em;
  margin-bottom: 1.5em;
}
div.imageblock, div.exampleblock, div.verseblock,
div.quoteblock, div.literalblock, div.listingblock, div.sidebarblock,
div.admonitionblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.admonitionblock {
  margin-top: 2.0em;
  margin-bottom: 2.0em;
  margin-right: 10%;
  color: #606060;
}

div.content { /* Block element content. */
  padding: 0;
}

/* Block element titles. */
div.title, caption.title {
  color: #527bbd;
  font-weight: bold;
  text-align: left;
  margin-top: 1.0em;
  margin-bottom: 0.5em;
}
div.title + * {
  margin-top: 0;
}

td div.title:first-child {
  margin-top: 0.0em;
}
div.content div.title:first-child {
  margin-top: 0.0em;
}
div.content + div.title {
  margin-top: 0.0em;
}

div.sidebarblock > div.content {
  background: #ffffee;
  border: 1px solid #dddddd;
  border-left: 4px solid #f0f0f0;
  padding: 0.5em;
}

div.listingblock > div.content {
  border: 1px solid #dddddd;
  border-left: 5px solid #f0f0f0;
  background: #f8f8f8;
  padding: 0.5em;
}

div.quoteblock, div.verseblock {
  padding-left: 1.0em;
  margin-left: 1.0em;
  margin-right: 10%;
  border-left: 5px solid #f0f0f0;
  color: #888;
}

div.quoteblock > div.attribution {
  padding-top: 0.5em;
  text-align: right;
}

div.verseblock > pre.content {
  font-family: inherit;
  font-size: inherit;
}
div.verseblock > div.attribution {
  padding-top: 0.75em;
  text-align: left;
}
/* DEPRECATED: Pre version 8.2.7 verse style literal block. */
div.verseblock + div.attribution {
  text-align: left;
}

div.admonitionblock .icon {
  vertical-align: top;
  font-size: 1.1em;
  font-weight: bold;
  text-decoration: underline;
  color: #527bbd;
  padding-right: 0.5em;
}
div.admonitionblock td.content {
  padding-left: 0.5em;
  border-left: 3px solid #dddddd;
}

div.exampleblock > div.content {
  border-left: 3px solid #dddddd;
  padding-left: 0.5em;
}

div.imageblock div.content { padding-left: 0; }
span.image img { border-style: none; vertical-align: text-bottom; }
a.image:visited { color: white; }

dl {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
dt {
  margin-top: 0.5em;
  margin-bottom: 0;
  font-style: normal;
  color: navy;
}
dd > *:first-child {
  margin-top: 0.1em;
}

ul, ol {
    list-style-position: outside;
}
ol.arabic {
  list-style-type: decimal;
}
ol.loweralpha {
  list-style-type: lower-alpha;
}
ol.upperalpha {
  list-style-type: upper-alpha;
}
ol.lowerroman {
  list-style-type: lower-roman;
}
ol.upperroman {
  list-style-type: upper-roman;
}

div.compact ul, div.compact ol,
div.compact p, div.compact p,
div.compact div, div.compact div {
  margin-top: 0.1em;
  margin-bottom: 0.1em;
}

tfoot {
  font-weight: bold;
}
td > div.verse {
  white-space: pre;
}

div.hdlist {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
div.hdlist tr {
  padding-bottom: 15px;
}
dt.hdlist1.strong, td.hdlist1.strong {
  font-weight: bold;
}
td.hdlist1 {
  vertical-align: top;
  font-style: normal;
  padding-right: 0.8em;
  color: navy;
}
td.hdlist2 {
  vertical-align: top;
}
div.hdlist.compact tr {
  margin: 0;
  padding-bottom: 0;
}

.comment {
  background: yellow;
}

.footnote, .footnoteref {
  font-size: 0.8em;
}

span.footnote, span.footnoteref {
  vertical-align: super;
}

#footnotes {
  margin: 20px 0 20px 0;
  padding: 7px 0 0 0;
}

#footnotes div.footnote {
  margin: 0 0 5px 0;
}

#footnotes hr {
  border: none;
  border-top: 1px solid silver;
  height: 1px;
  text-align: left;
  margin-left: 0;
  width: 20%;
  min-width: 100px;
}

div.colist td {
  padding-right: 0.5em;
  padding-bottom: 0.3em;
  vertical-align: top;
}
div.colist td img {
  margin-top: 0.3em;
}

@media print {
  #footer-badges { display: none; }
}

#toc {
  margin-bottom: 2.5em;
}

#toctitle {
  color: #527bbd;
  font-size: 1.1em;
  font-weight: bold;
  margin-top: 1.0em;
  margin-bottom: 0.1em;
}

div.toclevel0, div.toclevel1, div.toclevel2, div.toclevel3, div.toclevel4 {
  margin-top: 0;
  margin-bottom: 0;
}
div.toclevel2 {
  margin-left: 2em;
  font-size: 0.9em;
}
div.toclevel3 {
  margin-left: 4em;
  font-size: 0.9em;
}
div.toclevel4 {
  margin-left: 6em;
  font-size: 0.9em;
}

span.aqua { color: aqua; }
span.black { color: black; }
span.blue { color: blue; }
span.fuchsia { color: fuchsia; }
span.gray { color: gray; }
span.green { color: green; }
span.lime { color: lime; }
span.maroon { color: maroon; }
span.navy { color: navy; }
span.olive { color: olive; }
span.purple { color: purple; }
span.red { color: red; }
span.silver { color: silver; }
span.teal { color: teal; }
span.white { color: white; }
span.yellow { color: yellow; }

span.aqua-background { background: aqua; }
span.black-background { background: black; }
span.blue-background { background: blue; }
span.fuchsia-background { background: fuchsia; }
span.gray-background { background: gray; }
span.green-background { background: green; }
span.lime-background { background: lime; }
span.maroon-background { background: maroon; }
span.navy-background { background: navy; }
span.olive-background { background: olive; }
span.purple-background { background: purple; }
span.red-background { background: red; }
span.silver-background { background: silver; }
span.teal-background { background: teal; }
span.white-background { background: white; }
span.yellow-background { background: yellow; }

span.big { font-size: 2em; }
span.small { font-size: 0.6em; }

span.underline { text-decoration: underline; }
span.overline { text-decoration: overline; }
span.line-through { text-decoration: line-through; }

div.unbreakable { page-break-inside: avoid; }


/*
 * xhtml11 specific
 *
 * */

div.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.tableblock > table {
  border: 3px solid #527bbd;
}
thead, p.table.header {
  font-weight: bold;
  color: #527bbd;
}
p.table {
  margin-top: 0;
}
/* Because the table frame attribute is overriden by CSS in most browsers. */
div.tableblock > table[frame="void"] {
  border-style: none;
}
div.tableblock > table[frame="hsides"] {
  border-left-style: none;
  border-right-style: none;
}
div.tableblock > table[frame="vsides"] {
  border-top-style: none;
  border-bottom-style: none;
}


/*
 * html5 specific
 *
 * */

table.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
thead, p.tableblock.header {
  font-weight: bold;
  color: #527bbd;
}
p.tableblock {
  margin-top: 0;
}
table.tableblock {
  border-width: 3px;
  border-spacing: 0px;
  border-style: solid;
  border-color: #527bbd;
  border-collapse: collapse;
}
th.tableblock, td.tableblock {
  border-width: 1px;
  padding: 4px;
  border-style: solid;
  border-color: #527bbd;
}

table.tableblock.frame-topbot {
  border-left-style: hidden;
  border-right-style: hidden;
}
table.tableblock.frame-sides {
  border-top-style: hidden;
  border-bottom-style: hidden;
}
table.tableblock.frame-none {
  border-style: hidden;
}

th.tableblock.halign-left, td.tableblock.halign-left {
  text-align: left;
}
th.tableblock.halign-center, td.tableblock.halign-center {
  text-align: center;
}
th.tableblock.halign-right, td.tableblock.halign-right {
  text-align: right;
}

th.tableblock.valign-top, td.tableblock.valign-top {
  vertical-align: top;
}
th.tableblock.valign-middle, td.tableblock.valign-middle {
  vertical-align: middle;
}
th.tableblock.valign-bottom, td.tableblock.valign-bottom {
  vertical-align: bottom;
}


/*
 * manpage specific
 *
 * */

body.manpage h1 {
  padding-top: 0.5em;
  padding-bottom: 0.5em;
  border-top: 2px solid silver;
  border-bottom: 2px solid silver;
}
body.manpage h2 {
  border-style: none;
}
body.manpage div.sectionbody {
  margin-left: 3em;
}

@media print {
  body.manpage div#toc { display: none; }
}


@media screen {
  body {
    max-width: 50em; /* approximately 80 characters wide */
    margin-left: 16em;
  }

  #toc {
    position: fixed;
    top: 0;
    left: 0;
    bottom: 0;
    width: 13em;
    padding: 0.5em;
    padding-bottom: 1.5em;
    margin: 0;
    overflow: auto;
    border-right: 3px solid #f8f8f8;
    background-color: white;
  }

  #toc .toclevel1 {
    margin-top: 0.5em;
  }

  #toc .toclevel2 {
    margin-top: 0.25em;
    display: list-item;
    color: #aaaaaa;
  }

  #toctitle {
    margin-top: 0.5em;
  }
}
</style>
<script type="text/javascript">
/*<![CDATA[*/
var asciidoc = {  // Namespace.

/////////////////////////////////////////////////////////////////////
// Table Of Contents generator
/////////////////////////////////////////////////////////////////////

/* Author: Mihai Bazon, September 2002
 * http://students.infoiasi.ro/~mishoo
 *
 * Table Of Content generator
 * Version: 0.4
 *
 * Feel free to use this script under the terms of the GNU General Public
 * License, as long as you do not remove or alter this notice.
 */

 /* modified by Troy D. Hanson, September 2006. License: GPL */
 /* modified by Stuart Rackham, 2006, 2009. License: GPL */

// toclevels = 1..4.
toc: function (toclevels) {

  function getText(el) {
    var text = "";
    for (var i = el.firstChild; i != null; i = i.nextSibling) {
      if (i.nodeType == 3 /* Node.TEXT_NODE */) // IE doesn't speak constants.
        text += i.data;
      else if (i.firstChild != null)
        text += getText(i);
    }
    return text;
  }

  function TocEntry(el, text, toclevel) {
    this.element = el;
    this.text = text;
    this.toclevel = toclevel;
  }

  function tocEntries(el, toclevels) {
    var result = new Array;
    var re = new RegExp('[hH]([1-'+(toclevels+1)+'])');
    // Function that scans the DOM tree for header elements (the DOM2
    // nodeIterator API would be a better technique but not supported by all
    // browsers).
    var iterate = function (el) {
      for (var i = el.firstChild; i != null; i = i.nextSibling) {
        if (i.nodeType == 1 /* Node.ELEMENT_NODE */) {
          var mo = re.exec(i.tagName);
          if (mo && (i.getAttribute("class") || i.getAttribute("className")) != "float") {
            result[result.length] = new TocEntry(i, getText(i), mo[1]-1);
          }
          iterate(i);
        }
      }
    }
    iterate(el);
    return result;
  }

  var toc = document.getElementById("toc");
  if (!toc) {
    return;
  }

  // Delete existing TOC entries in case we're reloading the TOC.
  var tocEntriesToRemove = [];
  var i;
  for (i = 0; i < toc.childNodes.length; i++) {
    var entry = toc.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div'
     && entry.getAttribute("class")
     && entry.getAttribute("class").match(/^toclevel/))
      tocEntriesToRemove.push(entry);
  }
  for (i = 0; i < tocEntriesToRemove.length; i++) {
    toc.removeChild(tocEntriesToRemove[i]);
  }

  // Rebuild TOC entries.
  var entries = tocEntries(document.getElementById("content"), toclevels);
  for (var i = 0; i < entries.length; ++i) {
    var entry = entries[i];
    if (entry.element.id == "")
      entry.element.id = "_toc_" + i;
    var a = document.createElement("a");
    a.href = "#" + entry.element.id;
    a.appendChild(document.createTextNode(entry.text));
    var div = document.createElement("div");
    div.appendChild(a);
    div.className = "toclevel" + entry.toclevel;
    toc.appendChild(div);
  }
  if (entries.length == 0)
    toc.parentNode.removeChild(toc);
},


/////////////////////////////////////////////////////////////////////
// Footnotes generator
/////////////////////////////////////////////////////////////////////

/* Based on footnote generation code from:
 * http://www.brandspankingnew.net/archive/2005/07/format_footnote.html
 */

footnotes: function () {
  // Delete existing footnote entries in case we're reloading the footnodes.
  var i;
  var noteholder = document.getElementById("footnotes");
  if (!noteholder) {
    return;
  }
  var entriesToRemove = [];
  for (i = 0; i < noteholder.childNodes.length; i++) {
    var entry = noteholder.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div' && entry.getAttribute("class") == "footnote")
      entriesToRemove.push(entry);
  }
  for (i = 0; i < entriesToRemove.length; i++) {
    noteholder.removeChild(entriesToRemove[i]);
  }

  // Rebuild footnote entries.
  var cont = document.getElementById("content");
  var spans = cont.getElementsByTagName("span");
  var refs = {};
  var n = 0;
  for (i=0; i<spans.length; i++) {
    if (spans[i].className == "footnote") {
      n++;
      var note = spans[i].getAttribute("data-note");
      if (!note) {
        // Use [\s\S] in place of . so multi-line matches work.
        // Because JavaScript has no s (dotall) regex flag.
        note = spans[i].innerHTML.match(/\s*\[([\s\S]*)]\s*/)[1];
        spans[i].innerHTML =
          "[<a id='_footnoteref_" + n + "' href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
        spans[i].setAttribute("data-note", note);
      }
      noteholder.innerHTML +=
        "<div class='footnote' id='_footnote_" + n + "'>" +
        "<a href='#_footnoteref_" + n + "' title='Return to text'>" +
        n + "</a>. " + note + "</div>";
      var id =spans[i].getAttribute("id");
      if (id != null) refs["#"+id] = n;
    }
  }
  if (n == 0)
    noteholder.parentNode.removeChild(noteholder);
  else {
    // Process footnoterefs.
    for (i=0; i<spans.length; i++) {
      if (spans[i].className == "footnoteref") {
        var href = spans[i].getElementsByTagName("a")[0].getAttribute("href");
        href = href.match(/#.*/)[0];  // Because IE return full URL.
        n = refs[href];
        spans[i].innerHTML =
          "[<a href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
      }
    }
  }
},

install: function(toclevels) {
  var timerId;

  function reinstall() {
    asciidoc.footnotes();
    if (toclevels) {
      asciidoc.toc(toclevels);
    }
  }

  function reinstallAndRemoveTimer() {
    clearInterval(timerId);
    reinstall();
  }

  timerId = setInterval(reinstall, 500);
  if (document.addEventListener)
    document.addEventListener("DOMContentLoaded", reinstallAndRemoveTimer, false);
  else
    window.onload = reinstallAndRemoveTimer;
}

}
asciidoc.install(2);
/*]]>*/
</script>
<script type="text/javascript">
/*<![CDATA[*/
/*
LaTeXMathML.js
==============

This file, in this form, is due to Douglas Woodall, June 2006.
It contains JavaScript functions to convert (most simple) LaTeX
math notation to Presentation MathML.  It was obtained by
downloading the file ASCIIMathML.js from
	http://www1.chapman.edu/~jipsen/mathml/asciimathdownload/
and modifying it so that it carries out ONLY those conversions
that would be carried out in LaTeX.  A description of the original
file, with examples, can be found at
	www1.chapman.edu/~jipsen/mathml/asciimath.html
	ASCIIMathML: Math on the web for everyone

Here is the header notice from the original file:

ASCIIMathML.js
==============
This file contains JavaScript functions to convert ASCII math notation
to Presentation MathML. The conversion is done while the (X)HTML page
loads, and should work with Firefox/Mozilla/Netscape 7+ and Internet
Explorer 6+MathPlayer (http://www.dessci.com/en/products/mathplayer/).
Just add the next line to your (X)HTML page with this file in the same folder:
This is a convenient and inexpensive solution for authoring MathML.

Version 1.4.7 Dec 15, 2005, (c) Peter Jipsen http://www.chapman.edu/~jipsen
Latest version at http://www.chapman.edu/~jipsen/mathml/ASCIIMathML.js
For changes see http://www.chapman.edu/~jipsen/mathml/asciimathchanges.txt
If you use it on a webpage, please send the URL to jipsen@chapman.edu

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
General Public License (at http://www.gnu.org/copyleft/gpl.html)
for more details.

LaTeXMathML.js (ctd)
==============

The instructions for use are the same as for the original
ASCIIMathML.js, except that of course the line you add to your
file should be
Or use absolute path names if the file is not in the same folder
as your (X)HTML page.
*/

var checkForMathML = true;   // check if browser can display MathML
var notifyIfNoMathML = true; // display note if no MathML capability
var alertIfNoMathML = false;  // show alert box if no MathML capability
// was "red":
var mathcolor = "";	     // change it to "" (to inherit) or any other color
// was "serif":
var mathfontfamily = "";      // change to "" to inherit (works in IE)
                              // or another family (e.g. "arial")
var showasciiformulaonhover = true; // helps students learn ASCIIMath
/*
// Commented out by DRW -- not now used -- see DELIMITERS (twice) near the end
var displaystyle = false;     // puts limits above and below large operators
var decimalsign = ".";        // change to "," if you like, beware of `(1,2)`!
var AMdelimiter1 = "`", AMescape1 = "\\\\`"; // can use other characters
var AMdelimiter2 = "$", AMescape2 = "\\\\\\$", AMdelimiter2regexp = "\\$";
var doubleblankmathdelimiter = false; // if true,  x+1  is equal to `x+1`
                                      // for IE this works only in <!--   -->
//var separatetokens;// has been removed (email me if this is a problem)
*/
var isIE = document.createElementNS==null;

if (document.getElementById==null)
  alert("This webpage requires a recent browser such as\
\nMozilla/Netscape 7+ or Internet Explorer 6+MathPlayer")

// all further global variables start with "AM"

function AMcreateElementXHTML(t) {
  if (isIE) return document.createElement(t);
  else return document.createElementNS("http://www.w3.org/1999/xhtml",t);
}

function AMnoMathMLNote() {
  var nd = AMcreateElementXHTML("h3");
  nd.setAttribute("align","center")
  nd.appendChild(AMcreateElementXHTML("p"));
  nd.appendChild(document.createTextNode("To view the "));
  var an = AMcreateElementXHTML("a");
  an.appendChild(document.createTextNode("LaTeXMathML"));
  an.setAttribute("href","http://www.maths.nott.ac.uk/personal/drw/lm.html");
  nd.appendChild(an);
  nd.appendChild(document.createTextNode(" notation use Internet Explorer 6+"));
  an = AMcreateElementXHTML("a");
  an.appendChild(document.createTextNode("MathPlayer"));
  an.setAttribute("href","http://www.dessci.com/en/products/mathplayer/download.htm");
  nd.appendChild(an);
  nd.appendChild(document.createTextNode(" or Netscape/Mozilla/Firefox"));
  nd.appendChild(AMcreateElementXHTML("p"));
  return nd;
}

function AMisMathMLavailable() {
  if (navigator.appName.slice(0,8)=="Netscape")
    if (navigator.appVersion.slice(0,1)>="5") return null;
    else return AMnoMathMLNote();
  else if (navigator.appName.slice(0,9)=="Microsoft")
    try {
        var ActiveX = new ActiveXObject("MathPlayer.Factory.1");
        return null;
    } catch (e) {
        return AMnoMathMLNote();
    }
  else return AMnoMathMLNote();
}

// character lists for Mozilla/Netscape fonts
var AMcal = [0xEF35,0x212C,0xEF36,0xEF37,0x2130,0x2131,0xEF38,0x210B,0x2110,0xEF39,0xEF3A,0x2112,0x2133,0xEF3B,0xEF3C,0xEF3D,0xEF3E,0x211B,0xEF3F,0xEF40,0xEF41,0xEF42,0xEF43,0xEF44,0xEF45,0xEF46];
var AMfrk = [0xEF5D,0xEF5E,0x212D,0xEF5F,0xEF60,0xEF61,0xEF62,0x210C,0x2111,0xEF63,0xEF64,0xEF65,0xEF66,0xEF67,0xEF68,0xEF69,0xEF6A,0x211C,0xEF6B,0xEF6C,0xEF6D,0xEF6E,0xEF6F,0xEF70,0xEF71,0x2128];
var AMbbb = [0xEF8C,0xEF8D,0x2102,0xEF8E,0xEF8F,0xEF90,0xEF91,0x210D,0xEF92,0xEF93,0xEF94,0xEF95,0xEF96,0x2115,0xEF97,0x2119,0x211A,0x211D,0xEF98,0xEF99,0xEF9A,0xEF9B,0xEF9C,0xEF9D,0xEF9E,0x2124];

var CONST = 0, UNARY = 1, BINARY = 2, INFIX = 3, LEFTBRACKET = 4,
    RIGHTBRACKET = 5, SPACE = 6, UNDEROVER = 7, DEFINITION = 8,
    TEXT = 9, BIG = 10, LONG = 11, STRETCHY = 12, MATRIX = 13; // token types

var AMsqrt = {input:"\\sqrt",	tag:"msqrt", output:"sqrt",	ttype:UNARY},
  AMroot = {input:"\\root",	tag:"mroot", output:"root",	ttype:BINARY},
  AMfrac = {input:"\\frac",	tag:"mfrac", output:"/",	ttype:BINARY},
  AMover = {input:"\\stackrel", tag:"mover", output:"stackrel", ttype:BINARY},
  AMatop = {input:"\\atop",	tag:"mfrac", output:"",		ttype:INFIX},
  AMchoose = {input:"\\choose", tag:"mfrac", output:"",		ttype:INFIX},
  AMsub  = {input:"_",		tag:"msub",  output:"_",	ttype:INFIX},
  AMsup  = {input:"^",		tag:"msup",  output:"^",	ttype:INFIX},
  AMtext = {input:"\\mathrm",	tag:"mtext", output:"text",	ttype:TEXT},
  AMmbox = {input:"\\mbox",	tag:"mtext", output:"mbox",	ttype:TEXT};

// Commented out by DRW to prevent 1/2 turning into a 2-line fraction
// AMdiv   = {input:"/",	 tag:"mfrac", output:"/",    ttype:INFIX},
// Commented out by DRW so that " prints literally in equations
// AMquote = {input:"\"",	 tag:"mtext", output:"mbox", ttype:TEXT};

var AMsymbols = [
//Greek letters
{input:"\\alpha",	tag:"mi", output:"\u03B1", ttype:CONST},
{input:"\\beta",	tag:"mi", output:"\u03B2", ttype:CONST},
{input:"\\gamma",	tag:"mi", output:"\u03B3", ttype:CONST},
{input:"\\delta",	tag:"mi", output:"\u03B4", ttype:CONST},
{input:"\\epsilon",	tag:"mi", output:"\u03B5", ttype:CONST},
{input:"\\varepsilon",  tag:"mi", output:"\u025B", ttype:CONST},
{input:"\\zeta",	tag:"mi", output:"\u03B6", ttype:CONST},
{input:"\\eta",		tag:"mi", output:"\u03B7", ttype:CONST},
{input:"\\theta",	tag:"mi", output:"\u03B8", ttype:CONST},
{input:"\\vartheta",	tag:"mi", output:"\u03D1", ttype:CONST},
{input:"\\iota",	tag:"mi", output:"\u03B9", ttype:CONST},
{input:"\\kappa",	tag:"mi", output:"\u03BA", ttype:CONST},
{input:"\\lambda",	tag:"mi", output:"\u03BB", ttype:CONST},
{input:"\\mu",		tag:"mi", output:"\u03BC", ttype:CONST},
{input:"\\nu",		tag:"mi", output:"\u03BD", ttype:CONST},
{input:"\\xi",		tag:"mi", output:"\u03BE", ttype:CONST},
{input:"\\pi",		tag:"mi", output:"\u03C0", ttype:CONST},
{input:"\\varpi",	tag:"mi", output:"\u03D6", ttype:CONST},
{input:"\\rho",		tag:"mi", output:"\u03C1", ttype:CONST},
{input:"\\varrho",	tag:"mi", output:"\u03F1", ttype:CONST},
{input:"\\varsigma",	tag:"mi", output:"\u03C2", ttype:CONST},
{input:"\\sigma",	tag:"mi", output:"\u03C3", ttype:CONST},
{input:"\\tau",		tag:"mi", output:"\u03C4", ttype:CONST},
{input:"\\upsilon",	tag:"mi", output:"\u03C5", ttype:CONST},
{input:"\\phi",		tag:"mi", output:"\u03C6", ttype:CONST},
{input:"\\varphi",	tag:"mi", output:"\u03D5", ttype:CONST},
{input:"\\chi",		tag:"mi", output:"\u03C7", ttype:CONST},
{input:"\\psi",		tag:"mi", output:"\u03C8", ttype:CONST},
{input:"\\omega",	tag:"mi", output:"\u03C9", ttype:CONST},
{input:"\\Gamma",	tag:"mo", output:"\u0393", ttype:CONST},
{input:"\\Delta",	tag:"mo", output:"\u0394", ttype:CONST},
{input:"\\Theta",	tag:"mo", output:"\u0398", ttype:CONST},
{input:"\\Lambda",	tag:"mo", output:"\u039B", ttype:CONST},
{input:"\\Xi",		tag:"mo", output:"\u039E", ttype:CONST},
{input:"\\Pi",		tag:"mo", output:"\u03A0", ttype:CONST},
{input:"\\Sigma",	tag:"mo", output:"\u03A3", ttype:CONST},
{input:"\\Upsilon",	tag:"mo", output:"\u03A5", ttype:CONST},
{input:"\\Phi",		tag:"mo", output:"\u03A6", ttype:CONST},
{input:"\\Psi",		tag:"mo", output:"\u03A8", ttype:CONST},
{input:"\\Omega",	tag:"mo", output:"\u03A9", ttype:CONST},

//fractions
{input:"\\frac12",	tag:"mo", output:"\u00BD", ttype:CONST},
{input:"\\frac14",	tag:"mo", output:"\u00BC", ttype:CONST},
{input:"\\frac34",	tag:"mo", output:"\u00BE", ttype:CONST},
{input:"\\frac13",	tag:"mo", output:"\u2153", ttype:CONST},
{input:"\\frac23",	tag:"mo", output:"\u2154", ttype:CONST},
{input:"\\frac15",	tag:"mo", output:"\u2155", ttype:CONST},
{input:"\\frac25",	tag:"mo", output:"\u2156", ttype:CONST},
{input:"\\frac35",	tag:"mo", output:"\u2157", ttype:CONST},
{input:"\\frac45",	tag:"mo", output:"\u2158", ttype:CONST},
{input:"\\frac16",	tag:"mo", output:"\u2159", ttype:CONST},
{input:"\\frac56",	tag:"mo", output:"\u215A", ttype:CONST},
{input:"\\frac18",	tag:"mo", output:"\u215B", ttype:CONST},
{input:"\\frac38",	tag:"mo", output:"\u215C", ttype:CONST},
{input:"\\frac58",	tag:"mo", output:"\u215D", ttype:CONST},
{input:"\\frac78",	tag:"mo", output:"\u215E", ttype:CONST},

//binary operation symbols
{input:"\\pm",		tag:"mo", output:"\u00B1", ttype:CONST},
{input:"\\mp",		tag:"mo", output:"\u2213", ttype:CONST},
{input:"\\triangleleft",tag:"mo", output:"\u22B2", ttype:CONST},
{input:"\\triangleright",tag:"mo",output:"\u22B3", ttype:CONST},
{input:"\\cdot",	tag:"mo", output:"\u22C5", ttype:CONST},
{input:"\\star",	tag:"mo", output:"\u22C6", ttype:CONST},
{input:"\\ast",		tag:"mo", output:"\u002A", ttype:CONST},
{input:"\\times",	tag:"mo", output:"\u00D7", ttype:CONST},
{input:"\\div",		tag:"mo", output:"\u00F7", ttype:CONST},
{input:"\\circ",	tag:"mo", output:"\u2218", ttype:CONST},
//{input:"\\bullet",	  tag:"mo", output:"\u2219", ttype:CONST},
{input:"\\bullet",	tag:"mo", output:"\u2022", ttype:CONST},
{input:"\\oplus",	tag:"mo", output:"\u2295", ttype:CONST},
{input:"\\ominus",	tag:"mo", output:"\u2296", ttype:CONST},
{input:"\\otimes",	tag:"mo", output:"\u2297", ttype:CONST},
{input:"\\bigcirc",	tag:"mo", output:"\u25CB", ttype:CONST},
{input:"\\oslash",	tag:"mo", output:"\u2298", ttype:CONST},
{input:"\\odot",	tag:"mo", output:"\u2299", ttype:CONST},
{input:"\\land",	tag:"mo", output:"\u2227", ttype:CONST},
{input:"\\wedge",	tag:"mo", output:"\u2227", ttype:CONST},
{input:"\\lor",		tag:"mo", output:"\u2228", ttype:CONST},
{input:"\\vee",		tag:"mo", output:"\u2228", ttype:CONST},
{input:"\\cap",		tag:"mo", output:"\u2229", ttype:CONST},
{input:"\\cup",		tag:"mo", output:"\u222A", ttype:CONST},
{input:"\\sqcap",	tag:"mo", output:"\u2293", ttype:CONST},
{input:"\\sqcup",	tag:"mo", output:"\u2294", ttype:CONST},
{input:"\\uplus",	tag:"mo", output:"\u228E", ttype:CONST},
{input:"\\amalg",	tag:"mo", output:"\u2210", ttype:CONST},
{input:"\\bigtriangleup",tag:"mo",output:"\u25B3", ttype:CONST},
{input:"\\bigtriangledown",tag:"mo",output:"\u25BD", ttype:CONST},
{input:"\\dag",		tag:"mo", output:"\u2020", ttype:CONST},
{input:"\\dagger",	tag:"mo", output:"\u2020", ttype:CONST},
{input:"\\ddag",	tag:"mo", output:"\u2021", ttype:CONST},
{input:"\\ddagger",	tag:"mo", output:"\u2021", ttype:CONST},
{input:"\\lhd",		tag:"mo", output:"\u22B2", ttype:CONST},
{input:"\\rhd",		tag:"mo", output:"\u22B3", ttype:CONST},
{input:"\\unlhd",	tag:"mo", output:"\u22B4", ttype:CONST},
{input:"\\unrhd",	tag:"mo", output:"\u22B5", ttype:CONST},


//BIG Operators
{input:"\\sum",		tag:"mo", output:"\u2211", ttype:UNDEROVER},
{input:"\\prod",	tag:"mo", output:"\u220F", ttype:UNDEROVER},
{input:"\\bigcap",	tag:"mo", output:"\u22C2", ttype:UNDEROVER},
{input:"\\bigcup",	tag:"mo", output:"\u22C3", ttype:UNDEROVER},
{input:"\\bigwedge",	tag:"mo", output:"\u22C0", ttype:UNDEROVER},
{input:"\\bigvee",	tag:"mo", output:"\u22C1", ttype:UNDEROVER},
{input:"\\bigsqcap",	tag:"mo", output:"\u2A05", ttype:UNDEROVER},
{input:"\\bigsqcup",	tag:"mo", output:"\u2A06", ttype:UNDEROVER},
{input:"\\coprod",	tag:"mo", output:"\u2210", ttype:UNDEROVER},
{input:"\\bigoplus",	tag:"mo", output:"\u2A01", ttype:UNDEROVER},
{input:"\\bigotimes",	tag:"mo", output:"\u2A02", ttype:UNDEROVER},
{input:"\\bigodot",	tag:"mo", output:"\u2A00", ttype:UNDEROVER},
{input:"\\biguplus",	tag:"mo", output:"\u2A04", ttype:UNDEROVER},
{input:"\\int",		tag:"mo", output:"\u222B", ttype:CONST},
{input:"\\oint",	tag:"mo", output:"\u222E", ttype:CONST},

//binary relation symbols
{input:":=",		tag:"mo", output:":=",	   ttype:CONST},
{input:"\\lt",		tag:"mo", output:"<",	   ttype:CONST},
{input:"\\gt",		tag:"mo", output:">",	   ttype:CONST},
{input:"\\ne",		tag:"mo", output:"\u2260", ttype:CONST},
{input:"\\neq",		tag:"mo", output:"\u2260", ttype:CONST},
{input:"\\le",		tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\leq",		tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\leqslant",	tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\ge",		tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\geq",		tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\geqslant",	tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\equiv",	tag:"mo", output:"\u2261", ttype:CONST},
{input:"\\ll",		tag:"mo", output:"\u226A", ttype:CONST},
{input:"\\gg",		tag:"mo", output:"\u226B", ttype:CONST},
{input:"\\doteq",	tag:"mo", output:"\u2250", ttype:CONST},
{input:"\\prec",	tag:"mo", output:"\u227A", ttype:CONST},
{input:"\\succ",	tag:"mo", output:"\u227B", ttype:CONST},
{input:"\\preceq",	tag:"mo", output:"\u227C", ttype:CONST},
{input:"\\succeq",	tag:"mo", output:"\u227D", ttype:CONST},
{input:"\\subset",	tag:"mo", output:"\u2282", ttype:CONST},
{input:"\\supset",	tag:"mo", output:"\u2283", ttype:CONST},
{input:"\\subseteq",	tag:"mo", output:"\u2286", ttype:CONST},
{input:"\\supseteq",	tag:"mo", output:"\u2287", ttype:CONST},
{input:"\\sqsubset",	tag:"mo", output:"\u228F", ttype:CONST},
{input:"\\sqsupset",	tag:"mo", output:"\u2290", ttype:CONST},
{input:"\\sqsubseteq",  tag:"mo", output:"\u2291", ttype:CONST},
{input:"\\sqsupseteq",  tag:"mo", output:"\u2292", ttype:CONST},
{input:"\\sim",		tag:"mo", output:"\u223C", ttype:CONST},
{input:"\\simeq",	tag:"mo", output:"\u2243", ttype:CONST},
{input:"\\approx",	tag:"mo", output:"\u2248", ttype:CONST},
{input:"\\cong",	tag:"mo", output:"\u2245", ttype:CONST},
{input:"\\Join",	tag:"mo", output:"\u22C8", ttype:CONST},
{input:"\\bowtie",	tag:"mo", output:"\u22C8", ttype:CONST},
{input:"\\in",		tag:"mo", output:"\u2208", ttype:CONST},
{input:"\\ni",		tag:"mo", output:"\u220B", ttype:CONST},
{input:"\\owns",	tag:"mo", output:"\u220B", ttype:CONST},
{input:"\\propto",	tag:"mo", output:"\u221D", ttype:CONST},
{input:"\\vdash",	tag:"mo", output:"\u22A2", ttype:CONST},
{input:"\\dashv",	tag:"mo", output:"\u22A3", ttype:CONST},
{input:"\\models",	tag:"mo", output:"\u22A8", ttype:CONST},
{input:"\\perp",	tag:"mo", output:"\u22A5", ttype:CONST},
{input:"\\smile",	tag:"mo", output:"\u2323", ttype:CONST},
{input:"\\frown",	tag:"mo", output:"\u2322", ttype:CONST},
{input:"\\asymp",	tag:"mo", output:"\u224D", ttype:CONST},
{input:"\\notin",	tag:"mo", output:"\u2209", ttype:CONST},

//matrices
{input:"\\begin{eqnarray}",	output:"X",	ttype:MATRIX, invisible:true},
{input:"\\begin{array}",	output:"X",	ttype:MATRIX, invisible:true},
{input:"\\\\",			output:"}&{",	ttype:DEFINITION},
{input:"\\end{eqnarray}",	output:"}}",	ttype:DEFINITION},
{input:"\\end{array}",		output:"}}",	ttype:DEFINITION},

//grouping and literal brackets -- ieval is for IE
{input:"\\big",	   tag:"mo", output:"X", atval:"1.2", ieval:"2.2", ttype:BIG},
{input:"\\Big",	   tag:"mo", output:"X", atval:"1.6", ieval:"2.6", ttype:BIG},
{input:"\\bigg",   tag:"mo", output:"X", atval:"2.2", ieval:"3.2", ttype:BIG},
{input:"\\Bigg",   tag:"mo", output:"X", atval:"2.9", ieval:"3.9", ttype:BIG},
{input:"\\left",   tag:"mo", output:"X", ttype:LEFTBRACKET},
{input:"\\right",  tag:"mo", output:"X", ttype:RIGHTBRACKET},
{input:"{",	   output:"{", ttype:LEFTBRACKET,  invisible:true},
{input:"}",	   output:"}", ttype:RIGHTBRACKET, invisible:true},

{input:"(",	   tag:"mo", output:"(",      atval:"1", ttype:STRETCHY},
{input:"[",	   tag:"mo", output:"[",      atval:"1", ttype:STRETCHY},
{input:"\\lbrack", tag:"mo", output:"[",      atval:"1", ttype:STRETCHY},
{input:"\\{",	   tag:"mo", output:"{",      atval:"1", ttype:STRETCHY},
{input:"\\lbrace", tag:"mo", output:"{",      atval:"1", ttype:STRETCHY},
{input:"\\langle", tag:"mo", output:"\u2329", atval:"1", ttype:STRETCHY},
{input:"\\lfloor", tag:"mo", output:"\u230A", atval:"1", ttype:STRETCHY},
{input:"\\lceil",  tag:"mo", output:"\u2308", atval:"1", ttype:STRETCHY},

// rtag:"mi" causes space to be inserted before a following sin, cos, etc.
// (see function AMparseExpr() )
{input:")",	  tag:"mo",output:")",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"]",	  tag:"mo",output:"]",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rbrack",tag:"mo",output:"]",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\}",	  tag:"mo",output:"}",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rbrace",tag:"mo",output:"}",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rangle",tag:"mo",output:"\u232A", rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rfloor",tag:"mo",output:"\u230B", rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rceil", tag:"mo",output:"\u2309", rtag:"mi",atval:"1",ttype:STRETCHY},

// "|", "\\|", "\\vert" and "\\Vert" modified later: lspace = rspace = 0em
{input:"|",		tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\|",		tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"\\vert",	tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\Vert",	tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"\\mid",		tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\parallel",	tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"/",		tag:"mo", output:"/",	atval:"1.01", ttype:STRETCHY},
{input:"\\backslash",	tag:"mo", output:"\u2216", atval:"1", ttype:STRETCHY},
{input:"\\setminus",	tag:"mo", output:"\\",	   ttype:CONST},

//miscellaneous symbols
{input:"\\!",	  tag:"mspace", atname:"width", atval:"-0.167em", ttype:SPACE},
{input:"\\,",	  tag:"mspace", atname:"width", atval:"0.167em", ttype:SPACE},
{input:"\\>",	  tag:"mspace", atname:"width", atval:"0.222em", ttype:SPACE},
{input:"\\:",	  tag:"mspace", atname:"width", atval:"0.222em", ttype:SPACE},
{input:"\\;",	  tag:"mspace", atname:"width", atval:"0.278em", ttype:SPACE},
{input:"~",	  tag:"mspace", atname:"width", atval:"0.333em", ttype:SPACE},
{input:"\\quad",  tag:"mspace", atname:"width", atval:"1em", ttype:SPACE},
{input:"\\qquad", tag:"mspace", atname:"width", atval:"2em", ttype:SPACE},
//{input:"{}",		  tag:"mo", output:"\u200B", ttype:CONST}, // zero-width
{input:"\\prime",	tag:"mo", output:"\u2032", ttype:CONST},
{input:"'",		tag:"mo", output:"\u02B9", ttype:CONST},
{input:"''",		tag:"mo", output:"\u02BA", ttype:CONST},
{input:"'''",		tag:"mo", output:"\u2034", ttype:CONST},
{input:"''''",		tag:"mo", output:"\u2057", ttype:CONST},
{input:"\\ldots",	tag:"mo", output:"\u2026", ttype:CONST},
{input:"\\cdots",	tag:"mo", output:"\u22EF", ttype:CONST},
{input:"\\vdots",	tag:"mo", output:"\u22EE", ttype:CONST},
{input:"\\ddots",	tag:"mo", output:"\u22F1", ttype:CONST},
{input:"\\forall",	tag:"mo", output:"\u2200", ttype:CONST},
{input:"\\exists",	tag:"mo", output:"\u2203", ttype:CONST},
{input:"\\Re",		tag:"mo", output:"\u211C", ttype:CONST},
{input:"\\Im",		tag:"mo", output:"\u2111", ttype:CONST},
{input:"\\aleph",	tag:"mo", output:"\u2135", ttype:CONST},
{input:"\\hbar",	tag:"mo", output:"\u210F", ttype:CONST},
{input:"\\ell",		tag:"mo", output:"\u2113", ttype:CONST},
{input:"\\wp",		tag:"mo", output:"\u2118", ttype:CONST},
{input:"\\emptyset",	tag:"mo", output:"\u2205", ttype:CONST},
{input:"\\infty",	tag:"mo", output:"\u221E", ttype:CONST},
{input:"\\surd",	tag:"mo", output:"\\sqrt{}", ttype:DEFINITION},
{input:"\\partial",	tag:"mo", output:"\u2202", ttype:CONST},
{input:"\\nabla",	tag:"mo", output:"\u2207", ttype:CONST},
{input:"\\triangle",	tag:"mo", output:"\u25B3", ttype:CONST},
{input:"\\therefore",	tag:"mo", output:"\u2234", ttype:CONST},
{input:"\\angle",	tag:"mo", output:"\u2220", ttype:CONST},
//{input:"\\\\ ",	  tag:"mo", output:"\u00A0", ttype:CONST},
{input:"\\diamond",	tag:"mo", output:"\u22C4", ttype:CONST},
//{input:"\\Diamond",	  tag:"mo", output:"\u25CA", ttype:CONST},
{input:"\\Diamond",	tag:"mo", output:"\u25C7", ttype:CONST},
{input:"\\neg",		tag:"mo", output:"\u00AC", ttype:CONST},
{input:"\\lnot",	tag:"mo", output:"\u00AC", ttype:CONST},
{input:"\\bot",		tag:"mo", output:"\u22A5", ttype:CONST},
{input:"\\top",		tag:"mo", output:"\u22A4", ttype:CONST},
{input:"\\square",	tag:"mo", output:"\u25AB", ttype:CONST},
{input:"\\Box",		tag:"mo", output:"\u25A1", ttype:CONST},
{input:"\\wr",		tag:"mo", output:"\u2240", ttype:CONST},

//standard functions
//Note UNDEROVER *must* have tag:"mo" to work properly
{input:"\\arccos", tag:"mi", output:"arccos", ttype:UNARY, func:true},
{input:"\\arcsin", tag:"mi", output:"arcsin", ttype:UNARY, func:true},
{input:"\\arctan", tag:"mi", output:"arctan", ttype:UNARY, func:true},
{input:"\\arg",	   tag:"mi", output:"arg",    ttype:UNARY, func:true},
{input:"\\cos",	   tag:"mi", output:"cos",    ttype:UNARY, func:true},
{input:"\\cosh",   tag:"mi", output:"cosh",   ttype:UNARY, func:true},
{input:"\\cot",	   tag:"mi", output:"cot",    ttype:UNARY, func:true},
{input:"\\coth",   tag:"mi", output:"coth",   ttype:UNARY, func:true},
{input:"\\csc",	   tag:"mi", output:"csc",    ttype:UNARY, func:true},
{input:"\\deg",	   tag:"mi", output:"deg",    ttype:UNARY, func:true},
{input:"\\det",	   tag:"mi", output:"det",    ttype:UNARY, func:true},
{input:"\\dim",	   tag:"mi", output:"dim",    ttype:UNARY, func:true}, //CONST?
{input:"\\exp",	   tag:"mi", output:"exp",    ttype:UNARY, func:true},
{input:"\\gcd",	   tag:"mi", output:"gcd",    ttype:UNARY, func:true}, //CONST?
{input:"\\hom",	   tag:"mi", output:"hom",    ttype:UNARY, func:true},
{input:"\\inf",	      tag:"mo", output:"inf",	 ttype:UNDEROVER},
{input:"\\ker",	   tag:"mi", output:"ker",    ttype:UNARY, func:true},
{input:"\\lg",	   tag:"mi", output:"lg",     ttype:UNARY, func:true},
{input:"\\lim",	      tag:"mo", output:"lim",	 ttype:UNDEROVER},
{input:"\\liminf",    tag:"mo", output:"liminf", ttype:UNDEROVER},
{input:"\\limsup",    tag:"mo", output:"limsup", ttype:UNDEROVER},
{input:"\\ln",	   tag:"mi", output:"ln",     ttype:UNARY, func:true},
{input:"\\log",	   tag:"mi", output:"log",    ttype:UNARY, func:true},
{input:"\\max",	      tag:"mo", output:"max",	 ttype:UNDEROVER},
{input:"\\min",	      tag:"mo", output:"min",	 ttype:UNDEROVER},
{input:"\\Pr",	   tag:"mi", output:"Pr",     ttype:UNARY, func:true},
{input:"\\sec",	   tag:"mi", output:"sec",    ttype:UNARY, func:true},
{input:"\\sin",	   tag:"mi", output:"sin",    ttype:UNARY, func:true},
{input:"\\sinh",   tag:"mi", output:"sinh",   ttype:UNARY, func:true},
{input:"\\sup",	      tag:"mo", output:"sup",	 ttype:UNDEROVER},
{input:"\\tan",	   tag:"mi", output:"tan",    ttype:UNARY, func:true},
{input:"\\tanh",   tag:"mi", output:"tanh",   ttype:UNARY, func:true},

//arrows
{input:"\\gets",		tag:"mo", output:"\u2190", ttype:CONST},
{input:"\\leftarrow",		tag:"mo", output:"\u2190", ttype:CONST},
{input:"\\to",			tag:"mo", output:"\u2192", ttype:CONST},
{input:"\\rightarrow",		tag:"mo", output:"\u2192", ttype:CONST},
{input:"\\leftrightarrow",	tag:"mo", output:"\u2194", ttype:CONST},
{input:"\\uparrow",		tag:"mo", output:"\u2191", ttype:CONST},
{input:"\\downarrow",		tag:"mo", output:"\u2193", ttype:CONST},
{input:"\\updownarrow",		tag:"mo", output:"\u2195", ttype:CONST},
{input:"\\Leftarrow",		tag:"mo", output:"\u21D0", ttype:CONST},
{input:"\\Rightarrow",		tag:"mo", output:"\u21D2", ttype:CONST},
{input:"\\Leftrightarrow",	tag:"mo", output:"\u21D4", ttype:CONST},
{input:"\\iff", tag:"mo", output:"~\\Longleftrightarrow~", ttype:DEFINITION},
{input:"\\Uparrow",		tag:"mo", output:"\u21D1", ttype:CONST},
{input:"\\Downarrow",		tag:"mo", output:"\u21D3", ttype:CONST},
{input:"\\Updownarrow",		tag:"mo", output:"\u21D5", ttype:CONST},
{input:"\\mapsto",		tag:"mo", output:"\u21A6", ttype:CONST},
{input:"\\longleftarrow",	tag:"mo", output:"\u2190", ttype:LONG},
{input:"\\longrightarrow",	tag:"mo", output:"\u2192", ttype:LONG},
{input:"\\longleftrightarrow",	tag:"mo", output:"\u2194", ttype:LONG},
{input:"\\Longleftarrow",	tag:"mo", output:"\u21D0", ttype:LONG},
{input:"\\Longrightarrow",	tag:"mo", output:"\u21D2", ttype:LONG},
{input:"\\Longleftrightarrow",  tag:"mo", output:"\u21D4", ttype:LONG},
{input:"\\longmapsto",		tag:"mo", output:"\u21A6", ttype:CONST},
							// disaster if LONG

//commands with argument
AMsqrt, AMroot, AMfrac, AMover, AMsub, AMsup, AMtext, AMmbox, AMatop, AMchoose,
//AMdiv, AMquote,

//diacritical marks
{input:"\\acute",	tag:"mover",  output:"\u00B4", ttype:UNARY, acc:true},
//{input:"\\acute",	  tag:"mover",  output:"\u0317", ttype:UNARY, acc:true},
//{input:"\\acute",	  tag:"mover",  output:"\u0301", ttype:UNARY, acc:true},
//{input:"\\grave",	  tag:"mover",  output:"\u0300", ttype:UNARY, acc:true},
//{input:"\\grave",	  tag:"mover",  output:"\u0316", ttype:UNARY, acc:true},
{input:"\\grave",	tag:"mover",  output:"\u0060", ttype:UNARY, acc:true},
{input:"\\breve",	tag:"mover",  output:"\u02D8", ttype:UNARY, acc:true},
{input:"\\check",	tag:"mover",  output:"\u02C7", ttype:UNARY, acc:true},
{input:"\\dot",		tag:"mover",  output:".",      ttype:UNARY, acc:true},
{input:"\\ddot",	tag:"mover",  output:"..",     ttype:UNARY, acc:true},
//{input:"\\ddot",	  tag:"mover",  output:"\u00A8", ttype:UNARY, acc:true},
{input:"\\mathring",	tag:"mover",  output:"\u00B0", ttype:UNARY, acc:true},
{input:"\\vec",		tag:"mover",  output:"\u20D7", ttype:UNARY, acc:true},
{input:"\\overrightarrow",tag:"mover",output:"\u20D7", ttype:UNARY, acc:true},
{input:"\\overleftarrow",tag:"mover", output:"\u20D6", ttype:UNARY, acc:true},
{input:"\\hat",		tag:"mover",  output:"\u005E", ttype:UNARY, acc:true},
{input:"\\widehat",	tag:"mover",  output:"\u0302", ttype:UNARY, acc:true},
{input:"\\tilde",	tag:"mover",  output:"~",      ttype:UNARY, acc:true},
//{input:"\\tilde",	  tag:"mover",  output:"\u0303", ttype:UNARY, acc:true},
{input:"\\widetilde",	tag:"mover",  output:"\u02DC", ttype:UNARY, acc:true},
{input:"\\bar",		tag:"mover",  output:"\u203E", ttype:UNARY, acc:true},
{input:"\\overbrace",	tag:"mover",  output:"\u23B4", ttype:UNARY, acc:true},
{input:"\\overline",	tag:"mover",  output:"\u00AF", ttype:UNARY, acc:true},
{input:"\\underbrace",  tag:"munder", output:"\u23B5", ttype:UNARY, acc:true},
{input:"\\underline",	tag:"munder", output:"\u00AF", ttype:UNARY, acc:true},
//{input:"underline",	tag:"munder", output:"\u0332", ttype:UNARY, acc:true},

//typestyles and fonts
{input:"\\displaystyle",tag:"mstyle",atname:"displaystyle",atval:"true", ttype:UNARY},
{input:"\\textstyle",tag:"mstyle",atname:"displaystyle",atval:"false", ttype:UNARY},
{input:"\\scriptstyle",tag:"mstyle",atname:"scriptlevel",atval:"1", ttype:UNARY},
{input:"\\scriptscriptstyle",tag:"mstyle",atname:"scriptlevel",atval:"2", ttype:UNARY},
{input:"\\textrm", tag:"mstyle", output:"\\mathrm", ttype: DEFINITION},
{input:"\\mathbf", tag:"mstyle", atname:"mathvariant", atval:"bold", ttype:UNARY},
{input:"\\textbf", tag:"mstyle", atname:"mathvariant", atval:"bold", ttype:UNARY},
{input:"\\mathit", tag:"mstyle", atname:"mathvariant", atval:"italic", ttype:UNARY},
{input:"\\textit", tag:"mstyle", atname:"mathvariant", atval:"italic", ttype:UNARY},
{input:"\\mathtt", tag:"mstyle", atname:"mathvariant", atval:"monospace", ttype:UNARY},
{input:"\\texttt", tag:"mstyle", atname:"mathvariant", atval:"monospace", ttype:UNARY},
{input:"\\mathsf", tag:"mstyle", atname:"mathvariant", atval:"sans-serif", ttype:UNARY},
{input:"\\mathbb", tag:"mstyle", atname:"mathvariant", atval:"double-struck", ttype:UNARY, codes:AMbbb},
{input:"\\mathcal",tag:"mstyle", atname:"mathvariant", atval:"script", ttype:UNARY, codes:AMcal},
{input:"\\mathfrak",tag:"mstyle",atname:"mathvariant", atval:"fraktur",ttype:UNARY, codes:AMfrk}
];

function compareNames(s1,s2) {
  if (s1.input > s2.input) return 1
  else return -1;
}

var AMnames = []; //list of input symbols

function AMinitSymbols() {
  AMsymbols.sort(compareNames);
  for (i=0; i<AMsymbols.length; i++) AMnames[i] = AMsymbols[i].input;
}

var AMmathml = "http://www.w3.org/1998/Math/MathML";

function AMcreateElementMathML(t) {
  if (isIE) return document.createElement("m:"+t);
  else return document.createElementNS(AMmathml,t);
}

function AMcreateMmlNode(t,frag) {
//  var node = AMcreateElementMathML(name);
  if (isIE) var node = document.createElement("m:"+t);
  else var node = document.createElementNS(AMmathml,t);
  node.appendChild(frag);
  return node;
}

function newcommand(oldstr,newstr) {
  AMsymbols = AMsymbols.concat([{input:oldstr, tag:"mo", output:newstr,
                                 ttype:DEFINITION}]);
}

function AMremoveCharsAndBlanks(str,n) {
//remove n characters and any following blanks
  var st;
  st = str.slice(n);
  for (var i=0; i<st.length && st.charCodeAt(i)<=32; i=i+1);
  return st.slice(i);
}

function AMposition(arr, str, n) {
// return position >=n where str appears or would be inserted
// assumes arr is sorted
  if (n==0) {
    var h,m;
    n = -1;
    h = arr.length;
    while (n+1<h) {
      m = (n+h) >> 1;
      if (arr[m]<str) n = m; else h = m;
    }
    return h;
  } else
    for (var i=n; i<arr.length && arr[i]<str; i++);
  return i; // i=arr.length || arr[i]>=str
}

function AMgetSymbol(str) {
//return maximal initial substring of str that appears in names
//return null if there is none
  var k = 0; //new pos
  var j = 0; //old pos
  var mk; //match pos
  var st;
  var tagst;
  var match = "";
  var more = true;
  for (var i=1; i<=str.length && more; i++) {
    st = str.slice(0,i); //initial substring of length i
    j = k;
    k = AMposition(AMnames, st, j);
    if (k<AMnames.length && str.slice(0,AMnames[k].length)==AMnames[k]){
      match = AMnames[k];
      mk = k;
      i = match.length;
    }
    more = k<AMnames.length && str.slice(0,AMnames[k].length)>=AMnames[k];
  }
  AMpreviousSymbol=AMcurrentSymbol;
  if (match!=""){
    AMcurrentSymbol=AMsymbols[mk].ttype;
    return AMsymbols[mk];
  }
  AMcurrentSymbol=CONST;
  k = 1;
  st = str.slice(0,1); //take 1 character
  if ("0"<=st && st<="9") tagst = "mn";
  else tagst = (("A">st || st>"Z") && ("a">st || st>"z")?"mo":"mi");
/*
// Commented out by DRW (not fully understood, but probably to do with
// use of "/" as an INFIX version of "\\frac", which we don't want):
//}
//if (st=="-" && AMpreviousSymbol==INFIX) {
//  AMcurrentSymbol = INFIX;  //trick "/" into recognizing "-" on second parse
//  return {input:st, tag:tagst, output:st, ttype:UNARY, func:true};
//}
*/
  return {input:st, tag:tagst, output:st, ttype:CONST};
}


/*Parsing ASCII math expressions with the following grammar
v ::= [A-Za-z] | greek letters | numbers | other constant symbols
u ::= sqrt | text | bb | other unary symbols for font commands
b ::= frac | root | stackrel	binary symbols
l ::= { | \left			left brackets
r ::= } | \right		right brackets
S ::= v | lEr | uS | bSS	Simple expression
I ::= S_S | S^S | S_S^S | S	Intermediate expression
E ::= IE | I/I			Expression
Each terminal symbol is translated into a corresponding mathml node.*/

var AMpreviousSymbol,AMcurrentSymbol;

function AMparseSexpr(str) { //parses str and returns [node,tailstr,(node)tag]
  var symbol, node, result, result2, i, st,// rightvert = false,
    newFrag = document.createDocumentFragment();
  str = AMremoveCharsAndBlanks(str,0);
  symbol = AMgetSymbol(str);             //either a token or a bracket or empty
  if (symbol == null || symbol.ttype == RIGHTBRACKET)
    return [null,str,null];
  if (symbol.ttype == DEFINITION) {
    str = symbol.output+AMremoveCharsAndBlanks(str,symbol.input.length);
    symbol = AMgetSymbol(str);
    if (symbol == null || symbol.ttype == RIGHTBRACKET)
      return [null,str,null];
  }
  str = AMremoveCharsAndBlanks(str,symbol.input.length);
  switch (symbol.ttype) {
  case SPACE:
    node = AMcreateElementMathML(symbol.tag);
    node.setAttribute(symbol.atname,symbol.atval);
    return [node,str,symbol.tag];
  case UNDEROVER:
    if (isIE) {
      if (symbol.input.substr(0,4) == "\\big") {   // botch for missing symbols
	str = "\\"+symbol.input.substr(4)+str;	   // make \bigcup = \cup etc.
	symbol = AMgetSymbol(str);
	symbol.ttype = UNDEROVER;
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
      }
    }
    return [AMcreateMmlNode(symbol.tag,
			document.createTextNode(symbol.output)),str,symbol.tag];
  case CONST:
    var output = symbol.output;
    if (isIE) {
      if (symbol.input == "'")
	output = "\u2032";
      else if (symbol.input == "''")
	output = "\u2033";
      else if (symbol.input == "'''")
	output = "\u2033\u2032";
      else if (symbol.input == "''''")
	output = "\u2033\u2033";
      else if (symbol.input == "\\square")
	output = "\u25A1";	// same as \Box
      else if (symbol.input.substr(0,5) == "\\frac") {
						// botch for missing fractions
	var denom = symbol.input.substr(6,1);
	if (denom == "5" || denom == "6") {
	  str = symbol.input.replace(/\\frac/,"\\frac ")+str;
	  return [node,str,symbol.tag];
	}
      }
    }
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(output));
    return [node,str,symbol.tag];
  case LONG:  // added by DRW
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    node.setAttribute("minsize","1.5");
    node.setAttribute("maxsize","1.5");
    node = AMcreateMmlNode("mover",node);
    node.appendChild(AMcreateElementMathML("mspace"));
    return [node,str,symbol.tag];
  case STRETCHY:  // added by DRW
    if (isIE && symbol.input == "\\backslash")
	symbol.output = "\\";	// doesn't expand, but then nor does "\u2216"
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    if (symbol.input == "|" || symbol.input == "\\vert" ||
	symbol.input == "\\|" || symbol.input == "\\Vert") {
	  node.setAttribute("lspace","0em");
	  node.setAttribute("rspace","0em");
    }
    node.setAttribute("maxsize",symbol.atval);  // don't allow to stretch here
    if (symbol.rtag != null)
      return [node,str,symbol.rtag];
    else
      return [node,str,symbol.tag];
  case BIG:  // added by DRW
    var atval = symbol.atval;
    if (isIE)
      atval = symbol.ieval;
    symbol = AMgetSymbol(str);
    if (symbol == null)
	return [null,str,null];
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    if (isIE) {		// to get brackets to expand
      var space = AMcreateElementMathML("mspace");
      space.setAttribute("height",atval+"ex");
      node = AMcreateMmlNode("mrow",node);
      node.appendChild(space);
    } else {		// ignored in IE
      node.setAttribute("minsize",atval);
      node.setAttribute("maxsize",atval);
    }
    return [node,str,symbol.tag];
  case LEFTBRACKET:   //read (expr+)
    if (symbol.input == "\\left") { // left what?
      symbol = AMgetSymbol(str);
      if (symbol != null) {
	if (symbol.input == ".")
	  symbol.invisible = true;
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
      }
    }
    result = AMparseExpr(str,true,false);
    if (symbol==null ||
	(typeof symbol.invisible == "boolean" && symbol.invisible))
      node = AMcreateMmlNode("mrow",result[0]);
    else {
      node = AMcreateMmlNode("mo",document.createTextNode(symbol.output));
      node = AMcreateMmlNode("mrow",node);
      node.appendChild(result[0]);
    }
    return [node,result[1],result[2]];
  case MATRIX:	 //read (expr+)
    if (symbol.input == "\\begin{array}") {
      var mask = "";
      symbol = AMgetSymbol(str);
      str = AMremoveCharsAndBlanks(str,0);
      if (symbol == null)
	mask = "l";
      else {
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
	if (symbol.input != "{")
	  mask = "l";
	else do {
	  symbol = AMgetSymbol(str);
	  if (symbol != null) {
	    str = AMremoveCharsAndBlanks(str,symbol.input.length);
	    if (symbol.input != "}")
	      mask = mask+symbol.input;
	  }
	} while (symbol != null && symbol.input != "" && symbol.input != "}");
      }
      result = AMparseExpr("{"+str,true,true);
//    if (result[0]==null) return [AMcreateMmlNode("mo",
//			   document.createTextNode(symbol.input)),str];
      node = AMcreateMmlNode("mtable",result[0]);
      mask = mask.replace(/l/g,"left ");
      mask = mask.replace(/r/g,"right ");
      mask = mask.replace(/c/g,"center ");
      node.setAttribute("columnalign",mask);
      node.setAttribute("displaystyle","false");
      if (isIE)
	return [node,result[1],null];
// trying to get a *little* bit of space around the array
// (IE already includes it)
      var lspace = AMcreateElementMathML("mspace");
      lspace.setAttribute("width","0.167em");
      var rspace = AMcreateElementMathML("mspace");
      rspace.setAttribute("width","0.167em");
      var node1 = AMcreateMmlNode("mrow",lspace);
      node1.appendChild(node);
      node1.appendChild(rspace);
      return [node1,result[1],null];
    } else {	// eqnarray
      result = AMparseExpr("{"+str,true,true);
      node = AMcreateMmlNode("mtable",result[0]);
      if (isIE)
	node.setAttribute("columnspacing","0.25em"); // best in practice?
      else
	node.setAttribute("columnspacing","0.167em"); // correct (but ignored?)
      node.setAttribute("columnalign","right center left");
      node.setAttribute("displaystyle","true");
      node = AMcreateMmlNode("mrow",node);
      return [node,result[1],null];
    }
  case TEXT:
      if (str.charAt(0)=="{") i=str.indexOf("}");
      else i = 0;
      if (i==-1)
		 i = str.length;
      st = str.slice(1,i);
      if (st.charAt(0) == " ") {
	node = AMcreateElementMathML("mspace");
	node.setAttribute("width","0.33em");	// was 1ex
	newFrag.appendChild(node);
      }
      newFrag.appendChild(
        AMcreateMmlNode(symbol.tag,document.createTextNode(st)));
      if (st.charAt(st.length-1) == " ") {
	node = AMcreateElementMathML("mspace");
	node.setAttribute("width","0.33em");	// was 1ex
	newFrag.appendChild(node);
      }
      str = AMremoveCharsAndBlanks(str,i+1);
      return [AMcreateMmlNode("mrow",newFrag),str,null];
  case UNARY:
      result = AMparseSexpr(str);
      if (result[0]==null) return [AMcreateMmlNode(symbol.tag,
                             document.createTextNode(symbol.output)),str];
      if (typeof symbol.func == "boolean" && symbol.func) { // functions hack
	st = str.charAt(0);
//	if (st=="^" || st=="_" || st=="/" || st=="|" || st==",") {
	if (st=="^" || st=="_" || st==",") {
	  return [AMcreateMmlNode(symbol.tag,
		    document.createTextNode(symbol.output)),str,symbol.tag];
        } else {
	  node = AMcreateMmlNode("mrow",
	   AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output)));
	  if (isIE) {
	    var space = AMcreateElementMathML("mspace");
	    space.setAttribute("width","0.167em");
	    node.appendChild(space);
	  }
	  node.appendChild(result[0]);
	  return [node,result[1],symbol.tag];
        }
      }
      if (symbol.input == "\\sqrt") {		// sqrt
	if (isIE) {	// set minsize, for \surd
	  var space = AMcreateElementMathML("mspace");
	  space.setAttribute("height","1.2ex");
	  space.setAttribute("width","0em");	// probably no effect
	  node = AMcreateMmlNode(symbol.tag,result[0])
//	  node.setAttribute("minsize","1");	// ignored
//	  node = AMcreateMmlNode("mrow",node);  // hopefully unnecessary
	  node.appendChild(space);
	  return [node,result[1],symbol.tag];
	} else
	  return [AMcreateMmlNode(symbol.tag,result[0]),result[1],symbol.tag];
      } else if (typeof symbol.acc == "boolean" && symbol.acc) {   // accent
        node = AMcreateMmlNode(symbol.tag,result[0]);
	var output = symbol.output;
	if (isIE) {
		if (symbol.input == "\\hat")
			output = "\u0302";
		else if (symbol.input == "\\widehat")
			output = "\u005E";
		else if (symbol.input == "\\bar")
			output = "\u00AF";
		else if (symbol.input == "\\grave")
			output = "\u0300";
		else if (symbol.input == "\\tilde")
			output = "\u0303";
	}
	var node1 = AMcreateMmlNode("mo",document.createTextNode(output));
	if (symbol.input == "\\vec" || symbol.input == "\\check")
						// don't allow to stretch
	    node1.setAttribute("maxsize","1.2");
		 // why doesn't "1" work?  \vec nearly disappears in firefox
	if (isIE && symbol.input == "\\bar")
	    node1.setAttribute("maxsize","0.5");
	if (symbol.input == "\\underbrace" || symbol.input == "\\underline")
	  node1.setAttribute("accentunder","true");
	else
	  node1.setAttribute("accent","true");
	node.appendChild(node1);
	if (symbol.input == "\\overbrace" || symbol.input == "\\underbrace")
	  node.ttype = UNDEROVER;
	return [node,result[1],symbol.tag];
      } else {			      // font change or displaystyle command
        if (!isIE && typeof symbol.codes != "undefined") {
          for (i=0; i<result[0].childNodes.length; i++)
            if (result[0].childNodes[i].nodeName=="mi" || result[0].nodeName=="mi") {
              st = (result[0].nodeName=="mi"?result[0].firstChild.nodeValue:
                              result[0].childNodes[i].firstChild.nodeValue);
              var newst = [];
              for (var j=0; j<st.length; j++)
                if (st.charCodeAt(j)>64 && st.charCodeAt(j)<91) newst = newst +
                  String.fromCharCode(symbol.codes[st.charCodeAt(j)-65]);
                else newst = newst + st.charAt(j);
              if (result[0].nodeName=="mi")
                result[0]=AMcreateElementMathML("mo").
                          appendChild(document.createTextNode(newst));
              else result[0].replaceChild(AMcreateElementMathML("mo").
          appendChild(document.createTextNode(newst)),result[0].childNodes[i]);
            }
        }
        node = AMcreateMmlNode(symbol.tag,result[0]);
        node.setAttribute(symbol.atname,symbol.atval);
	if (symbol.input == "\\scriptstyle" ||
	    symbol.input == "\\scriptscriptstyle")
		node.setAttribute("displaystyle","false");
	return [node,result[1],symbol.tag];
      }
  case BINARY:
    result = AMparseSexpr(str);
    if (result[0]==null) return [AMcreateMmlNode("mo",
			   document.createTextNode(symbol.input)),str,null];
    result2 = AMparseSexpr(result[1]);
    if (result2[0]==null) return [AMcreateMmlNode("mo",
			   document.createTextNode(symbol.input)),str,null];
    if (symbol.input=="\\root" || symbol.input=="\\stackrel")
      newFrag.appendChild(result2[0]);
    newFrag.appendChild(result[0]);
    if (symbol.input=="\\frac") newFrag.appendChild(result2[0]);
    return [AMcreateMmlNode(symbol.tag,newFrag),result2[1],symbol.tag];
  case INFIX:
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    return [AMcreateMmlNode("mo",document.createTextNode(symbol.output)),
	str,symbol.tag];
  default:
    return [AMcreateMmlNode(symbol.tag,        //its a constant
	document.createTextNode(symbol.output)),str,symbol.tag];
  }
}

function AMparseIexpr(str) {
  var symbol, sym1, sym2, node, result, tag, underover;
  str = AMremoveCharsAndBlanks(str,0);
  sym1 = AMgetSymbol(str);
  result = AMparseSexpr(str);
  node = result[0];
  str = result[1];
  tag = result[2];
  symbol = AMgetSymbol(str);
  if (symbol.ttype == INFIX) {
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    result = AMparseSexpr(str);
    if (result[0] == null) // show box in place of missing argument
      result[0] = AMcreateMmlNode("mo",document.createTextNode("\u25A1"));
    str = result[1];
    tag = result[2];
    if (symbol.input == "_" || symbol.input == "^") {
      sym2 = AMgetSymbol(str);
      tag = null;	// no space between x^2 and a following sin, cos, etc.
// This is for \underbrace and \overbrace
      underover = ((sym1.ttype == UNDEROVER) || (node.ttype == UNDEROVER));
//    underover = (sym1.ttype == UNDEROVER);
      if (symbol.input == "_" && sym2.input == "^") {
        str = AMremoveCharsAndBlanks(str,sym2.input.length);
        var res2 = AMparseSexpr(str);
	str = res2[1];
	tag = res2[2];  // leave space between x_1^2 and a following sin etc.
        node = AMcreateMmlNode((underover?"munderover":"msubsup"),node);
        node.appendChild(result[0]);
        node.appendChild(res2[0]);
      } else if (symbol.input == "_") {
	node = AMcreateMmlNode((underover?"munder":"msub"),node);
        node.appendChild(result[0]);
      } else {
	node = AMcreateMmlNode((underover?"mover":"msup"),node);
        node.appendChild(result[0]);
      }
      node = AMcreateMmlNode("mrow",node); // so sum does not stretch
    } else {
      node = AMcreateMmlNode(symbol.tag,node);
      if (symbol.input == "\\atop" || symbol.input == "\\choose")
	node.setAttribute("linethickness","0ex");
      node.appendChild(result[0]);
      if (symbol.input == "\\choose")
	node = AMcreateMmlNode("mfenced",node);
    }
  }
  return [node,str,tag];
}

function AMparseExpr(str,rightbracket,matrix) {
  var symbol, node, result, i, tag,
  newFrag = document.createDocumentFragment();
  do {
    str = AMremoveCharsAndBlanks(str,0);
    result = AMparseIexpr(str);
    node = result[0];
    str = result[1];
    tag = result[2];
    symbol = AMgetSymbol(str);
    if (node!=undefined) {
      if ((tag == "mn" || tag == "mi") && symbol!=null &&
	typeof symbol.func == "boolean" && symbol.func) {
			// Add space before \sin in 2\sin x or x\sin x
	  var space = AMcreateElementMathML("mspace");
	  space.setAttribute("width","0.167em");
	  node = AMcreateMmlNode("mrow",node);
	  node.appendChild(space);
      }
      newFrag.appendChild(node);
    }
  } while ((symbol.ttype != RIGHTBRACKET)
        && symbol!=null && symbol.output!="");
  tag = null;
  if (symbol.ttype == RIGHTBRACKET) {
    if (symbol.input == "\\right") { // right what?
      str = AMremoveCharsAndBlanks(str,symbol.input.length);
      symbol = AMgetSymbol(str);
      if (symbol != null && symbol.input == ".")
	symbol.invisible = true;
      if (symbol != null)
	tag = symbol.rtag;
    }
    if (symbol!=null)
      str = AMremoveCharsAndBlanks(str,symbol.input.length); // ready to return
    var len = newFrag.childNodes.length;
    if (matrix &&
      len>0 && newFrag.childNodes[len-1].nodeName == "mrow" && len>1 &&
      newFrag.childNodes[len-2].nodeName == "mo" &&
      newFrag.childNodes[len-2].firstChild.nodeValue == "&") { //matrix
	var pos = []; // positions of ampersands
        var m = newFrag.childNodes.length;
        for (i=0; matrix && i<m; i=i+2) {
          pos[i] = [];
          node = newFrag.childNodes[i];
	  for (var j=0; j<node.childNodes.length; j++)
	    if (node.childNodes[j].firstChild.nodeValue=="&")
	      pos[i][pos[i].length]=j;
        }
	var row, frag, n, k, table = document.createDocumentFragment();
	for (i=0; i<m; i=i+2) {
	  row = document.createDocumentFragment();
	  frag = document.createDocumentFragment();
	  node = newFrag.firstChild; // <mrow> -&-&...&-&- </mrow>
	  n = node.childNodes.length;
	  k = 0;
	  for (j=0; j<n; j++) {
	    if (typeof pos[i][k] != "undefined" && j==pos[i][k]){
	      node.removeChild(node.firstChild); //remove &
	      row.appendChild(AMcreateMmlNode("mtd",frag));
	      k++;
	    } else frag.appendChild(node.firstChild);
	  }
	  row.appendChild(AMcreateMmlNode("mtd",frag));
	  if (newFrag.childNodes.length>2) {
	    newFrag.removeChild(newFrag.firstChild); //remove <mrow> </mrow>
	    newFrag.removeChild(newFrag.firstChild); //remove <mo>&</mo>
	  }
	  table.appendChild(AMcreateMmlNode("mtr",row));
	}
	return [table,str];
    }
    if (typeof symbol.invisible != "boolean" || !symbol.invisible) {
      node = AMcreateMmlNode("mo",document.createTextNode(symbol.output));
      newFrag.appendChild(node);
    }
  }
  return [newFrag,str,tag];
}

function AMparseMath(str) {
  var result, node = AMcreateElementMathML("mstyle");
  if (mathcolor != "") node.setAttribute("mathcolor",mathcolor);
  if (mathfontfamily != "") node.setAttribute("fontfamily",mathfontfamily);
  node.appendChild(AMparseExpr(str.replace(/^\s+/g,""),false,false)[0]);
  node = AMcreateMmlNode("math",node);
  if (showasciiformulaonhover)                      //fixed by djhsu so newline
    node.setAttribute("title",str.replace(/\s+/g," "));//does not show in Gecko
  if (mathfontfamily != "" && (isIE || mathfontfamily != "serif")) {
    var fnode = AMcreateElementXHTML("font");
    fnode.setAttribute("face",mathfontfamily);
    fnode.appendChild(node);
    return fnode;
  }
  return node;
}

function AMstrarr2docFrag(arr, linebreaks) {
  var newFrag=document.createDocumentFragment();
  var expr = false;
  for (var i=0; i<arr.length; i++) {
    if (expr) newFrag.appendChild(AMparseMath(arr[i]));
    else {
      var arri = (linebreaks ? arr[i].split("\n\n") : [arr[i]]);
      newFrag.appendChild(AMcreateElementXHTML("span").
      appendChild(document.createTextNode(arri[0])));
      for (var j=1; j<arri.length; j++) {
        newFrag.appendChild(AMcreateElementXHTML("p"));
        newFrag.appendChild(AMcreateElementXHTML("span").
        appendChild(document.createTextNode(arri[j])));
      }
    }
    expr = !expr;
  }
  return newFrag;
}

function AMprocessNodeR(n, linebreaks) {
  var mtch, str, arr, frg, i;
  if (n.childNodes.length == 0) {
   if ((n.nodeType!=8 || linebreaks) &&
    n.parentNode.nodeName!="form" && n.parentNode.nodeName!="FORM" &&
    n.parentNode.nodeName!="textarea" && n.parentNode.nodeName!="TEXTAREA" &&
    n.parentNode.nodeName!="pre" && n.parentNode.nodeName!="PRE") {
    str = n.nodeValue;
    if (!(str == null)) {
      str = str.replace(/\r\n\r\n/g,"\n\n");
      str = str.replace(/\x20+/g," ");
      str = str.replace(/\s*\r\n/g," ");
// DELIMITERS:
      mtch = (str.indexOf("\$")==-1 ? false : true);
      str = str.replace(/([^\\])\$/g,"$1 \$");
      str = str.replace(/^\$/," \$");	// in case \$ at start of string
      arr = str.split(" \$");
      for (i=0; i<arr.length; i++)
	arr[i]=arr[i].replace(/\\\$/g,"\$");
      if (arr.length>1 || mtch) {
        if (checkForMathML) {
          checkForMathML = false;
          var nd = AMisMathMLavailable();
          AMnoMathML = nd != null;
          if (AMnoMathML && notifyIfNoMathML)
            if (alertIfNoMathML)
              alert("To view the ASCIIMathML notation use Internet Explorer 6 +\nMathPlayer (free from www.dessci.com)\n\
                or Firefox/Mozilla/Netscape");
            else AMbody.insertBefore(nd,AMbody.childNodes[0]);
        }
        if (!AMnoMathML) {
          frg = AMstrarr2docFrag(arr,n.nodeType==8);
          var len = frg.childNodes.length;
          n.parentNode.replaceChild(frg,n);
          return len-1;
        } else return 0;
      }
    }
   } else return 0;
  } else if (n.nodeName!="math") {
    for (i=0; i<n.childNodes.length; i++)
      i += AMprocessNodeR(n.childNodes[i], linebreaks);
  }
  return 0;
}

function AMprocessNode(n, linebreaks, spanclassAM) {
  var frag,st;
  if (spanclassAM!=null) {
    frag = document.getElementsByTagName("span")
    for (var i=0;i<frag.length;i++)
      if (frag[i].className == "AM")
        AMprocessNodeR(frag[i],linebreaks);
  } else {
    try {
      st = n.innerHTML;
    } catch(err) {}
// DELIMITERS:
    if (st==null || st.indexOf("\$")!=-1)
      AMprocessNodeR(n,linebreaks);
  }
  if (isIE) { //needed to match size and font of formula to surrounding text
    frag = document.getElementsByTagName('math');
    for (var i=0;i<frag.length;i++) frag[i].update()
  }
}

var AMbody;
var AMnoMathML = false, AMtranslated = false;

function translate(spanclassAM) {
  if (!AMtranslated) { // run this only once
    AMtranslated = true;
    AMinitSymbols();
    AMbody = document.getElementsByTagName("body")[0];
    AMprocessNode(AMbody, false, spanclassAM);
  }
}

if (isIE) { // avoid adding MathPlayer info explicitly to each webpage
  document.write("<object id=\"mathplayer\"\
  classid=\"clsid:32F66A20-7614-11D4-BD11-00104BD3F987\"></object>");
  document.write("<?import namespace=\"m\" implementation=\"#mathplayer\"?>");
}

// GO1.1 Generic onload by Brothercake
// http://www.brothercake.com/
//onload function (replaces the onload="translate()" in the <body> tag)
function generic()
{
  translate();
};
//setup onload function
if(typeof window.addEventListener != 'undefined')
{
  //.. gecko, safari, konqueror and standard
  window.addEventListener('load', generic, false);
}
else if(typeof document.addEventListener != 'undefined')
{
  //.. opera 7
  document.addEventListener('load', generic, false);
}
else if(typeof window.attachEvent != 'undefined')
{
  //.. win/ie
  window.attachEvent('onload', generic);
}
//** remove this condition to degrade older browsers
else
{
  //.. mac/ie5 and anything else that gets this far
  //if there's an existing onload function
  if(typeof window.onload == 'function')
  {
    //store it
    var existing = onload;
    //add new onload handler
    window.onload = function()
    {
      //call existing onload function
      existing();
      //call generic onload function
      generic();
    };
  }
  else
  {
    //setup onload function
    window.onload = generic;
  }
}
/*]]>*/
</script>
</head>
<body class="book">
<div id="header">
<h1>Thermodynamic Analytics Toolkit (TATi)</h1>
<span id="author">Frederik Heber, Zofia Trstanova, Benedict Leimkuhler</span><br>
<span id="email" class="monospaced">&lt;<a href="mailto:frederik.heber@gmail.com">frederik.heber@gmail.com</a>&gt;</span><br>
<span id="revnumber">version v0.9.5-2-g887d52f,</span>
<span id="revdate">2019-04-02</span>
<div id="toc">
  <div id="toctitle">Table of Contents</div>
  <noscript><p><b>JavaScript must be enabled in your browser to display the table of contents.</b></p></noscript>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="imageblock" style="text-align:center;">
<div class="content">
<img src="./doc/userguide/./pictures/tati_logo.png" alt="TATi logo" width="700">
</div>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">2019-04-02 thermodynamicanalyticstoolkit: v0.9.5-2-g887d52f</div>
<div class="paragraph"><p>TATi is a software suite written in Python based on <a href="https://www.tensorflow.org/">tensorflow</a>'s
Python API. It brings advanced sampling methods to <em>neural network training</em>.
Its <strong>tools</strong> allow to assess the loss manifold&#8217;s topology that depends on the
employed neural network and the dataset. Moreover, its <strong>simulation</strong> module makes
applying present sampling Python codes in the context of neural networks easy
and straight-forward. The goal of the software is to enable the user to analyze
and adapt the network employed for a specific classification problem to best
fit her or his needs.</p></div>
<div class="paragraph"><p>TATi has received financial support from a seed funding grant and through a
Rutherford fellowship from the Alan Turing Institute in London (R-SIS-003,
R-RUT-001) and EPSRC grant no. EP/P006175/1 (Data Driven Coarse Graining using
Space-Time Diffusion Maps, B. Leimkuhler PI). Moreover, the development was
aided by a Microsoft Azure Sponsorship (MS-AZR-0143P).</p></div>
<div class="paragraph"><p><em>Frederik Heber</em>, <em>Zofia Trstanova</em>, <em>Benedict Leimkuhler</em></p></div>
</div></div>
</div>
</div>
<div class="sect1">
<h2 id="introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph"><p>TATi contains a set of distinct guides aimed at different user audiences. All
of them reside in
<span class="monospaced">&lt;installation folder&gt;/share/doc/thermodynamicanalyticstoolkit/</span> in the
installation folder.</p></div>
<div class="ulist"><ul>
<li>
<p>
User guide (this document: <span class="monospaced">userguide.html</span> or <span class="monospaced">userguide.pdf</span>)
</p>
</li>
<li>
<p>
Programmer&#8217;s guide (see <span class="monospaced">programmersguide.html</span> or <span class="monospaced">programmersguide.pdf</span>)
</p>
</li>
<li>
<p>
Reference documentation (see <span class="monospaced">thermodynamicanalyticstoolkit-API-reference.pdf</span> or <span class="monospaced">html/index.html</span>)
</p>
</li>
<li>
<p>
Roadmap (see <span class="monospaced">roadmap.html</span> or <span class="monospaced">roadmap.pdf</span>)
</p>
</li>
</ul></div>
<div class="sect2">
<h3 id="introduction.needtoknow">1.1. Before you start</h3>
<div class="paragraph"><p>In the following we assume that you, the reader, has a general
familiarity with neural networks. You should know what a classification
problem is, what an associated dataset for (supervised) learning needs
to contain. You should know about what weights and biases in a neural
network are and what the loss function does. You should also have a
rough idea of what optimization is and that gradients with respect to
the chosen loss function can be obtained through so-called
backpropagation.</p></div>
<div class="paragraph"><p>If you are <em>not</em> familiar with the above terminology, then we recommend an
introductory book on neural networks such as <a href="#Bishop2006">[Bishop2006]</a>.</p></div>
</div>
<div class="sect2">
<h3 id="introduction.whatis">1.2. What is ThermodynamicAnalyticsToolkit?</h3>
<div class="paragraph"><p>The Thermodynamic Analytics Toolkit allows to perform thermodynamic sampling
and analysis of large neural network loss manifolds. It extends
 <a href="https://www.tensorflow.org/">Tensorflow</a> by several samplers, by a framework
to rapidly prototype new samplers, by the capability to sample several networks
in parallel and provides tools for analysis and visualization of loss
manifolds, see Figure <a href="#introduction.figure.tools_module">[introduction.figure.tools_module]</a> for an overview.
We rely  on the Python programming language as only for that Tensorflow&#8217;s
interface has an API stability promise.</p></div>
<div class="paragraph"><p>There are two approaches to using TATi:
On the one hand, it is a toolkit consisting of command-line programs such as
TATiOptimizer, TATiSampler, TATiLossFunctionSampler, and TATiAnalyzer that,
when fed a dataset and given the network specifics, directly allow to optimize
and sample the network parameters and analyze the exlored loss manifold, see
Figure <a href="#introduction.figure.tools_module">[introduction.figure.tools_module]</a>.</p></div>
<div class="imageblock" id="introduction.figure.tools_module">
<div class="content">
<img src="./doc/userguide/pictures/tati_tools-architecture.png" alt="pictures/tati_tools-architecture.png" width="500">
</div>
<div class="title">Figure 1. Architecture of TATi&#8217;s tools</div>
</div>
<div class="paragraph"><p>On the other hand, TATi can be readily used inside Python programs by using its
modules: <span class="monospaced">simulation</span>, <span class="monospaced">model</span>, <span class="monospaced">analysis</span>.
The <span class="monospaced">simulation</span> module, see Figure <a href="#introduction.figure.simulation_module">[introduction.figure.simulation_module]</a>,
contains a very easy-to-use, high-level interface to neural network modelling,
granting full access to the network&#8217;s parameters, its gradients, its loss and all
other quantities of interest. It is especially practical for rapid prototyping.
The module <span class="monospaced">model</span> is its low-level counterpart and allows for more efficient
implementations.</p></div>
<div class="imageblock" id="introduction.figure.simulation_module">
<div class="content">
<img src="./doc/userguide/pictures/tati_simulations-architecture.png" alt="pictures/tati_simulations-architecture.png" width="500">
</div>
<div class="title">Figure 2. Architecture of the <strong>simulation</strong> module</div>
</div>
<div class="paragraph"><p>Beginning with section <a href="#quickstart">[quickstart]</a> we give an introduction to either way of
using TATi.</p></div>
</div>
<div class="sect2">
<h3 id="introduction.installation">1.3. Installation</h3>
<div class="paragraph"><p>In the following we explain the installation procedure to get
ThermodynamicAnalyticsToolkit up and running.</p></div>
<div class="paragraph"><p>The easiest way to tati is through
````
pip install tati
````</p></div>
<div class="paragraph"><p>If you want to install from a cloned repository or from a release tarball, then
read on.</p></div>
<div class="sect3">
<h4 id="introduction.installation.requirements">1.3.1. Installation requirements</h4>
<div class="paragraph"><p>This program suite is implemented using python3 and the development
mainly focused on Linux (development machine used Ubuntu 14.04 up to 18.04). At
the moment other operating systems are not supported but may still work.</p></div>
<div class="paragraph"><p>It has the following non-trivial dependencies:</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="https://www.tensorflow.org/">TensorFlow</a>: version 1.4.1 till currently
1.10 supported
</p>
</li>
<li>
<p>
<a href="https://www.numpy.org/">Numpy</a>:
</p>
</li>
<li>
<p>
<a href="https://pandas.pydata.org/">Pandas</a>
</p>
</li>
<li>
<p>
<a href="https://scikit-learn.org/">sklearn</a>
</p>
</li>
<li>
<p>
<a href="http://matplotlib.org/">matplotlib</a>
</p>
</li>
<li>
<p>
<a href="https://pypi.org/project/acor/">acor</a>
</p>
</li>
</ul></div>
<div class="paragraph"><p>Note that these packages can be easily installed using either
the repository tool (using some linux derivate such as Ubuntu), e.g.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>sudo apt install python3-numpy</pre>
</div></div>
<div class="paragraph"><p>or via <strong>pip3</strong>, i.e.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>pip3 install numpy</pre>
</div></div>
<div class="paragraph"><p>For <strong>acor</strong> a few extra changes are required.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>pip3 install acor
sed -i -e "s#import _acor#import acor._acor as _acor#" &lt;install path&gt;/acor/acor.py</pre>
</div></div>
<div class="paragraph"><p>The last command replaces the third line in the file <strong>acor/acor.py</strong> such that the
function <strong>acor</strong> (and not the module <strong>acor</strong>) is used.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content"><strong>acor</strong> is only required for the Integrated Autocorrelation Time analysis and
may be ignored if this functionality is not required.</td>
</tr></table>
</div>
<div class="paragraph"><p>Moreover, the following packages are not ultimately required but
examples or tests may depend on them:</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="https://www.sqlite.org">sqlite3</a>
</p>
</li>
<li>
<p>
gawk
</p>
</li>
</ul></div>
<div class="paragraph"><p>The documentation is written in <a href="https://asciidoc.org/">AsciiDoc</a> and
<a href="http://www.doxygen.nl/">doxygen</a> and requires a suitable package to compile
to HTML or create a PDF, e.g., using dblatex</p></div>
<div class="ulist"><ul>
<li>
<p>
doxygen
</p>
</li>
<li>
<p>
asciidoc
</p>
</li>
<li>
<p>
dblatex
</p>
</li>
</ul></div>
<div class="paragraph"><p>Finally, for the diffusion map analysis we recommend using the pydiffmap
package, see <a href="https://github.com/DiffusionMapsAcademics/pyDiffMap">https://github.com/DiffusionMapsAcademics/pyDiffMap</a>.</p></div>
<div class="paragraph"><p>In our setting what typically worked best was to use
<a href="https://anaconda.org/">anaconda</a> in the following manner:</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>conda create -n tensorflow python=3.5 -y
conda install -n tensorflow -y \
     tensorflow numpy scipy pandas scikit-learn matplotlib</pre>
</div></div>
<div class="paragraph"><p>In case your machine has GPU hardware for tensorflow, replace
&#8220;tensorflow&#8221; by &#8220;tensorflow-gpu&#8221;.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">On systems with typical core i7 architecture recompiling
tensorflow from source provided only very small runtime gains in our
tests which in most cases do not support the extra effort. You may find
it necessary for tackling really large networks (&gt;1e6 dofs) and datasets and
especially if you desire to use Intel&#8217;s MKL library for the CPU-based
linear algebra computations.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content"><strong>acor</strong> cannot be installed using anaconda (not available). Hence, it
needs to be installed using <span class="monospaced">pip</span> for the respective environment. See above for
installation instructions.</td>
</tr></table>
</div>
<div class="paragraph"><p>Henceforth, we assume that there is a working tensorflow on your system,
i.e. inside the python3 shell</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> tensorflow as tf

a<span style="color: #990000">=</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">constant</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Hello world"</span><span style="color: #990000">)</span>
sess<span style="color: #990000">=</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>a<span style="color: #990000">))</span>
</tt></pre></div></div>
<div class="paragraph"><p>should print &#8220;Hello world&#8221; or similar.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph"><p>You can check the version of your <strong>tensorflow</strong> installation at any time
by inspecting <span class="monospaced">print(tf.__version__)</span>.</p></div>
<div class="paragraph"><p>Similarly, TATi&#8217;s version can be obtained through</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>TATi<span style="color: #990000">.</span>__version__<span style="color: #990000">)</span>
</tt></pre></div></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="introduction.installation.procedure">1.3.2. Installation procedure</h4>
<div class="paragraph"><p>Installation comes in three flavors: as a PyPI package, or through either via
a tarball or a cloned repository.</p></div>
<div class="paragraph"><p>In general, the PyPI (<span class="monospaced">pip</span>) packages are strongly recommended, especially if
you only want to use the software.</p></div>
<div class="paragraph"><p>The tarball releases are recommended if you only plan to use TATi and do not
intend ito modify its code. If, however, you need to use a development branch,
then you have to clone from the repository.</p></div>
<div class="paragraph"><p>In general, this package is distributed via autotools, "compiled" and installed via
automake. If you are familiar with this set of tools, there should be no
problem. If not, please refer to the text <span class="monospaced">INSTALL</span> file that is included
in the tarball.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Only the tarball contains precompiled PDFs of the userguides. The cloned
repository contains only the HTML pages.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="introduction.installation.procedure.tarball">1.3.3. From Tarball</h4>
<div class="paragraph"><p>Unpack the archive, assuming its suffix is <span class="monospaced">.bz2</span>.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>tar -jxvf thermodynamicanalyticstoolkit-${revnumber}.tar.bz2</pre>
</div></div>
<div class="paragraph"><p>If the ending is <span class="monospaced">.gz</span>, you need to unpack by</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>tar -zxvf thermodynamicanalyticstoolkit-${revnumber}.tar.gz</pre>
</div></div>
<div class="paragraph"><p>Enter the directory</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>cd thermodynamicanalyticstoolkit</pre>
</div></div>
<div class="paragraph"><p>Continue then in section <a href="#configure_make_install">Configure, make, install</a>.</p></div>
</div>
<div class="sect3">
<h4 id="introduction.installation.repository">1.3.4. From cloned repository</h4>
<div class="paragraph"><p>While the tarball does not require any autotools packages installed on your
system, the cloned repository does. You need the following packages:</p></div>
<div class="ulist"><ul>
<li>
<p>
autotools
</p>
</li>
<li>
<p>
automake
</p>
</li>
</ul></div>
<div class="paragraph"><p>To prepare code in the working directory, enter</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>./bootstrap.sh</pre>
</div></div>
</div>
<div class="sect3">
<h4 id="introduction.installation.configure_make_install">1.3.5. Configure, make, make install</h4>
<div class="paragraph"><p>Next, we recommend to build the toolkit not in the source folder but in an
extra folder, e.g., &#8220;build64&#8221;. In the autotools lingo this is called an
<em>out-of-source</em> build. It prevents cluttering of the source folder.
Naturally, you may pick any name (and actually any location on your
computer) as you see fit.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>mkdir build64
cd build64
../configure --prefix="somepath" -C PYTHON="path to python3"
make
make install</pre>
</div></div>
<div class="paragraph"><p>More importantly, please replace &#8220;somepath&#8221; and &#8220;path to python3&#8221; by
the desired installation path and the full path to the <span class="monospaced">python3</span>
executable on your system.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>In case of having used <em>anaconda</em> for the installation of required packages,
then you need to use</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>$HOME/.conda/envs/tensorflow/bin/python3</pre>
</div></div>
<div class="paragraph"><p>for the respective command, where <span class="monospaced">$HOME</span> is your home folder. This assumes
that your anaconda environment is named <strong>tensorflow</strong> as in the example
installation steps above.</p></div>
</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>We recommend executing (after <span class="monospaced">make install</span> was run)</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>make -j4 check</pre>
</div></div>
<div class="paragraph"><p>additionally. This will execute every test on the extensive testsuite
and report any errors. None should fail. If all fail, a possible cause
might be a not working tensorflow installation. If some fail, please
contact the author.
The extra argument <strong>-j4</strong> instructs <span class="monospaced">make</span> to use four threads in parallel for
testing. Use as many as you have cores on your machine.</p></div>
<div class="paragraph"><p>In case you run the testcases on strongly parallel hardware, tests may
fail because of cancellation effects during parallel summation. In this
case you made degrade the test threshold using the environment variable
<span class="monospaced">TATI_TEST_THRESHOLD</span>. If unset, <strong>it defaults to 1e-7</strong>.</p></div>
</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph"><p>Tests may fail due to numerical inaccuracies due to reduction operations
executed in parallel. Tensorflow does not emphasize on determinism but on
speed and scaling. Therefore, if your system has many cores or is GPU-assisted,
some tests may fail.</p></div>
<div class="paragraph"><p>In this case you can set the environment variable <strong>TATI_TEST_THRESHOLD</strong> when
calling <span class="monospaced">configure</span>. Its default value is <strong>1e-7</strong>. For the DGX-1 we found <strong>4e-6</strong>
to work. If the threshold need to run all test successfully is much larger than
this, you should contact us, see <a href="#introduction.feedback">[introduction.feedback]</a>.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="introduction.license">1.4. License</h3>
<div class="paragraph"><p>As long as no other license statement is given,
ThermodynamicAnalyticsToolkit is free for use under the GNU Public
License (GPL) Version 3 (see <a href="https://www.gnu.org/licenses/gpl-3.0.en.html">https://www.gnu.org/licenses/gpl-3.0.en.html</a> for
full text).</p></div>
</div>
<div class="sect2">
<h3 id="introduction.disclaimer">1.5. Disclaimer</h3>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Because the program is licensed free of charge, there is not warranty
for the program, to the extent permitted by applicable law. Except when
otherwise stated in writing in the copyright holders and/or other
parties provide the program "as is" without warranty of any kind, either
expressed or implied. Including, but not limited to, the implied
warranties of merchantability and fitness for a particular purpose. The
entire risk as to the quality and performance of the program is with
you. Should the program prove defective, you assume the cost of all
necessary servicing, repair, or correction.</p></div>
</div>
<div class="attribution">
<em>https://www.gnu.org/licenses/gpl-3.0.en.html</em><br>
&#8212; section 11 of the GPLv3 license
</div></div>
</div>
<div class="sect2">
<h3 id="introduction.feedback">1.6. Feedback</h3>
<div class="paragraph"><p>If you encounter any bugs, errors, or would like to submit feature request,
please open an issue at <a href="https://github.com/alan-turing-institute/ThermodynamicAnalyticsToolkit">GitHub</a> or write to <a href="mailto:">frederik.heber@gmail.com</a>.
The authors are especially thankful for any description of all related events
prior to occurrence of the error and auxiliary files. More explicitly, the
<strong>following information is crucial</strong> in enabling assistance:</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>operating system</strong> and version, e.g., Ubuntu 16.04
</p>
</li>
<li>
<p>
<strong>Tensorflow version</strong>, e.g., TF 1.6
</p>
</li>
<li>
<p>
<strong>TATi version</strong> (or respective branch on GitHub), e.g., TATi 0.8
</p>
</li>
<li>
<p>
steps that lead to the error, possibly with <strong>sample Python code</strong>
</p>
</li>
</ul></div>
<div class="paragraph"><p>Please mind sensible space restrictions of email attachments.</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="quickstart">2. Quickstart</h2>
<div class="sectionbody">
<div class="paragraph"><p>Before we come to actually using TATi, we explain what possible approaches
there are to sampling a high-dimensional function such as the loss manifold of
neural networks. To this end, we talk about grid-based sampling that however
suffers from the Curse of Dimensionality. Moreover, we will discuss Monte Carlo
and especially Markov Chain Monte Carlo methods. In the latter category we have
what we will call dynamics-based sampling. This approach does not suffer in
principle from the Curse of Dimensionality but moreover may have additional
savings by looking only at areas of the manifold that have small loss.
At the end, want to set the stage with a little example: We will look  at a
very simple classification task and see how it is solved using neural networks.</p></div>
<div class="sect2">
<h3 id="quickstart.sampling">2.1. Sampling</h3>
<div class="paragraph"><p>Let be given a high-dimensional loss manifold</p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>$L_{D}(\theta) = \sum_{(x_i,y_i) \in {D}} l_\theta(x_i,y_i)$</p></div>
</div>
<div class="attribution">
</div></div>
<div class="paragraph"><p>that depends explicitly on the parameters $\theta \in \Omega \subset \mathrm{R}^N$ of the network with $N$ degrees of freedom and implicitly on a given dataset ${D} = \{x_i,y_i\}$. Furthermore, the loss function $l_\theta$ itself depends implicitly on the prediction $f_\theta(x_i)$ of the network for a given input $x_i$ and fixed parameters $\theta$ and therefore on the chosen network architecture.</p></div>
<div class="paragraph"><p>If ${D}$ were the set of all possible realisations of the data, then $L_{D}(\theta)$ would measure the so-called <strong>generalization error</strong>. As typically only a finite set of data is available, it is split into training and test parts, the latter set aside for approximating this error. Furthermore, during training the gradient is often computed on a subset of the training set ${D}$, the mini-batches. In the following we will ignore these details as they are irrelevant to the general procedure described.</p></div>
<div class="paragraph"><p>In order to deduce information such as the number of minima, the number of saddle points, the typical barrier height between minima and other characteristics to classify the landscape, we need to <strong>explore the whole domain</strong> $\Omega$. As $L_{D}$ depends on an arbitrary dataset, we do not know anything a-priori apart from the general regularity properties <span class="footnote"><br>[As activation functions contained in the loss may be non-smooth, e.g., the rectified linear unit, even this can be not taken as solid grounds.]<br></span> and possible boundedness of $l_\theta$.</p></div>
<div class="paragraph"><p>A very similar task is found in function approximation where an unknown and possibly high-dimensional function $f(x): \mathrm{R}^N \rightarrow \mathrm{R}$ is approximated through a set of point evaluations in conjunction with a basis set. A typical usecase is the numerical integration of this high-dimensional function. Naturally, the quality of the approximation hinges not only on the basis set but even more on the choice of the precise location of point evaluations, the <strong>sampling</strong>.</p></div>
<div class="paragraph"><p>Very generally, sampling approaches can be placed into two categories: Grids and sequences. Sequences can be random, deterministic, or both. We briefly discuss them, focusing on their general computational cost in high dimensions.</p></div>
<div class="sect3">
<h4 id="quickstart.sampling.grid">2.1.1. Grids</h4>
<div class="paragraph"><p>Structured grids such as naive full grid approaches, where a fixed number of points with equidistant spacing is used per axis, suffer from the Curse of Dimensionality, a term coined by Bellman. With increasing number of parameters $N$ the computational cost becomes prohibitive. This Curse of Dimensionality is alleviated to some extent by so-called Sparse Grids, see <a href="#Bungartz2004">[Bungartz2004]</a>, where the quality bounds on the approximation are kept but fewer grid points need to be evaluated. However, they only alleviate the curse to some extent, see <a href="#Pflueger2010">[Pflueger2010]</a>, and are still infeasible at the moment for the extremely high-dimensional manifolds encountered in neural network losses.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.sampling.sequences">2.1.2. Sequences</h4>
<div class="paragraph"><p>Random sequences are Monte Carlo approaches where a specific sequence of random numbers decides which point in the whole space to evaluate. Due to their inherent stochasticity these lack the rigor of the structured grid methods but neither rely on, nor exploit any regularity as the structured grid methods. Therefore, they suffer no Curse of Dimensionality with respect to their convergence rate. The rate is bounded by the Central Limit Theorem to ${O}(n^{-\frac 1 2})$ if $n$ is the number of evaluated points.</p></div>
<div class="paragraph"><p>Quasi-Monte Carlo (QMC) methods use deterministic sequences, such as Lattice rules (Hammersley, \ldots) or digital nets, and are able to obtain higher convergence rates of ${O}(n^{-1} (\log{n})^d)$ at the price of a moderate dependence on dimension $d$.</p></div>
<div class="paragraph"><p>In this category of sequences we also have dynamics-based sequences. As examples of Markov Chain Monte Carlo (MCMC) methods, they are closely connected to pure Monte Carlo approaches. They make use of the ergodic property of the chosen dynamics allowing to replace high-dimensional whole-space integrals by single-dimensional time-integrals, where the accuracy depends on the trajectory length.</p></div>
<div class="paragraph"><p>The chain can be generated through suitable dynamics such as Hamiltonian,</p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>$d\theta= M^{-1} p dt, \quad dp= -\nabla L(\theta) dt$</p></div>
</div>
<div class="attribution">
</div></div>
<div class="paragraph"><p>or Langevin dynamics,</p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>$d\theta= M^{-1} p dt, \quad dp= \Bigl (-\nabla L(\theta) - \gamma p \Bigr ) dt + \sigma M^{\frac 1 2} dW$</p></div>
</div>
<div class="attribution">
</div></div>
<div class="paragraph"><p>for positions (or parameters) $\theta$, momenta $p$, mass matrix $M$, potential (or loss) $L(\theta)$, friction constant $\gamma$ and a stationary normal random process $W$ with variance $\sigma$.
Note that Langevin dynamics has both Hamiltonian dynamics and  Brownian dynamics as limiting cases of the friction constant $\gamma$ going to zero or infinity, respectively.</p></div>
<div class="paragraph"><p>There are also hybrid approaches where a purely deterministic sequence is randomized by a Metropolis-Hastings criterion to remove possible bias, such as Hybrid or Hamiltonian Monte Carlo, see <a href="#Neal2011">[Neal2011]</a>. Note that in the case of Langevin dynamics the stochasticity enters through the random process.</p></div>
<div class="paragraph"><p>If we consider the function $L_{D}(\theta)$ as a potential energy function, then we may cast this into a probability distribution using the canonical Gibbs distribution
$Z \cdot \exp( -\beta L_{D}(\theta))$,
where $Z$ is a normalization constant and $\beta$ is the inverse temperature factor. Then, we have sampling in the typical sense in statistics where an unknown distribution is evaluated. We remark that this Gibbs measure is known in the neural networks community through the energy interpretation (<a href="#LeCun2006">[LeCun2006</a>) of a probability distribution in relation to a certain reference energy, given by the <strong>temperature</strong>.</p></div>
<div class="paragraph"><p>And indeed, dynamics-based sampling does not aim to approximate $L$ as best as possible but its Gibbs (also called the canonical) distribution $\exp{(-\beta L)}$. In our case we are only interested in particular subsets of the space, namely those associated with a small loss. As we have given ample evidence, the central challenge in sampling is the computational cost. The dynamics-based sequences allow to save computational cost in the high-dimensional spaces by incorporating gradient information. Hence, the more accurately we sample from the Gibbs distribution, the more efficiently we sample only those subsets of interest. This saving could not be obtained with a pure Monte Carlo approach.</p></div>
<div class="paragraph"><p>Therefore, <strong>we focus on dynamics-based sampling</strong> for this high-dimensional exploration problem.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.sampling.iat">2.1.3. Integrated Autocorrelation Time</h4>
<div class="paragraph"><p>In principle, the underlying challenge is to assure that the sampling trajectory covers a sufficient area of the whole space for validity. However, stating when to terminate is as difficult as the exploration task itself. Hence, the best measure is to assess when a new independent state has been obtained.</p></div>
<div class="paragraph"><p>When inspecting sampled MCMC trajectories, we need to assess how many consecutive steps it takes to get from one independent state to another. This is estimated by the <strong>Integrated Autocorrelation Time</strong> (IAT) $\tau_s$. For any observable $A$, we have that its variance generally behaves as $var_{A} = \frac {var_{\pi} (\varphi(X))}{T_s/\tau_s}$, where $\pi$ is the target density, $\varphi(X)$ is the function of interest of the random variable $X$, and $T_s$ is the sampling time, see <a href="#Goodman2010">[Goodman2010]</a>. Obviously, when time steps are discrete and $\tau_s$ is measured in number of steps, then $\tau_s = 1$ is highly desirable, i. e.~immediately stepping from one independent state to the next. The IAT $\tau_s$ is defined as</p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>$\tau_s = \sum^{\infty}_{-\infty} \frac{ C_s(t)} {C_s(0)} \quad      {with} \quad C_s(t) = \lim_{t' \rightarrow \infty} cov[\varphi \bigl (X(t'+t) \bigr ), \varphi \bigl (X(t) \bigr)$].</p></div>
</div>
<div class="attribution">
</div></div>
<div class="paragraph"><p>The above holds also for sampling approaches based on Langevin Dynamics. There, we may use the IAT to gauge the <strong>exploration speed</strong> for each sampled trajectory $X(t)$.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.sampling.example">2.1.4. Example: Sampling of a Perceptron</h4>
<div class="paragraph"><p>Let us give a trivial example to illustrate the above with a few figures. We
want to highlight in the following the key aspect about the dynamics-based
sampling approach, namely sampling the probability distribution function associated
with the Gibbs measure and not the loss manifold directly.</p></div>
<div class="imageblock" id="quickstart.dataset" style="text-align:center;">
<div class="content">
<img src="./doc/userguide/pictures/dataset_two_clusters.png" alt="pictures/dataset_two_clusters.png" width="400">
</div>
<div class="title">Figure 3. Dataset: "Two Clusters" dataset consisting of two normally distributed point clouds in two dimensions</div>
</div>
<div class="paragraph"><p>Assume we are given a very simple data set as depicted in
<a href="#quickstart.dataset">Dataset</a>. The goal is to
classify all red and blue dots into two different classes. This problem
is quite simple to solve: a line in the two-dimensional space can easily
separate the two classes.</p></div>
<div class="paragraph"><p>A very simple neural network, a perceptron: it would use one
input node, either of the coordinate, $x_{1}$ or
$x_{2}$, and a single output node with an activation
function $f$ whose sign gives the class the input item
belongs to. The network is given in
<a href="#quickstart.network">Network</a>. The network is chosen non-ideal by design to illustrate a point.</p></div>
<div class="paragraph" id="quickstart.network"><div class="title">Network: Neural network with permutation symmetry to provoke multiple minima</div><p><span class="image">
<img src="./doc/userguide/pictures/neuralnetwork_permutation_symmetry.png" alt="pictures/neuralnetwork_permutation_symmetry.png" width="500">
</span></p></div>
<div class="paragraph"><p>In Figure <a href="#quickstart.landscape.loss">Loss manifold</a> we then
turn to the two-dimensional loss landscape depending on the two weights. In
this very low-dimensional case we turn to the "naive grid" approach and
partition each axis equidistantly.
We see two minima basins both of hyperbole or "banana" shape. Here, we see that
there is not a single minima but two of them. This is caused by the deliberate
permutation symmetry of the two weights in the network.</p></div>
<div class="paragraph"><p>Assume we additionally perform dynamics-based sampling. In the figure the
resulting trajectory is given as squiggly black line. Here, we have
chosen such an (inverse) temperature value such that it is able to pass the
potential barrier and reach the other minima basin.</p></div>
<div class="paragraph" id="quickstart.landscape.loss"><div class="title">Loss manifold: Loss landscape with underlying naive grid sampling and a dynamics-based trajectory obtained with the BAOAB sampler.</div><p><span class="image">
<img src="./doc/userguide/pictures/losslandscape_permutation_symmetry.png" alt="scaledwidth=45.0%">
</span></p></div>
<div class="paragraph"><p>As we clearly see, the grid-based approach does not distinguish between the
areas of high loss and the areas of low loss. Note that the coloring comes from a spline interpolation from the grid points. The dynamics-based trajectory on the other hand remains in areas of low loss all the time, where "low" is
relative to its inverse temperature parameter. This is because the areas of
low loss have a much higher probability in the Gibbs measure which our sampler
is faithful to.</p></div>
<div class="paragraph"><p>This quick description of the sampling loss manifolds in the context of
neural networks in data science should have acquainted we with some of the concepts underlying the idea of sampling.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.simulation">2.2. Using module simulation</h3>
<div class="paragraph"><p>The <span class="monospaced">simulation</span> module has been designed explicitly for
ease-of-use.
Typically, everything is achieved through two or three commands: One to setup
TATi by handing it a dash of options, then calling a function to <span class="monospaced">fit()</span> or
<span class="monospaced">sample()</span>. In the very end of this quickstart we will learn how to implement
our first sampler using this interface.</p></div>
<div class="paragraph"><p>For a more extensive description of the simulation module, we refer to its
<a href="#reference.simulation">reference section</a>.</p></div>
<div class="paragraph"><p>If we have installed the TATi package in the folder <span class="monospaced">/foo</span>, i.e. we have
a folder <span class="monospaced">TATi</span> with a file <span class="monospaced">simulation.py</span> residing in there, then we
probably need to add it to the <span class="monospaced">PYTHONPATH</span> as follows</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>PYTHONPATH=/foo python3</pre>
</div></div>
<div class="paragraph"><p>In this shell, we may import the sampling part of the package as
follows</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>import TATi.simulation as tati</pre>
</div></div>
<div class="paragraph"><p>This will import the <span class="monospaced">Simulation</span> interface class as the shortcut <span class="monospaced">tati</span> from
the file mentioned before. This class contains a set of convenience functions
that hides all the complexity of setting up of input pipelines and networks.
Accessing the loss function, gradients and alike or training and sampling can
be done in just a few keystrokes.</p></div>
<div class="paragraph"><p>In order to make our own python scripts executable and know about the
correct (possibly non-standard) path to ThermodynamicAnalyticsToolkit, place
the following two lines at the very beginning of the script:</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>import sys
sys.path.insert(1,"&lt;path_to_TATi&gt;/lib/python3.5/site-packages/")</pre>
</div></div>
<div class="paragraph"><p>where <span class="monospaced">&lt;path_to_TATi&gt;</span> needs to be replaced by our specific
installation path and <span class="monospaced">python3.5</span> needs to be replaced if we are using a
different python version. However, for the examples in this quickstart tutorial
it is not necessary if we use <span class="monospaced">PYTHONPATH</span>.</p></div>
<div class="sect3">
<h4 id="quickstart.simulation.notation">2.2.1. Notation</h4>
<div class="paragraph"><p>In the following, we will use the following notation:</p></div>
<div class="ulist"><ul>
<li>
<p>
dataset: $D = \{X,Y\}$ with features $X=\{x_d\}$ and labels
$Y=\{y_d\}$
</p>
</li>
<li>
<p>
batch of the dataset: $D_i=\{X_i, Y_i\}$
</p>
</li>
<li>
<p>
network parameters: $w=\{w_1, \ldots, w_M\}$
</p>
</li>
<li>
<p>
momenta of network parameters: $p=\{p_1, \ldots, p_M\}$
</p>
</li>
<li>
<p>
neural network function: $F_w(x)$
</p>
</li>
<li>
<p>
loss function: $L_D(w) = \sum_i l(F_w(x_i), y_i)$ with a loss
$l(x,y)$
</p>
</li>
<li>
<p>
gradients: $\nabla_w L_D(w)$
</p>
</li>
<li>
<p>
Hessians: $H_{ij} = \partial_{w_i} \partial_{w_j} L_D(w)$
</p>
</li>
</ul></div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.general">2.2.2. Instantiating TATi</h4>
<div class="paragraph"><p>The first thing in all the following example we will do is instantiate the
<span class="monospaced">tati</span> class.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
  <span style="font-style: italic"><span style="color: #9A1900"># comma-separated list of options</span></span>
<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Although it is the <span class="monospaced">simulation</span> module, we "nickname" it <span class="monospaced">tati</span> in the following
and hence will simply refer to this instance as <span class="monospaced">tati.</span></p></div>
<div class="paragraph"><p>This class takes a list of options in its construction or <span class="monospaced">__init__()</span> call.
These options inform it about the dataset to use, the specific network topology,
what sampler or optimizer to use and its parameters and so on.</p></div>
<div class="paragraph"><p>To see how this works, we will first need a dataset to work on.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">All of the examples below can also be found in the folders
<span class="monospaced">doc/userguide/python</span>, <span class="monospaced">doc/userguide/simulation</span>, and <span class="monospaced">doc/userguide/simulation/complex</span>.</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.general.help_options">Help on Options</h5>
<div class="paragraph"><p><span class="monospaced">tati</span> has quite a number of options that control its behavior. we can
request help to a specific option.
Let us inspect the help for <span class="monospaced">batch_data_files</span>:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">&gt;&gt;&gt;</span> <span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="color: #990000">&gt;&gt;&gt;</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">help</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"batch_data_files"</span><span style="color: #990000">)</span>
Option name<span style="color: #990000">:</span> batch_data_files
Description<span style="color: #990000">:</span> set of files to read input <span style="font-weight: bold"><span style="color: #000080">from</span></span>
Type       <span style="color: #990000">:</span> list of <span style="color: #990000">&lt;</span><span style="font-weight: bold"><span style="color: #0000FF">class</span></span> <span style="color: #FF0000">'str'</span><span style="color: #990000">&gt;</span>
Default    <span style="color: #990000">:</span> <span style="color: #990000">[]</span></tt></pre></div></div>
<div class="paragraph"><p>This will print a description, give the default value and expected type.</p></div>
<div class="paragraph"><p>Moreover, in case we have forgotten the name of one of the options.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">&gt;&gt;&gt;</span> <span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="color: #990000">&gt;&gt;&gt;</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">help</span></span><span style="color: #990000">()</span>
averages_file<span style="color: #990000">:</span>             CSV file name to write ensemble averages information such as average kinetic<span style="color: #990000">,</span> potential<span style="color: #990000">,</span> virial
batch_data_file_type<span style="color: #990000">:</span>      type of the files to read input <span style="font-weight: bold"><span style="color: #000080">from</span></span>
 <span style="color: #990000">&lt;</span>remainder omitted<span style="color: #990000">&gt;</span></tt></pre></div></div>
<div class="paragraph"><p>This will print a general help listing all available options.</p></div>
<div class="paragraph"><p>Use <span class="monospaced">get_options()</span> to get a dict of all options or to request the currently set
value of a specific option. Moreover, use <span class="monospaced">set_options()</span> to modify them.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    verbose<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_options</span></span><span style="color: #990000">())</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">set_options</span></span><span style="color: #990000">(</span>verbose<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_options</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"verbose"</span><span style="color: #990000">))</span></tt></pre></div></div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.setup">2.2.3. Setup</h4>
<div class="paragraph"><p>In the following we will first be creating a dataset to work on. This example
code will be the most extensive one. All following ones are rather short and
straight-forward.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.setup.writing_data">Preparing a dataset</h5>
<div class="paragraph"><p>Therefore, let us prepare the dataset, see the Figure <a href="#quickstart.dataset">Dataset</a>,
for our following experiments.</p></div>
<div class="paragraph"><p>At the moment, datasets are parsed from Comma Separated Values (CSV)
or Tensorflow&#8217;s own TFRecord files or can be provided in-memory from numpy
arrays. In order for the following examples on optimization and sampling to
work, we need such a data file containing features and labels.</p></div>
<div class="paragraph"><p>TATi provides a few simple dataset generators contained in the class
<span class="monospaced">ClassificationDatasets</span>.</p></div>
<div class="paragraph"><p>One option therefore is to use the TATiDatasetWriter that provides access to
<span class="monospaced">ClassificationDatasets</span>, see <a href="#quickstart.cmdline.writing_dataset">Writing a dataset</a>.
However, we can do the same using python as well. This should give us an idea
that we are not constrained to the <span class="monospaced">simulation</span> part of the Python interface,
see the reference on the general Python interface where we go through the
same examples without importing <span class="monospaced">simulation</span>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>datasets<span style="color: #990000">.</span>classificationdatasets <span style="color: #990000">\</span>
    <span style="font-weight: bold"><span style="color: #000080">import</span></span> ClassificationDatasets as DatasetGenerator
<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>common <span style="font-weight: bold"><span style="color: #000080">import</span></span> data_numpy_to_csv

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

<span style="font-style: italic"><span style="color: #9A1900"># fix random seed for reproducibility</span></span>
np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">426</span><span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># generate test dataset: two clusters</span></span>
dataset_generator <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">DatasetGenerator</span></span><span style="color: #990000">()</span>
xs<span style="color: #990000">,</span> ys <span style="color: #990000">=</span> dataset_generator<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">generate</span></span><span style="color: #990000">(</span>
    dimension<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    noise<span style="color: #990000">=</span><span style="color: #993399">0.01</span><span style="color: #990000">,</span>
    data_type<span style="color: #990000">=</span>dataset_generator<span style="color: #990000">.</span>TWOCLUSTERS<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># always shuffle data set is good practice</span></span>
randomize <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">arange</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #000000">len</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">))</span>
np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">shuffle</span></span><span style="color: #990000">(</span>randomize<span style="color: #990000">)</span>
xs<span style="color: #990000">[:]</span> <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">array</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">)[</span>randomize<span style="color: #990000">]</span>
ys<span style="color: #990000">[:]</span> <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">array</span></span><span style="color: #990000">(</span>ys<span style="color: #990000">)[</span>randomize<span style="color: #990000">]</span>

<span style="font-style: italic"><span style="color: #9A1900"># call helper to write as properly formatted CSV</span></span>
<span style="font-weight: bold"><span style="color: #000000">data_numpy_to_csv</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">,</span>ys<span style="color: #990000">,</span> <span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">)</span></tt></pre></div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">The labels need to be integer values. Importing will fail if they are not.</td>
</tr></table>
</div>
<div class="paragraph"><p>After importing some modules we first fix the numpy seed to 426 in order
to get the same items reproducibly. Then, we first create 500 items
using the <span class="monospaced">ClassificationDatasets</span> class from the <strong>TWOCLUSTERS</strong> dataset with
a random perturbation of relative 0.01 magnitude. We shuffle the dataset as the
generators typically create first items of one label class and then items of
the  other label class. This is not needed here as our <em>batch_size</em> will equal
the dataset size but it is good practice generally.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The class <span class="monospaced">ClassificationDatasets</span> mimicks the dataset examples that can also
be found on the <a href="https://playground.tensorflow.org/">Tensorflow playground</a>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Afterwards, we write the dataset to a simple CSV file with columns "x1", "x2",
and "label1" using a helper function contained in the <span class="monospaced">TATi.common</span> module.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Caution</div>
</td>
<td class="content">The file <span class="monospaced">dataset-twoclusters.csv</span> is used in the following examples, so keep
it around.</td>
</tr></table>
</div>
<div class="paragraph"><p>This is the very simple dataset we want to learn, sample from and exlore in the
following.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.setup.setting_up_network">Setting up the network</h5>
<div class="paragraph"><p>Let&#8217;s first create a neural network. At the moment of writing TATi is
constrained to multi-layer perceptrons but this will soon be extended to
convolutional and other networks.</p></div>
<div class="paragraph"><p>Multi-layer perceptrons are characterized by the number of layers, the
number of nodes per layer and the output function used in each node.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-style: italic"><span style="color: #9A1900"># prepare parameters</span></span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
        batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
        hidden_dimension<span style="color: #990000">=[</span><span style="color: #993399">8</span><span style="color: #990000">,</span> <span style="color: #993399">8</span><span style="color: #990000">],</span>
        hidden_activation<span style="color: #990000">=</span><span style="color: #FF0000">"relu"</span><span style="color: #990000">,</span>
        input_dimension<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
        loss<span style="color: #990000">=</span><span style="color: #FF0000">"mean_squared"</span><span style="color: #990000">,</span>
        output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
        output_dimension<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">,</span>
        seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>In the above example, we specify a neural network of two hidden layers,
each having 8 nodes. We use the "rectified linear" activation function
for these nodes. The output nodes are activated by a linear function. At the
end, we print the number of parameters, i.e. $M=105$ for the set of
parameters $w=\{w_1, \ldots, w_M\}$.</p></div>
<div class="paragraph"><p>The network&#8217;s weights are initialized randomly in the interval [-0.5,0.5] and
the biases are set to 0.1 (small, non-zero values).</p></div>
<div class="paragraph"><p>Let us briefly highlight the essential options (a full and up-to-date list
is given in the API reference in the class <span class="monospaced">PythonOptions</span>):</p></div>
<div class="ulist"><ul>
<li>
<p>
<span class="monospaced">input_columns</span>: This option allows to add an additional layer after the
input that selects a subset of the input nodes and additionally modifies them,
e.g., by passing through a sine function. Example: <span class="monospaced">input_columns=["sin(x1), x2^2"]</span>
</p>
</li>
<li>
<p>
<span class="monospaced">input_dimension</span>: This is the number of input nodes of the network, one node
per dimension of the supplied dataset. Example <span class="monospaced">input_dimension=10</span>
</p>
</li>
<li>
<p>
<span class="monospaced">output_activation</span>: Defines the activation function for the output layer.
Example: <span class="monospaced">output_activation="sigmoid"</span>
</p>
</li>
<li>
<p>
<span class="monospaced">output_dimension</span>: Sets the number of output nodes. Example: <span class="monospaced">output_dimension=1</span>
</p>
</li>
<li>
<p>
<span class="monospaced">hidden_activation</span>: Defines the common activation function for all hidden
layers. Example: <span class="monospaced">hidden_activation="relu6"</span>
</p>
</li>
<li>
<p>
<span class="monospaced">hidden_dimension</span>: Gives the hidden layers and the nodes per layer by giving
a list of integers. Example: <span class="monospaced">hidden_dimension=[2,2]</span> defines two hidden layers,
each with 2 nodes.
</p>
</li>
<li>
<p>
<span class="monospaced">loss</span>: Sets the loss function $l(x,y)$ to use. Example:
<span class="monospaced">loss="softmax_cross_entropy"</span>
</p>
</li>
</ul></div>
<div class="paragraph"><p>A complete set of all actications functions can be obtained.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_activations</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>Similar, there is a also a list of all available loss functions.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_losses</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>At the moment it is not possible to set different activation functions
for individual nodes or between hidden layers.</p></div>
</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Note that (re-)creating the <span class="monospaced">tati</span> instance  will always reset the
computational graph of tensorflow in case we need to add nodes.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.setup.freezing_parameters">Freezing network parameters</h5>
<div class="paragraph"><p>Sometimes it might be desirable to freeze parameters during training or
sampling. This can be done as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    fix_parameters<span style="color: #990000">=</span><span style="color: #FF0000">"output/biases/Variable:0=2."</span><span style="color: #990000">,</span>
    hidden_dimension<span style="color: #990000">=[</span><span style="color: #993399">8</span><span style="color: #990000">,</span> <span style="color: #993399">8</span><span style="color: #990000">],</span>
    hidden_activation<span style="color: #990000">=</span><span style="color: #FF0000">"relu"</span><span style="color: #990000">,</span>
    input_dimension<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
    loss<span style="color: #990000">=</span><span style="color: #FF0000">"mean_squared"</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    output_dimension<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>This is same code as before when
<a href="#quickstart.simulation.setup.setting_up_network">setting up the network</a>
the only exception is the additional option <em>fix_parameters</em>.</p></div>
<div class="paragraph"><p>Note that we give the parameter&#8217;s name in full tensorflow
namescope: "output" for the network layer, "biases" for the weights ("weights"
alternatively) and "Variable:0" is fixed (as it is the only one). This is
followed by a comma-separated list of values, one for each component.</p></div>
<div class="exampleblock">
<div class="content">
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Single values cannot be frozen but only entire weight matrices or bias
vectors per layer at the moment. As each component has to be listed, at the
moment this is not suitable for large vectors.</td>
</tr></table>
</div>
</div></div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.simple_evaluation">2.2.4. Evaluating loss and gradients</h4>
<div class="paragraph"><p>Having created the dataset and explained how the network is set up, we know
see how to evaluate the loss and the gradients.</p></div>
<div class="paragraph"><p>The main idea of the <span class="monospaced">simulation</span> module is to be used as a simplified
interface to access the loss and the gradients of the neural network without
having to know about the internal of the neural network. In other words, we
want to treat it as an abstract high-dimensional function, depending
implicitly on the weights and explicitly on the dataset. To this end, the
weights and biases are represented as one linearized vector. Moreover, we have
another abstract high-dimensional function, the loss that depends explicitly
on the weights and implicitly on the dataset, whose derivative (the gradients
with respect to the parameters) is available as a numpy array, see also the
section <a href="#quickstart.simulation.notation">Notation</a>.</p></div>
<div class="paragraph"><p>In the following example we set up a simple fully-connected hidden network
and evaluates loss and then the associated gradients.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

<span style="font-style: italic"><span style="color: #9A1900"># prepare parameters</span></span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">10</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span>
<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># assign parameters of NN</span></span>
nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">([</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()])</span>

<span style="font-style: italic"><span style="color: #9A1900"># simply evaluate loss</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">())</span>

<span style="font-style: italic"><span style="color: #9A1900"># also evaluate gradients (from same batch)</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">())</span>


</tt></pre></div></div>
<div class="paragraph"><p>Again, we set up the network as before, here it is a single-layer perceptron
as the default value for <span class="monospaced">hidden_dimension</span> is 0. Next, we evaluate the loss
$L_D(w)$ using <span class="monospaced">nn.loss()</span> and the gradients $\nabla_w L_D(w)$
using <span class="monospaced">nn.gradients()</span>, returning a vector with the gradient per degree of
freedom of the network.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Under the hood it is a bit more complicated: loss and gradients are inherently
connected. If <em>batch_size</em> is chosen smaller than the dataset dimension,
naive evaluation of first loss and then gradients in two separate function calls
would cause them to be evaluated on different batches. Depending on the size of
the batch, the gradients will then not belong the to the respective loss
evaluation and vice versa.
Therefore, loss, accuracy, gradients, and hessians $H_{ij}$ (if
<em>do_hessians</em> is True) are cached. Only when one of them is evaluated for the
second time (e.g., inside the loop body on the next iteration), then the next
batch is used. This makes sure that calling either <span class="monospaced">loss()</span> first and then
<span class="monospaced">gradients()</span> or the other way round yields the same values connected to the
same dataset batch.
In essence, just don&#8217;t worry about it!</td>
</tr></table>
</div>
<div class="paragraph"><p>As we see in the above example, <span class="monospaced">tati</span> forms the general interface class that
contains the network along with the dataset and everything in its internal
state.</p></div>
<div class="paragraph"><p>This is basically all the access we need in order to use our own optimization,
sampling, or exploration methods in the context of neural networks in a
high-level, abstract way.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.optimizing">2.2.5. Optimizing the network</h4>
<div class="paragraph"><p>Let us then start with optimizing the network, i.e. learning the data.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    learning_rate<span style="color: #990000">=</span><span style="color: #993399">3e-2</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">1000</span><span style="color: #990000">,</span>
    optimizer<span style="color: #990000">=</span><span style="color: #FF0000">"GradientDescent"</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
training_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">fit</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Train results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>training_data<span style="color: #990000">.</span>run_info<span style="color: #990000">[-</span><span style="color: #993399">10</span><span style="color: #990000">:])</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>training_data<span style="color: #990000">.</span>trajectory<span style="color: #990000">[-</span><span style="color: #993399">10</span><span style="color: #990000">:])</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>training_data<span style="color: #990000">.</span>averages<span style="color: #990000">[-</span><span style="color: #993399">10</span><span style="color: #990000">:])</span></tt></pre></div></div>
<div class="paragraph"><p>Again all options are set in the init call to the interface. These options
control how the optimization is performed, what kind of network is created,
how often values are stored, and so on.
Next, we call <span class="monospaced">nn.fit()</span> to perform the actual training for the chosen number
of training steps (<em>max_steps</em>). We obtain a single return structure within
which we find three <span class="monospaced">pandas</span> dataframes: run info, trajectory, and averages.
Of each we print the last ten values.</p></div>
<div class="paragraph"><p>Let us quickly go through each of the new parameters:</p></div>
<div class="ulist"><ul>
<li>
<p>
<em>batch_size</em>
</p>
<div class="paragraph"><p>sets the subset size of the data set looked at per training step,
$|D_i|=|\{X_i, Y_i\}|$, if smaller than dimension, then we add
stochasticity/noise to the training but for the advantage of smaller runtime.</p></div>
</li>
<li>
<p>
<em>learning_rate</em>
</p>
<div class="paragraph"><p>defines the scaling of the gradients in each training step, i.e. the
learning rate. Values too large may miss the minimum, values too small
need longer to reach it. For automatic picking of the <strong>learning_rate</strong>, use
<em>BarzilaiBorweinGradientDescent</em>. This however is not compatible with
mini-batching at the moment as it requires exact gradients.</p></div>
</li>
<li>
<p>
<em>max_steps</em>
</p>
<div class="paragraph"><p>gives the amount of training steps to be performed.</p></div>
</li>
<li>
<p>
optimizer
</p>
<div class="paragraph"><p>defines the method to use for training. Here, we use Gradient Descent
(in case <em>batch_size</em> is smaller than dimension, then we actually have
Stochastic Gradient Descent). Alternatively, you
<em>BarzilaiBorweinGradientDescent</em> is also available. There the secant equation
is solved in a minimal l2 error fashion which allows to automatically determine
a suitable step width. This <strong>learning_rate</strong> is then only used for the initial
guess and as a fall-back.</p></div>
</li>
<li>
<p>
<em>seed</em>
</p>
<div class="paragraph"><p>sets the seed of the random number generator. We will still have full
randomness but in a deterministic manner, i.e. calling the same
procedure again will bring up the exactly same values.</p></div>
</li>
</ul></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">In case we need to change these options elsewhere in our python code,
use <span class="monospaced">set_options()</span>.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content"><span class="monospaced">set_options()</span> may need to reinitialize certain parts of <span class="monospaced">tati</span> s
internal state depending on what options we choose to reset. Keep in mind
that modifying the network will reinitialize all its parameters and other
possible side-effects. See <span class="monospaced">simulation._affected_map</span> in
<span class="monospaced">src/TATi/simulation.py</span> for an up-to-date list of what options affects what
part of the state.</td>
</tr></table>
</div>
<div class="paragraph"><p>For these small networks the option <em>do_hessians</em> might be useful which will
compute the hessian matrix at the end of the trajectory and use the
largest eigenvalue to compute the optimal step width. This will add
nodes to the underlying computational graph for computing the components
of the hessian matrix. However, we will not do so here.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Caution</div>
</td>
<td class="content">
<div class="paragraph"><p>The creation of these hessian evaluation nodes (not speaking of their
evaluation) is a $O(N^2)$ process in the number of parameters of the
network N. Hence, this should only be done for small networks and on purpose.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>After the options have been provided, the network is initialized internally
and automatically, we then call <span class="monospaced">fit()</span> which performs the training and returns
a structure containing runtime info, trajectory, and averages as a pandas
<span class="monospaced">DataFrame</span>.</p></div>
<div class="paragraph"><p>In the following section on <a href="#quickstart.simulation.sampling">sampling</a> we will
explain what each of these three dataframes contains exactly.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">In case more output of what is actually going on in each training step is
needed, set <span class="monospaced">verbose=1</span> or even <span class="monospaced">verbose=2</span> in the options when constructing
<span class="monospaced">tati()</span>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let us have a quick glance at the decrease of the loss function over the steps
by using <span class="monospaced">matplotlib</span>. In other words, let us look at how effective the training
has been.</p></div>
<div class="listingblock">
<a id="quickstart.simulation.optimizing.plot_optimize"></a>
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> pandas as pd
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib
<span style="font-style: italic"><span style="color: #9A1900"># use agg as backend to allow command-line use as well</span></span>
matplotlib<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">use</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"agg"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib<span style="color: #990000">.</span>pyplot as plt

df_run <span style="color: #990000">=</span> pd<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">read_csv</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"run.csv"</span><span style="color: #990000">,</span> sep<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> header<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>
run<span style="color: #990000">=</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>df_run<span style="color: #990000">.</span>loc<span style="color: #990000">[:,\</span>
   <span style="color: #990000">[</span><span style="color: #FF0000">'step'</span><span style="color: #990000">,</span><span style="color: #FF0000">'loss'</span><span style="color: #990000">,</span><span style="color: #FF0000">'kinetic_energy'</span><span style="color: #990000">,</span> <span style="color: #FF0000">'total_energy'</span><span style="color: #990000">]])</span>

plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">scatter</span></span><span style="color: #990000">(</span>run<span style="color: #990000">[:,</span><span style="color: #993399">0</span><span style="color: #990000">],</span> run<span style="color: #990000">[:,</span><span style="color: #993399">1</span><span style="color: #990000">])</span>
plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">savefig</span></span><span style="color: #990000">(</span><span style="color: #FF0000">'loss-step.png'</span><span style="color: #990000">,</span>
            bbox_inches<span style="color: #990000">=</span><span style="color: #FF0000">'tight'</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900">#plt.show()</span></span></tt></pre></div></div>
<div class="paragraph"><p>The loss per step is contained in both the run info and the trajectory dataframe
in the column <em>loss</em>.</p></div>
<div class="paragraph"><p>The graph should look similar to the one obtained with <span class="monospaced">pgfplots</span> here (see
<a href="https://sourceforge.net/pgfplots">https://sourceforge.net/pgfplots</a>).</p></div>
<div class="imageblock">
<div class="content">
<img src="./doc/userguide/pictures/optimization-step_loss.png" alt="pictures/optimization-step_loss.png" width="500">
</div>
<div class="title">Figure 4. Loss history: Behavior of the loss over the optimization run</div>
</div>
<div class="paragraph"><p>As we see the loss has decreased quite quickly down to 1e-3. Go and have a
look at the other columns such as accuracy. Or try to visualize the change
in the parameters (weights and biases) in the trajectories dataframe. See
<a href="https://pandas.pydata.org/pandas-docs/stable/10min.html">10 Minutes to pandas</a>
if we are unfamiliar with the <span class="monospaced">pandas</span> module, yet.</p></div>
<div class="paragraph"><p>Obviously, we did not use a different dataset set for testing the effectiveness
of the training which should commonly be done. This way we cannot check whether
we have overfitted or not. However, our example is trivial by design and the
network too small to be prone to overfitting this dataset.</p></div>
<div class="paragraph"><p>Nonetheless, we show how to supply a different dataset and evaluate loss and
accuracy on it.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.sampling.supply_dataset">Provide our own dataset</h5>
<div class="paragraph"><p>We can directly supply our own dataset, e.g., from a numpy array residing in
memory. See the following example where we do not generate the data but parse
them from a CSV file instead of using the pandas module.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> pandas as pd

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># e.g. parse dataset from CSV file into pandas frame</span></span>
input_dimension <span style="color: #990000">=</span> <span style="color: #993399">2</span>
output_dimension <span style="color: #990000">=</span> <span style="color: #993399">1</span>
parsed_csv <span style="color: #990000">=</span> pd<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">read_csv</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"dataset-twoclusters-test.csv"</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
                         sep<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> header<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># extract feature and label columns as numpy arrays</span></span>
features <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(\</span>
    parsed_csv<span style="color: #990000">.</span>iloc<span style="color: #990000">[:,</span> <span style="color: #993399">0</span><span style="color: #990000">:</span>input_dimension<span style="color: #990000">])</span>
labels <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(\</span>
    parsed_csv<span style="color: #990000">.</span>iloc<span style="color: #990000">[:,</span> <span style="color: #990000">\</span>
        input_dimension<span style="color: #990000">:</span>input_dimension <span style="color: #990000">\</span>
                        <span style="color: #990000">+</span> output_dimension<span style="color: #990000">])</span>

<span style="font-style: italic"><span style="color: #9A1900"># supply dataset (this creates the input layer)</span></span>
nn<span style="color: #990000">.</span>dataset <span style="color: #990000">=</span> <span style="color: #990000">[</span>features<span style="color: #990000">,</span> labels<span style="color: #990000">]</span>

<span style="font-style: italic"><span style="color: #9A1900"># this has created the network, now set</span></span>
<span style="font-style: italic"><span style="color: #9A1900"># parameters obtained from optimization run</span></span>
nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">array</span></span><span style="color: #990000">([</span><span style="color: #993399">2.42835492e-01</span><span style="color: #990000">,</span> <span style="color: #993399">2.40057245e-01</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
    <span style="color: #993399">2.66429665e-03</span><span style="color: #990000">])</span>

<span style="font-style: italic"><span style="color: #9A1900"># evaluate loss and accuracy</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Loss: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">()))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Accuracy: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">score</span></span><span style="color: #990000">()))</span></tt></pre></div></div>
<div class="paragraph"><p>The major difference is that <span class="monospaced">batch_data_files</span> in <span class="monospaced">tati()</span> is now empty and
instead we simply later assign <span class="monospaced">nn.dataset</span> a numpy array to use. Note that we
could also have supplied it directly with the filename <span class="monospaced">dataset-twoclusters.csv</span>,
i.e. <span class="monospaced">nn.dataset = "dataset-twoclusters.csv"</span>.
In this example we have parsed the same file as the in the previous section
into a numpy array using the pandas module. Natually, this is just one way of
creating a suitable numpy array.</p></div>
<div class="paragraph"><p>At the end we have stated <span class="monospaced">loss()</span> and the <span class="monospaced">score()</span>. While the loss is simply
the output of the training function, the score gives the accuracy in a
classification problem setting: We compare the label given in the dataset with
the label predicted by the network and take the average over the whole dataset
(or its mini-batch if <span class="monospaced">batch_size</span> is used). For multi-labels, we use the
largest entry as the label in multi classification.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Input and output dimensions are directly deduced from the the tuple sizes.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The nodes in the input layer can be modified using <span class="monospaced">input_columns</span>, e.g.,
<span class="monospaced">input_columns=["x1", "sin(x2)", "x1^2"]</span>.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.sampling">2.2.6. Sampling the network</h4>
<div class="paragraph"><p>Typically, as preparation to a sampling run, one would optimize or equilibrate
the initially random positions first. This might be considered a specific way
of initializing parameters.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="title">Statistical background</div>
<div class="paragraph"><p>In general, when sampling from a distribution (to compute empirical averages
for example), one wants to start <em>close to equilibrium</em>, i.e. from states which
are of high probability with respect to the target distribution (therefore
the minima of the loss). The initial optimization procedure is therefore a
first guess to find such states, or at least to get close to them.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>However, let us first ignore this good practice for a moment and simply look
at sampling from a random initial place on the loss manifold. We will come
back to it later on.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">1000</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    sampler<span style="color: #990000">=</span><span style="color: #FF0000">"GeometricLangevinAlgorithm_2ndOrder"</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
    step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span>
<span style="color: #990000">)</span>
sampling_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sample</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Sample results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>run_info<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>averages<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span></tt></pre></div></div>
<div class="paragraph"><p>Here, the <em>sampler</em> setting takes the place of the <em>optimizer</em> before as
it states which sampling scheme to use. See <a href="#reference.samplers">[reference.samplers]</a> for a
complete list and their parameter names. Apart from that the example code
is very much the same as in the example involving <span class="monospaced">fit()</span>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In the context of sampling we use <em>step_width</em> in place of <em>learning_rate</em>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Again, we produce a single data structure that contains three data frames: run
info, trajectory, and averages. Trajectories contains among others all
parameter degrees of freedom $w=\{w_1, \ldots, w_M\}$ for each step
(or <em>every_nth</em> step). Run info contains loss, accuracy, norm of gradient,
norm of noise and others, again for each step. Finally, in averages we compute
running averages over the trajectory such as average (ensemble) loss, averag
kinetic energy, average virial, see <a href="#reference.concepts">general concepts</a>.</p></div>
<div class="paragraph"><p>Take a peep at <span class="monospaced">sampling_data.run_info.columns</span> to see all columns in the
run info dataframe (and similarly for the others.)</p></div>
<div class="paragraph"><p>For the running averages it is advisable to skip some initial ateps
(<em>burn_in_steps</em>) to allow for some burn in time, i.e. for kinetic energies to
adjust from initially zero momenta.</p></div>
<div class="paragraph"><p>Some columns in averages and in run info depend on whether the sampler
provides the specific quantity, e.g. <a href="#reference.samplers.sgld">SGLD</a> does
not have momentum, hence there will be no average kinetic energy.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.sampling.priors">Using a prior</h5>
<div class="paragraph"><p>We may add a prior to the sampling. At the current state two kinds of
priors are available: wall-repelling and tethering.</p></div>
<div class="paragraph"><p>The options <em>prior_upper_boundary</em> and <em>prior_lower_boundary</em> give the admitted
interval per parameter. Within a relative distance of 0.01 (with respect to
length of domain and only in that small region next to the specified boundary)
an additional force acts upon the particles to drives them back into the desired
domain. Its magnitude increases with distance to the covered inside the boundary
region. The distance is taken to the poour of <em>prior_power</em>. The force
is scaled by <em>prior_factor</em>.</p></div>
<div class="paragraph"><p>In detail, the prior consists of an extra force added to the time integration
within each sampler. We compute its magnitude as</p></div>
<div class="mathblock">
<div class="content">\Theta(\frac{||w - \pi||}{\tau}-1.) \cdot a ||x - \pi||^n</div></div>
<div class="paragraph"><p>where <strong>w</strong> is the position of the particle, <strong>a</strong> is the <em>prior_factor</em>,
$\pi$ is the position of the boundary (<em>prior_upper_boundary</em>
$\pi_{ub}$ or <em>prior_lower_boundary</em> $\pi_{lb}$), and <strong>n</strong>
is the <span class="monospaced">prior_power</span>. Finally, the force is only in effect within a distance of
$\tau = 0.01 \cdot || \pi_{ub} - \pi_{lb} ||$ to either boundary by
virtue of the Heaviside function $\Theta()$.
Note that the direction of the force is such that it always points back into
the desired domain.</p></div>
<div class="paragraph"><p>If upper and lower boundary coincide, then we have the case of
tethering, where all parameters are pulled inward to the same point.</p></div>
<div class="paragraph"><p>At the moment applying prior on just a subset of particles is not supported.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The prior force is acting directly on the variables. It does not modify
momentum. Moreover, it is a force! In other words, it depends on step
width. If the step width is too large and if the repelling force
increases too steeply close to the walls with respect to the normal
dynamics of the system, it may blow up. On the other hand, if it is too weak,
then particles may even escape.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.sampling.optimize_then_sample">First optimize, then sample</h5>
<div class="paragraph"><p>As we have already alluded to before, optimizing before sampling is the
<strong>recommended</strong> procedure. In the following example, we concatenate the two.
To this end, we might need to modify some of the options in between. Let us have
a look, however with a slight twist.</p></div>
<div class="paragraph"><p>The dataset shown in Figure <a href="#quickstart.dataset">Dataset</a> can be even
learned by a simpler network: only one of the input nodes is actually
needed because of the symmetry.</p></div>
<div class="paragraph"><p>Hence, we look at such a network by using <em>input_columns</em> to only use input
column "x1" although the dataset contains both "x1" and "x2".</p></div>
<div class="paragraph"><p>Moreover, we will add a hidden layer with a single node and thus obtain a
network as depicted in Figure <a href="#quickstart.network">Network</a>.
We add this hidden node to make the loss manifold a little bit more
interesting.</p></div>
<div class="paragraph"><p>Additionally, we fix the biases to <strong>0</strong> for both the hidden layer bias and the
output bias. Effectively, we have two degrees of freedom left. This is not
strictly necessary but allows to plot all degrees of freedom at once.</p></div>
<div class="paragraph"><p>Finally, we add a <a href="#quickstart.simulation.sampling.priors">prior</a>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    every_nth<span style="color: #990000">=</span><span style="color: #993399">100</span><span style="color: #990000">,</span>
    fix_parameters<span style="color: #990000">=</span><span style="color: #FF0000">"layer1/biases/Variable:0=0.;output/biases/Variable:0=0."</span><span style="color: #990000">,</span>
    hidden_dimension<span style="color: #990000">=[</span><span style="color: #993399">1</span><span style="color: #990000">],</span>
    input_columns<span style="color: #990000">=[</span><span style="color: #FF0000">"x1"</span><span style="color: #990000">],</span>
    learning_rate<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">100</span><span style="color: #990000">,</span>
    optimizer<span style="color: #990000">=</span><span style="color: #FF0000">"GradientDescent"</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    sampler <span style="color: #990000">=</span> <span style="color: #FF0000">"BAOAB"</span><span style="color: #990000">,</span>
    prior_factor<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    prior_lower_boundary<span style="color: #990000">=-</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    prior_power<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    prior_upper_boundary<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">428</span><span style="color: #990000">,</span>
    step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span>
    trajectory_file<span style="color: #990000">=</span><span style="color: #FF0000">"trajectory.csv"</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
training_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">fit</span></span><span style="color: #990000">()</span>

nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">set_options</span></span><span style="color: #990000">(</span>
    friction_constant <span style="color: #990000">=</span> <span style="color: #993399">10</span><span style="color: #990000">.,</span>
    inverse_temperature <span style="color: #990000">=</span> <span style="color: #990000">.</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
    max_steps <span style="color: #990000">=</span> <span style="color: #993399">5000</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>

sampling_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sample</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Sample results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>run_info<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">])</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">])</span></tt></pre></div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Setting <em>every_nth</em> large enough is essential when playing around with
small networks and datsets as otherwise time spent writing files and adding
values to arrays will dominate the actual neural network computations by far.</td>
</tr></table>
</div>
<div class="paragraph"><p>As we see, some more options have popped up in the <span class="monospaced">__init__()</span> of the
simulation interface: <em>fix_parameters</em> is explained in section
<a href="#quickstart.simulation.setup.freezing_parameters">[quickstart.simulation.setup.freezing_parameters]</a>,
<em>hidden_dimension</em> which is a list of the number of
hidden nodes per layer, <em>input_columns</em> which contains a list of strings,
each giving the name of an input dimension (indexing starts at 1), and all
sorts of <em>prior_&#8230;</em> that define a wall-repelling prior, again see
<a id="quickstart.simulation.priors"></a> for details. This will keep parameter values
within the interval of [-2,2]. Last but not least, <em>trajectory_file</em> writes
all parameters per <em>every_nth</em> step to this file.</p></div>
<div class="paragraph"><p>Then, we call <span class="monospaced">nn.fit()</span> to perform the training as before.</p></div>
<div class="paragraph"><p>Next, we need to change the number of steps, set a sampling step width
and add the sampler (which might depend on additional parameters, see <a href="#reference.samplers">[reference.samplers]</a>
). This is done by calling <span class="monospaced">nn.set_options()</span>.</p></div>
<div class="paragraph"><p>Having set the stage for the sampling, we commence it by <span class="monospaced">nn.sample()</span>.</p></div>
<div class="paragraph"><p>At the very end we again obtain the data structure containing the <span class="monospaced">pandas</span>
DataFrame containing runtime information, trajectory, and averages as its
member variables.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">This time we need the trajectory file for the upcoming analysis. Hence,
we write it to a file using the <em>trajectory_file</em> option. Keep the file around
as it is needed in the following.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let us take a look at the two degrees of freedom of the network, namely the two
weights, where we plot one against the other similarly to the
<a href="#quickstart.simulation.optimizing.plot_optimize">Sampled weights</a> before.</p></div>
<div class="imageblock" id="quickstart.simulation.analysis.optimize_sample.weights" style="text-align:center;">
<div class="content">
<img src="./doc/userguide/pictures/weights.png" alt="pictures/weights.png" width="400">
</div>
<div class="title">Figure 5. Sampled weights: Plot of first against second weight.</div>
</div>
<div class="paragraph"><p>First of all, take note that the prior (given <em>prior_force</em> is strong enough
with respect to the chosen <em>inverse_temperature</em>) indeed retains both parameters
within the interval [-2,2] as requested.</p></div>
<div class="paragraph"><p>Compare this to the Figure <a href="#quickstart.landscape.loss">Loss manifold</a>. we
will notice that this trajectory (due to the large enough temperature) has also
jumped over the ridge around the origin.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">To bound the runtime of this example, we have set the parameters such that
we obtain a good example of a barrier-jumping trajectory. The original values
from the introduction are obtained when we reduce the <em>inverse_temperature</em>
to <strong>4.</strong> and increase <em>max_steps</em> to <strong>20000</strong> (or even more) if we do not mind
waiting a minute or two for the sampling to execute.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.analysis">2.2.7. Analysing trajectories</h4>
<div class="paragraph"><p>Analysis involves parsing in run and trajectory files that we have written
during optimization and sampling runs. Naturally, we could also perform this
on the <span class="monospaced">pandas</span> dataframes directly, i.e. sampling and analysis in the same
python script. However, for completeness we will read from files in the
examples of this section.</p></div>
<div class="paragraph"><p>The analysis functionality has been split into specific operations such as
computing averages, computing the covariance matrix or generating a diffusion
map analysis. See the source folder <span class="monospaced">src/TAT/analysis</span>, for all contained
modules therein each represent such an operation.</p></div>
<div class="paragraph"><p>In the following we will highlight just a few of them.</p></div>
<div class="paragraph"><p>All these analysis operations are also possible through <span class="monospaced">TATiAnalyser</span>, see
<span class="monospaced">tools/TATiAnalyser.in</span> in the repository.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.analysis.averages">Average parameters</h5>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>analysis<span style="color: #990000">.</span>parsedtrajectory <span style="font-weight: bold"><span style="color: #000080">import</span></span> ParsedTrajectory
<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>analysis<span style="color: #990000">.</span>averagetrajectorywriter <span style="font-weight: bold"><span style="color: #000080">import</span></span> AverageTrajectoryWriter

trajectory <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">ParsedTrajectory</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"trajectory.csv"</span><span style="color: #990000">)</span>
avg <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">AverageTrajectoryWriter</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">)</span>
steps <span style="color: #990000">=</span> trajectory<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_steps</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>avg<span style="color: #990000">.</span>average_params<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>avg<span style="color: #990000">.</span>variance_params<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>We use the helper class <span class="monospaced">ParsedTrajectory</span> which takes a trajectory file and
heeds additional options such as neglecting burn in steps or taking only
<em>every_nth</em> step into account.
Next, we instantiate the <span class="monospaced">AverageTrajectoryWriter</span> module. It computes
the averages and variance of each parameter, whose result we print.</p></div>
<div class="paragraph"><p><span class="monospaced">AverageTrajectoryWriter</span> can also write the values to a file. Then, two
rows are written (together with a header line) in CSV format. The first row
(step 0) represents the averages while the second row (step 1) represents the
variance of each parameter.</p></div>
<div class="paragraph"><p>The <strong>loss</strong> column is the average over all loss values. If an
<strong>inverse_temperature</strong> has been given, then it is the ensemble average, i.e.
each loss (and parameter set) is weighted not equivalently but by
$exp(-\beta L)$.</p></div>
<div class="paragraph"><p>In general, taking such an average is only useful if the trajectory has
remained essentially within a single minimum. If the loss manifold has the
overall shape of a large funnel with lots of local minima at the bottom, this
may be feasible as well.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Averages depend crucially on the number of steps we average over. I.e.
the more points we throw away (option <strong>every_nth</strong>), the less accurate it
becomes. In other words, if large accuracy is required, the averages data frame
(if it contains the value of interest) is a better place to look for.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.analysis.covariance">Covariance</h5>
<div class="paragraph"><p>The covariance matrix of the trajectory gives the joint variability of any two
of its components. Its eigenvalues give a notion the magnitude between
directions with large gradients and between directions with small gradients.</p></div>
<div class="paragraph"><p>In sampling this is important as directions with small gradients take longer
to be explored, see the concept of Integrated Autocorrelation Time (IAT) in
section <a href="#quickstart.sampling.iat">[quickstart.sampling.iat]</a>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">If we look at the covariance of a distribution, we essentially replace it by
a Gaussian mixture model defined by this covariance matrix.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let us compute the covariance using the analysis module <span class="monospaced">Covariance</span>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib

<span style="font-style: italic"><span style="color: #9A1900"># use agg as backend to allow command-line use as well</span></span>
matplotlib<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">use</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"agg"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib<span style="color: #990000">.</span>pyplot as plt

<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>analysis<span style="color: #990000">.</span>parsedtrajectory <span style="font-weight: bold"><span style="color: #000080">import</span></span> ParsedTrajectory
<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>analysis<span style="color: #990000">.</span>covariance <span style="font-weight: bold"><span style="color: #000080">import</span></span> Covariance

trajectory <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">ParsedTrajectory</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"trajectory.csv"</span><span style="color: #990000">)</span>

num_eigenvalues<span style="color: #990000">=</span><span style="color: #993399">2</span>
cov <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">Covariance</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">)</span>
cov<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">compute</span></span><span style="color: #990000">(</span> <span style="color: #990000">\</span>
    number_eigenvalues<span style="color: #990000">=</span>num_eigenvalues<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># plot in eigenvector directions</span></span>
x <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">matmul</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_trajectory</span></span><span style="color: #990000">(),</span> cov<span style="color: #990000">.</span>vectors<span style="color: #990000">[:,</span><span style="color: #993399">0</span><span style="color: #990000">])</span>
y <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">matmul</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_trajectory</span></span><span style="color: #990000">(),</span> cov<span style="color: #990000">.</span>vectors<span style="color: #990000">[:,</span><span style="color: #993399">1</span><span style="color: #990000">])</span>
plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">scatter</span></span><span style="color: #990000">(</span>x<span style="color: #990000">,</span>y<span style="color: #990000">,</span> marker<span style="color: #990000">=</span><span style="color: #FF0000">'o'</span><span style="color: #990000">,</span> c<span style="color: #990000">=-</span>x<span style="color: #990000">)</span>
plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">savefig</span></span><span style="color: #990000">(</span><span style="color: #FF0000">'covariance.png'</span><span style="color: #990000">,</span> bbox_inches<span style="color: #990000">=</span><span style="color: #FF0000">'tight'</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900">#plt.show()</span></span></tt></pre></div></div>
<div class="paragraph"><p>Note the helper class <span class="monospaced">ParsedTrajectory</span> which takes a trajectory file and
heeds additional options such as neglecting burn in steps or taking only
<em>every_nth</em> step into account.
This instance is handed to the <span class="monospaced">Covariance</span> class whose <span class="monospaced">compute()</span> function
performs the actual analysis. Typicallay, all analysis operations have such a
<span class="monospaced">compute()</span> function.
Again, we plot the result.</p></div>
<div class="imageblock" id="quickstart.simulation.analysis.covariance.figure" style="text-align:center;">
<div class="content">
<img src="./doc/userguide/pictures/covariance.png" alt="pictures/covariance.png" width="400">
</div>
<div class="title">Figure 6. Covariance: Trajectory plotted in the directions of two covariance eigenvectors</div>
</div>
<div class="paragraph"><p>We have depicted the weights in the direction of each eigenvector. Essentially,
we get a rotated view of the trajectory in <a href="#quickstart.simulation.analysis.optimize_sample.weights">[quickstart.simulation.analysis.optimize_sample.weights]</a>
where the <strong>x</strong> direction represents the dominant change.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.analysis.diffusion_map">Diffusion Map</h5>
<div class="paragraph"><p>Diffusion maps are a technique for unsupervised learning introduced by
<a href="#Coifman2006">[Coifman2006]</a>.</p></div>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Diffusion maps is a dimension reduction technique that can be used to discover
low dimensional structure in high dimensional data. It assumes that the data
points, which are given as points in a high dimensional metric space, actually
live on a lower dimensional structure. To uncover this structure, diffusion
maps builds a neighborhood graph on the data based on the distances between
nearby points. Then a graph Laplacian L is constructed on the neighborhood
graph. Many variants exist that approximate different differential operators.</p></div>
</div>
<div class="attribution">
<em>https://pydiffmap.readthedocs.io/en/master/theory.html</em><br>
&#8212; pydiffmap
</div></div>
<div class="paragraph"><p><a href="https://github.com/DiffusionMapsAcademics/pyDiffMap">pydiffmap</a> is an
excellent Python package that performs the analysis which consists of computing
the eigendecomposition of a sparse neighborhood graph where the Euclidean
metric is used as distance measure. If it is installed (using <span class="monospaced">pip</span>), it is
used for this type of analysis (use <span class="monospaced">method=pydiffmap</span> in this case).</p></div>
<div class="paragraph"><p>In a nutshell, the eigenvectors of the diffusion map kernel give us the main
directions on our trajectory. They represent collective variables learned
from the trajectory.</p></div>
<div class="paragraph"><p>Let us take a look at the eigenvectors from our trajectory that we have sampled
just before.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib

<span style="font-style: italic"><span style="color: #9A1900"># use agg as backend to allow command-line use as well</span></span>
matplotlib<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">use</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"agg"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib<span style="color: #990000">.</span>pyplot as plt

<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>analysis<span style="color: #990000">.</span>parsedtrajectory <span style="font-weight: bold"><span style="color: #000080">import</span></span> ParsedTrajectory
<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>analysis<span style="color: #990000">.</span>diffusionmap <span style="font-weight: bold"><span style="color: #000080">import</span></span> DiffusionMap

<span style="font-style: italic"><span style="color: #9A1900"># option values coming from the sampling</span></span>
inverse_temperature<span style="color: #990000">=</span><span style="color: #993399">2e-1</span>

trajectory <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">ParsedTrajectory</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"trajectory.csv"</span><span style="color: #990000">)</span>

num_eigenvalues<span style="color: #990000">=</span><span style="color: #993399">2</span>
dmap <span style="color: #990000">=</span> DiffusionMap<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">from_parsedtrajectory</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">)</span>
dmap<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">compute</span></span><span style="color: #990000">(</span> <span style="color: #990000">\</span>
    number_eigenvalues<span style="color: #990000">=</span>num_eigenvalues<span style="color: #990000">,</span>
    inverse_temperature<span style="color: #990000">=</span>inverse_temperature<span style="color: #990000">,</span>
    diffusion_map_method<span style="color: #990000">=</span><span style="color: #FF0000">"vanilla"</span><span style="color: #990000">,</span>
    use_reweighting<span style="color: #990000">=</span>False<span style="color: #990000">)</span>

plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">scatter</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_trajectory</span></span><span style="color: #990000">()[:,</span><span style="color: #993399">0</span><span style="color: #990000">],</span> trajectory<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_trajectory</span></span><span style="color: #990000">()[:,</span><span style="color: #993399">1</span><span style="color: #990000">],</span> c<span style="color: #990000">=</span>dmap<span style="color: #990000">.</span>vectors<span style="color: #990000">[:,</span><span style="color: #993399">0</span><span style="color: #990000">])</span>
plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">savefig</span></span><span style="color: #990000">(</span><span style="color: #FF0000">'eigenvectors.png'</span><span style="color: #990000">,</span> bbox_inches<span style="color: #990000">=</span><span style="color: #FF0000">'tight'</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900">#plt.show()</span></span></tt></pre></div></div>
<div class="paragraph"><p>We should then obtain the Figure <a href="#quickstart.simulation.analysis.diffusion_map.eigenvectors">Diffusion map analysis</a>.</p></div>
<div class="imageblock" id="quickstart.simulation.analysis.diffusion_map.eigenvectors" style="text-align:center;">
<div class="content">
<img src="./doc/userguide/pictures/eigenvectors.png" alt="pictures/eigenvectors.png" width="200">
</div>
<div class="title">Figure 7. Diffusion map analysis: Plot of first weight against second weight, colored by first eigenvector of the diffusion map kernel</div>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The true first eigenvector is constant and is therefore dropped in the
function <span class="monospaced">compute_diffusion_maps()</span>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Keep in mind that the ridge is at the origin and there are two hperbolic basins
on either side in the all-positive and all-negative orthant of the 2d space.
We see that as we color the trajectory points by the value of the dominant
eigenvector, the path between these two minima is highlighted: from
light green to dark blue.</p></div>
<div class="paragraph"><p>The eigenvector gives a direction: From the top right to the bottom left and
therefore from one minimum basin to the other.</p></div>
<div class="paragraph"><p>The eigenvector component give the implicit evaluation of the collective
variable function at each trajectory point, i.e. $e_i = \xi(x_i)$,
where $\xi(x)$ is the collective variable and $e_i$ is the
i-th component and $x_i$ the i-th trajectory point.</p></div>
<div class="paragraph"><p>The eigenvectors of the diffusion map kernel also give us a mean to assess
distances between trajectory points by looking at the difference in values,
$|e_i - e_j|$.
This is the so-called <em>diffusion distance</em>. It tells us how difficult it is to
diffuse from one point to the other.</p></div>
<div class="paragraph"><p>The main difference between the covariance and diffusion maps is that the latter
gives a non-linear mapping which is much more powerful.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion">2.2.8. Conclusion</h4>
<div class="paragraph"><p>This has been the quickstart introduction to the <span class="monospaced">simulation</span> interface.</p></div>
<div class="paragraph"><p>If we want to take this further, we recommend reading how to implement a
<a href="#reference.implementing_sampler">GLA2 sampler</a> using this module.</p></div>
<div class="paragraph"><p>If we still want to take it further, then we need to look at the
 <a href="programmersguide.html">programmer&#8217;s guide</a> 
that should accompany our installation.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.cmdline">2.3. Using command-line interface</h3>
<div class="paragraph"><p>All the tests use the command-line interface which suits itself well for
performing rigorous scientific experiments in scripted stages. We recommend
using this interface when doing parameter studies and performing extensive runs
using different seeds.</p></div>
<div class="paragraph"><p>The examples on this interface will be much briefer as you are hopefully
aware of the <span class="monospaced">simulation</span> module quickstart already and therefore much should
be familiar.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>All command-line option have equivalently named counterparts in the options given
to the <span class="monospaced">simulation</span> (or <span class="monospaced">model</span>) module on instantiation or to
<span class="monospaced">simulation.set_options()</span>, respectively.</p></div>
</td>
</tr></table>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.writing_dataset">2.3.1. Creating the dataset</h4>
<div class="paragraph"><p>As data is read from file, this file needs to be created beforehand.</p></div>
<div class="paragraph"><p>For a certain set of simple classification problems, namely those that
can be found in the tensorflow playground, we have added a
<span class="monospaced">TATiDatasetWriter</span> that spills out the dataset in CSV format.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiDatasetWriter <span style="color: #990000">\</span>
        --data_type <span style="color: #993399">2</span> <span style="color: #990000">\</span>
        --dimension <span style="color: #993399">500</span> <span style="color: #990000">\</span>
        --noise <span style="color: #993399">0.1</span> <span style="color: #990000">\</span>
        --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
        --train_test_ratio <span style="color: #993399">0</span> <span style="color: #990000">\</span>
        --test_data_file testset-twoclusters<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will write 500 datums of the dataset type 2 (<strong>two clusters</strong>) to a
file <span class="monospaced">testset-twoclusters.csv</span> using all of the points as we have set
the test/train ratio to <strong>0</strong>. Note that we also perturb the points by <strong>0.1</strong>
relative noise.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.parsing_dataset">2.3.2. Parsing the dataset</h4>
<div class="paragraph"><p>Similarly, for testing the dataset can be parsed using the same
tensorflow machinery as is done for sampling and optimizing, using</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiDatasetParser <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span></tt></pre></div></div>
<div class="paragraph"><p>where the <em>seed</em> is used for shuffling the dataset.</p></div>
<div class="paragraph"><p>This will print 20 randomly drawn items from the dataset.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.optimizing">2.3.3. Optimizing the network</h4>
<div class="paragraph"><p>As weights (and biases) are usually uniformly random initialized and the
potential may therefore start with large values, we first have to
optimize the network, using (Stochastic) Gradient Descent (GD).</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.parameter_freeze">2.3.4. Freezing parameters</h4>
<div class="paragraph"><p>Sometimes it might be desirable to freeze parameters during training or
sampling. This can be done as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>model <span style="font-weight: bold"><span style="color: #000080">import</span></span> Model as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

FLAGS <span style="color: #990000">=</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">setup_parameters</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters-small.csv"</span><span style="color: #990000">],</span>
    fix_parameters<span style="color: #990000">=</span><span style="color: #FF0000">"output/biases/Variable:0=2."</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">5</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>FLAGS<span style="color: #990000">)</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_input_pipeline</span></span><span style="color: #990000">()</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_network</span></span><span style="color: #990000">(</span>None<span style="color: #990000">,</span> setup<span style="color: #990000">=</span><span style="color: #FF0000">"train"</span><span style="color: #990000">)</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">reset_dataset</span></span><span style="color: #990000">()</span>
run_info<span style="color: #990000">,</span> trajectory<span style="color: #990000">,</span> _ <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">train</span></span><span style="color: #990000">(</span>return_run_info<span style="color: #990000">=</span>True<span style="color: #990000">,</span> <span style="color: #990000">\</span>
  return_trajectories<span style="color: #990000">=</span>True<span style="color: #990000">)</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Train results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">5</span><span style="color: #990000">]))</span></tt></pre></div></div>
<div class="paragraph"><p>Note that we fix the parameter where we give its name in full tensorflow
namescope: "layer1" for the network layer, "weights" for the weights ("biases"
alternatively) and "Variable:0" is fixed (as it is the only one). This is
followed by a comma-separated list of values, one for each component.</p></div>
<div class="exampleblock">
<div class="content">
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Single values cannot be frozen but only entire weight matrices or bias
vectors per layer at the moment. As each component has to be listed, at the
moment this is not suitable for large vectors.</td>
</tr></table>
</div>
</div></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiOptimizer <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">50</span> <span style="color: #990000">\</span>
    --loss mean_squared <span style="color: #990000">\</span>
    --learning_rate <span style="color: #993399">1e-2</span> <span style="color: #990000">\</span>
    --max_steps <span style="color: #993399">1000</span> <span style="color: #990000">\</span>
    --optimizer GradientDescent <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --save_model `pwd`/model<span style="color: #990000">.</span>ckpt<span style="color: #990000">.</span>meta <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
    -v</tt></pre></div></div>
<div class="paragraph"><p>This call will parse the dataset from the file
<span class="monospaced">dataset-twoclusters.csv</span>. It will then perform a (Stochastic) Gradient
Descent optimization in batches of <strong>50</strong> (10% of the dataset) of the
parameters of the network using a step width/learning rate of <strong>0.01</strong> and
do this for <strong>1000</strong> steps after which it stops and writes the resulting
neural network in a TensorFlow-specific format to a set of files, one of
which is called <span class="monospaced">model.ckpt.meta</span> (and the other filenames are derived
from this).</p></div>
<div class="paragraph"><p>We have also created a file <span class="monospaced">run.csv</span> which contains among others the
loss at each (<em>every_nth</em>, respectively) step of the optimization run.
Plotting the loss over the step column from the run file will result in
a figure similar to in
<a href="#quickstart.simulation.optimizing.plot">Loss history</a>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Since Tensorflow 1.4 an absolute path is required for the storing the
model. In the example we use the current directory returned by the unix
command <span class="monospaced">pwd</span>.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>If you need to compute the optimal step width, which is possible for
smaller networks from the largest eigenvalue of the hessian matrix, then
use the option <em>do_hessians 1</em> to activate it.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The creation of the nodes is costly, $O(N^2)$ in the number of
parameters of the network N. Hence, may not work for anything but small
networks and should be done on purpose.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.sampling">2.3.5. Sampling trajectories on the loss manifold</h4>
<div class="paragraph"><p>Next, we show how to use the sampling tool called <span class="monospaced">TATiSampler</span>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiSampler <span style="color: #990000">\</span>
    --averages_file averages<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">50</span> <span style="color: #990000">\</span>
    --friction_constant <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --inverse_temperature <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --loss mean_squared <span style="color: #990000">\</span>
    --max_steps <span style="color: #993399">1000</span> <span style="color: #990000">\</span>
    --sampler GeometricLangevinAlgorithm_2ndOrder <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
    --step_width <span style="color: #993399">1e-2</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will cause the sampler to parse the same dataset as before.
Afterwards it will use the <strong>GeometricLangevinAlgorithm_2nd</strong> order discetization
using again <em>step_width</em> of <strong>0.01</strong> and running for <strong>1000</strong> steps in total. The
GLA is a descretized variant of Langevin Dynamics whose accuracy scales with
the inverse square of the <em>step_width</em> (hence, 2nd order).</p></div>
<div class="paragraph"><p>The <em>seed</em> is needed as we sample using Langevin Dynamics where a noise
term is present whose magnitude scales with <span class="monospaced">inverse_temperature</span>.</p></div>
<div class="paragraph"><p>After it has finished, it will create three files; a run file
<span class="monospaced">run.csv</span> containing run time information such as the step, the
potential, kinetic and total energy at each step, a trajectory file
<span class="monospaced">trajectory.csv</span> with each parameter of the neural network at each step,
and an averages file <span class="monospaced">averages.csv</span> containing averages accumulated
along the trajectory such as average kinetic energy, average virial (
connected to the kinetic energy through the virial theorem, valid if a
prior keeps parameters bound to finite values), and the average
(ensemble) loss. Moreover, for the HMC sampler the average rejection
rate is stored there. The first two files we need in the next stage.</p></div>
<div class="paragraph"><p>As it is good practice to first optimize and then sample, also referred as to
start from an <span class="monospaced">equilibrated position</span>, we may load the model saved after
optimization by adding the option:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">...</span>
  --restore_model ‘pwd‘/model<span style="color: #990000">.</span>ckpt<span style="color: #990000">.</span>meta <span style="color: #990000">\</span>
<span style="color: #990000">...</span></tt></pre></div></div>
<div class="paragraph"><p>Otherwise we start from randomly initialized weights and non-zero biases.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.analysing">2.3.6. Analysing trajectories</h4>
<div class="paragraph"><p>Eventually, we now want to analyse the obtained trajectories. The trajectory
file written in the last step is simply a matrix of dimension (number of
parameters) times (number of trajectory steps).</p></div>
<div class="paragraph"><p>The analysis can perform a number of different operations where we name a few:</p></div>
<div class="ulist"><ul>
<li>
<p>
Calculating averages.
</p>
</li>
<li>
<p>
Calculating covariances.
</p>
</li>
<li>
<p>
Calculating the diffusion map&#8217;s largest eigenvalues and eigenvectors.
</p>
</li>
<li>
<p>
Calculating landmarks and level sets to obtain an approximation to the
free energy.
</p>
</li>
</ul></div>
<div class="sect4">
<h5 id="quickstart.cmdline.analysing.averages">Averages</h5>
<div class="paragraph"><p>Averages are calculated by specifying two options as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiAnalyser <span style="color: #990000">\</span>
    average_energies average_trajectory <span style="color: #990000">\</span>
    --average_run_file average_run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --average_trajectory_file average_trajectory<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --drop_burnin <span style="color: #993399">100</span> <span style="color: #990000">\</span>
    --every_nth <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --steps <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will load both the run file <span class="monospaced">run.csv</span> and the trajectory file
<span class="monospaced">trajectory.csv`</span> and average over them using only every <strong>10</strong> th data point
(<em>every_nth</em>) and also dropping the first steps below <strong>100</strong>
(<em>drop_burnin</em>). It will produce then ten averages (<em>steps</em>) for each of
energies in the run file and each of the parameters in the trajectories
file (along with the variance) from the first non-dropped step till one
of the ten end steps. These end steps are obtained by equidistantly
splitting up the whole step interval.</p></div>
<div class="paragraph"><p>Eventually, we have two output file. The averages over the run
information such as total, kinetic, and potential energy in
<span class="monospaced">average_run.csv</span>. Also, we have the averages over the degrees of
freedom in <span class="monospaced">average_trajectories.csv</span>.</p></div>
<div class="paragraph"><p>This second file contains two rows (together with a header line) in CSV format.
The first row (step 0) represents the averages while the second row (step 1)
represents th variance of each parameter.</p></div>
<div class="paragraph"><p>The <strong>loss</strong> column is the average over all loss values. If an
<strong>inverse_temperature</strong> has been given, then it is the ensemble average, i.e.
each loss (and parameter set) is weighted not equivalently (unit weight) but by
$exp(-\beta L)$.</p></div>
<div class="paragraph"><p>In general, taking such an average is only useful if the trajectory has
remained essentially within a single minimum. If the loss manifold has the
overall shape of a large funnel with lots of local minima at the bottom, this
may be feasible as well. Use a covariance analysis and <span class="monospaced">TATiLossFunctionSampler</span>
in directions of eigenvalues of the resulting covariance matrix whose magnitude
is large to find out.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Averages depend crucially on the number of steps we average over. I.e.
the more points we throw away (option <strong>every_nth</strong>), the less accurate it
becomes. In other words, if large accuracy is required, the averages file (if
it contains the value of interest) is a better place to look for.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="quickstart.cmdline.analysing.covariance">Covariance</h5>
<div class="paragraph"><p>Computing the covariance is done as follows.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiAnalyser <span style="color: #990000">\</span>
    covariance <span style="color: #990000">\</span>
    --covariance_matrix covariance<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --covariance_eigenvalues eigenvalues<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --covariance_eigenvectors eigenvectors<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --drop_burnin <span style="color: #993399">100</span> <span style="color: #990000">\</span>
    --every_nth <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --number_of_eigenvalues <span style="color: #993399">3</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>Here, we simply give the respective options to write the covariance matrix,
its eigenvectors and eigenvalues to CSV files. We drop the first 100 steps
and take only every 10th step into account.</p></div>
<div class="paragraph"><p>The covariance eigenvectors give use the directions of strong and weak
change while the eigenvalues give their magnitude. This correlates with strong
and weak gradients and therefore with general directions of fast and slow
exploration.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.cmdline.analysing.diffusion_map">Diffusion map</h5>
<div class="paragraph"><p>See section <a href="#quickstart.simulation.analysis.diffusion_map">[quickstart.simulation.analysis.diffusion_map]</a> for some more
information on diffusion maps.</p></div>
<div class="paragraph"><p>its eigenvalues and eigenvectors can be written as well to two output
files.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiAnalyser <span style="color: #990000">\</span>
    diffusion_map <span style="color: #990000">\</span>
    --diffusion_map_file diffusion_map_values<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --diffusion_map_method vanilla <span style="color: #990000">\</span>
    --diffusion_matrix_file diffusion_map_vectors<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --drop_burnin <span style="color: #993399">100</span> <span style="color: #990000">\</span>
    --every_nth <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --inverse_temperatur <span style="color: #993399">1e4</span> <span style="color: #990000">\</span>
    --number_of_eigenvalues <span style="color: #993399">4</span> <span style="color: #990000">\</span>
    --steps <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>The files ending in <span class="monospaced">..values.csv</span> contains the eigenvalues in two
columns, the first is the eigenvalue index, the second is the
eigenvalue.</p></div>
<div class="paragraph"><p>The other file ending in <span class="monospaced">..vectors.csv</span> is simply a matrix of the
eigenvector components in one direction and the trajectory steps in the
other. Additionally, it contains the parameters at the steps and also
the loss and the kernel matrix entry.</p></div>
<div class="paragraph"><p>Note that again the all values up till step <strong>100</strong> are dropped (due to option
<strong>drop_burnin</strong>) and only every 10th trajectory point (due to option <strong>every_nth</strong>)
is considered afterwards.</p></div>
<div class="paragraph"><p>There are two methods available. Here, we have used the simpler (and
less accurate) (plain old) <em>vanilla</em> method. The other is called <em>TMDMap</em>.</p></div>
<div class="paragraph"><p>If you have installed the <em>pydiffmap</em> python package, this may also be
specified as diffusion map method. It has the benefit of an internal
optimal parameter choice. Hence, it should behave more robustly than the
other two methods. <em>TMDMap</em> is different only in re-weighting the samples
according to the specific temperature.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.more_tools">2.3.7. More tools</h4>
<div class="paragraph"><p>There are a few more tools available in TATi. They allow to inspect the loss
manifold or the input space with respect to a certain parameter set.</p></div>
<div class="sect4">
<h5 id="quickstart.cmdline.more_tools.loss">The loss function</h5>
<div class="paragraph"><p>Let us give an example call of <span class="monospaced">TATiLossFunctionSampler</span> right away.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
  trajectory <span style="color: #990000">\</span>
  --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
  --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --parse_parameters_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>It takes as input the dataset file <span class="monospaced">dataset-twoclusters.csv</span> and
either a parameter file <span class="monospaced">trajectory.csv</span>. This will cause the program
the re-evaluate the loss function at the trajectory points which should
hopefully give the same values as already stored in the trajectory file
itself.</p></div>
<div class="paragraph"><p>However, this may be used with a different dataset file, e.g. the
testing or validation dataset, in order to evaluate the generalization
error in terms of the overall accuracy or the loss at the points along
the given trajectory.</p></div>
<div class="paragraph"><p>Interesting is also the second case, where instead of giving a
parameters file, we sample the parameter space equidistantly as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
  naive_grid <span style="color: #990000">\</span>
  --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
  --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --interval_weights -<span style="color: #993399">5</span> <span style="color: #993399">5</span> <span style="color: #990000">\</span>
  --interval_biases -<span style="color: #993399">1</span> <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --samples_weights <span style="color: #993399">10</span> <span style="color: #990000">\</span>
  --samples_biases <span style="color: #993399">4</span></tt></pre></div></div>
<div class="paragraph"><p>Here, sample for each weight in the interval [-5,5] at 11 points (10<br>
endpoint), and similarly for the weights in the interval [-1,1] at 5
points.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>For anything but trivial networks the computational cost quickly becomes
prohibitively large. However, we may use <span class="monospaced">fix_parameter</span> to lower the
computational cost by choosing a certain subsets of weights and biases to
sample.</p></div>
</td>
</tr></table>
</div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
  naive_grid <span style="color: #990000">\</span>
  --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
  --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --fix_parameters <span style="color: #FF0000">"output/weights/Variable:0=2.,2."</span> <span style="color: #990000">\</span>
  --interval_weights -<span style="color: #993399">5</span> <span style="color: #993399">5</span> <span style="color: #990000">\</span>
  --interval_biases -<span style="color: #993399">1</span> <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --samples_weights <span style="color: #993399">10</span> <span style="color: #990000">\</span>
  --samples_biases <span style="color: #993399">4</span></tt></pre></div></div>
<div class="paragraph"><p>Moreover, using <span class="monospaced">exclude_parameters</span> can be used to exclude parameters
from the variation, i.e. this subset is kept at fixed values read from
the file given by <span class="monospaced">parse_parameters_file</span> where the row designated by
the value in <span class="monospaced">parse_steps</span> is taken.</p></div>
<div class="paragraph"><p>This can be used to assess the shape of the loss manifold around a found
minimum.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
  naive_grid <span style="color: #990000">\</span>
  --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
  --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --exclude_parameters <span style="color: #FF0000">"w1"</span> <span style="color: #990000">\</span>
  --interval_weights -<span style="color: #993399">5</span> <span style="color: #993399">5</span> <span style="color: #990000">\</span>
  --interval_biases -<span style="color: #993399">1</span> <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --parse_parameters_file centers<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --parse_steps <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --samples_weights <span style="color: #993399">10</span> <span style="color: #990000">\</span>
  --samples_biases <span style="color: #993399">4</span> <span style="color: #990000">\</span>
  -vv</tt></pre></div></div>
<div class="paragraph"><p>Here, we have excluded the second weight, named <strong>w1</strong>, from the sampling.
Note that all weight and all bias degrees of freedom are simply
enumerated one after the other when going from the input layer till the
output layer.</p></div>
<div class="paragraph"><p>Furthermore, we have specified a file containing center points for all
excluded parameters. This file is of CSV style having a column <strong>step</strong> to
identify which row is to be used and moreover a column for every
(excluded) parameter that is fixed at a value unequal to 0. Note that
the minima file written by <span class="monospaced">TATiExplorer</span> can be used as this centers
file. Moreover, also the trajectory files have the same structure.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.cmdline.more_tools.inputspace">The learned function</h5>
<div class="paragraph"><p>The second little utility programs does not evaluate the loss function
itself but the unknown function learned by the neural network depending
on the loss function, called the <span class="monospaced">TATiInputSpaceSampler</span>. In other
words, it gives the classification result for data point sampled from an
equidistant grid. Let us give an example call right away.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiInputSpaceSampler <span style="color: #990000">\</span>
        --batch_data_files grid<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --csv_file TATiInputSpaceSampler-output<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --input_dimension <span style="color: #993399">2</span> <span style="color: #990000">\</span>
        --interval_input -<span style="color: #993399">4</span> <span style="color: #993399">4</span> <span style="color: #990000">\</span>
        --parse_steps <span style="color: #993399">1</span> <span style="color: #990000">\</span>
        --parse_parameters_file trajectory<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --samples_input <span style="color: #993399">10</span> <span style="color: #990000">\</span>
        --seed <span style="color: #993399">426</span></tt></pre></div></div>
<div class="paragraph"><p>Here, <span class="monospaced">batch_data_files</span> is an input file but it does not need to be
present. (Sorry about that abuse of the parameter as usually
<span class="monospaced">batch_data_files</span> is read-only. Here, it is overwritten!). Namely, it
is generated by the utility in that it equidistantly samples the input
space, using the interval [-4,4] for each input dimension and 10+1
samples (points on -4 and 4 included). The parameters file
<span class="monospaced">trajectory.csv</span> now contains the values of the parameters (weights
and biases) to use on which the learned function depends or by, in other
words, by which it is parametrized. As the trajectory contains a whole
flock of these, the <span class="monospaced">parse_steps</span> parameter tells it which steps to
use for evaluating each point on the equidistant input space grid,
simply referring to rows in said file.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>For anything but trivial input spaces the computational cost quickly
becomes prohibitively large. But again <span class="monospaced">fix_parameters</span> is heeded and can be
used to fix certain parameters. This is even necessary if parsing a trajectory
that was created using some parameters fixed as they then will <em>not</em>
appear in the set of parameters written to file. This will raise an
error as the file will contain too few values.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.conclusion">2.4. Conclusion</h3>
<div class="paragraph"><p>This has been the quickstart introduction.</p></div>
<div class="paragraph"><p>In the following reference section you may find the following pieces
interesting after having gone through this quickstart tutorial.</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="#reference.examples.harmonic_oscillator">[reference.examples.harmonic_oscillator]</a> for a light-weight example
  probability distribution function whose properties are well understood.
</p>
</li>
<li>
<p>
<a href="#reference.implementing_sampler">[reference.implementing_sampler]</a> explaining how to implement your own
  sampler using the <span class="monospaced">Simulation</span> module as rapid-prototyping framework.
</p>
</li>
<li>
<p>
<a href="#reference.simulation">[reference.simulation]</a> giving detailed examples on each function in
  <span class="monospaced">Simulations</span><em>s interface</em>.
</p>
</li>
</ul></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="reference">3. The reference</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="reference.concepts">3.1. General concepts</h3>
<div class="paragraph"><p>Here, we give definitions on a number of concepts that occur throughout this
userguide.</p></div>
<div class="ulist"><ul>
<li>
<p>
Dataset
</p>
<div class="paragraph"><p>The dataset $D = \{X,Y\}$ contains a fixed number of datums of input
tuples or features $X=\{x_d\}$ and output tuples or labels
$Y=\{y_d\}$. Basically, they are samples taken from
the unknown function which we wish to approximate using the neural
network. If the output tuples are binary in each component, the
approximation problem is called a <em>classification</em> problem. Otherwise,
it is a <em>regression</em> problem.</p></div>
</li>
<li>
<p>
Neural network
</p>
<div class="paragraph"><p>The neural network is a black-box representing a certain set of general
functions that are efficient in solving classification problems (among others).
They are parametrized explicitly using weights and biases
$w=\{w_1, \ldots, w_M\}$ and implicitly through the topology of the
network (connections of nodes residing in layers) and the activation functions
used. The network&#8217;s output $F_w(x_i)$ depends explicitly on the
dataset&#8217;s current datum (fed into the network) and implicitly on the parameters.</p></div>
</li>
<li>
<p>
Loss
</p>
<div class="paragraph"><p>The default is <em>mean_squared</em>.</p></div>
<div class="paragraph"><p>The loss function $L_D(w)$ determines for a given (labeled) dataset
what set of neural network&#8217;s parameters are best. Different losses result in
different set of parameters. It is a high-dimensional manifold that we want to
learn and capture using the neural network. It implicitly depends on the given
dataset and explicitly on the parameters of the neural network, namely
weights and biases, $w=\{w_1, \ldots, w_M\}$.</p></div>
<div class="paragraph"><p>Most important to understand about the loss is that it is a <em>non-convex</em>
function and therefore in general does not just have a single minimum.
This makes the task of finding a good set of parameters that (globally)
minimize the loss difficult as one would have to find each and every
minima in this high-dimensional manifold and check whether it is
actually the global one.</p></div>
</li>
<li>
<p>
Momenta and kinetic energy
</p>
<div class="paragraph"><p>Momenta $p=\{p_1, \ldots, p_M\}$ is a concept taken over from physics
where the parameters are considered as particles each in a one-dimensional
space where the loss is a potential function whose ( negative) gradient acts as
a force onto the particle driving them down-hill (towards the local minimum).
This force is integrated in a classical Newton&#8217;s mechanic style, i.e.
Newton&#8217;s equation of motion is discretized with small time steps
(similar to the learning rate in Gradient Descent). This gives first
rise to/velocity and second to momenta, i.e. second order ordinary
differential equation (ODE) split up into a system of two
one-dimensional ODEs. There are numerous stable time integrators, i.e.
velocity Verlet/Leapfrog, that are employed to propagate both particle
position (i.e. the parameter value) and its momentum through time. Note
that momentum and velocity are actually equivalent as usually the mass
is set to unity.</p></div>
<div class="paragraph"><p>The kinetic energy $0.5 \sum^M_i p^T_i p_i$ is computed as sum over
kinetic energies of each parameter.</p></div>
</li>
<li>
<p>
Virial
</p>
<div class="paragraph"><p>Virial $0.5 \sum^M_i w^T_i \nabla_{w_i} L_D(w)$ is defined as one
half of the sum over the scalar product of gradients with parameters. Given
that the loss function is unbounded from above and raises more quickly than a
linear function, the asymptotic value of the virial is related to the average
kinetic energy through the virial theorem, see
<a href="https://en.wikipedia.org/wiki/Virial_theorem">https://en.wikipedia.org/wiki/Virial_theorem</a>.</p></div>
</li>
<li>
<p>
Averages
</p>
<div class="paragraph"><p>Averages are meaningful when looking at the probability distribution function
instead of the loss directly, see Section <a href="#quickstart.sampling.sequences">[quickstart.sampling.sequences]</a>.
There, we compute the integral $\int A(w,p) \mu(w,p)$ for in our case
the (canonical) Gibbs measure $\mu(w,p) = \exp(-\beta L_D(w))$ that
turns the loss into probability distribution function. Here, $A(w,p)$
is an observable, i.e. a function depending on positions and momenta. Examples
of such observables are the kinetic energy or the virial, or maybe the
parameters themselves.</p></div>
</li>
</ul></div>
</div>
<div class="sect2">
<h3 id="reference.examples">3.2. Examples</h3>
<div class="paragraph"><p>We show how to set up some basic models, which can be a useful tool
to study and test methods.</p></div>
<div class="sect3">
<h4 id="reference.examples.harmonic_oscillator">3.2.1. Harmonic oscillator</h4>
<div class="paragraph"><p>It can be sometimes useful to use TATi to simulate a simple harmonic oscillator.
This model can be obtained by training the neural network with one parameter
on one point $X=1$ in dimension one with a label
equal to zero, i.e. $Y=0$, and using the mean square loss function and
linear activation function.
More precisely, the cost function becomes in this setting:</p></div>
<div class="paragraph"><p>$L(\omega | X,Y) = | \omega X + b - Y|^2$,</p></div>
<div class="paragraph"><p>where we fix the bias $b=0$.</p></div>
<div class="ulist"><ul>
<li>
<p>
Dataset:
</p>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>common <span style="font-weight: bold"><span style="color: #000080">import</span></span> data_numpy_to_csv
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

X <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">([[</span><span style="color: #993399">1</span><span style="color: #990000">]])</span>
Y <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">([[</span><span style="color: #993399">0</span><span style="color: #990000">]])</span>

<span style="font-style: italic"><span style="color: #9A1900"># prepare and save the trivial data set for later</span></span>
datasetName <span style="color: #990000">=</span> <span style="color: #FF0000">'dataset_ho.csv'</span>
<span style="font-weight: bold"><span style="color: #000000">data_numpy_to_csv</span></span><span style="color: #990000">(</span>X<span style="color: #990000">,</span> Y<span style="color: #990000">,</span> datasetName<span style="color: #990000">)</span>
numberOfPoints <span style="color: #990000">=</span> <span style="color: #993399">1</span></tt></pre></div></div>
</li>
<li>
<p>
Setup and train neural network:
</p>
<div class="listingblock">
<div class="content monospaced">
<pre>import TATi.simulation as tati

import numpy as np
import pandas as pd

nn = tati(
    batch_data_files=["dataset_ho.csv"],
    batch_size=1,
    fix_parameters="output/biases/Variable:0=0.",
    friction_constant=10.0,
    input_dimension=1,
    inverse_temperature=1.0,
    loss="mean_squared",
    max_steps=1000,
    output_activation = "linear",
    output_dimension=1,
    run_file="run_ho.csv",
    sampler="BAOAB",
    seed=426,
    step_width=0.5,
    trajectory_file="trajectory_ho.csv",
    verbose=1
)

data = nn.sample()

df_trajectories = pd.DataFrame(data.trajectory)
# sampled trajectory
weight0 = np.asarray(df_trajectories['weight0'])</pre>
</div></div>
</li>
<li>
<p>
Sampled trajectory:
</p>
<div class="paragraph"><p>The output trajectory in <span class="monospaced">trajectory_ho.csv</span> or <span class="monospaced">weight0</span> is distributed w.r.t.
a Gaussian, i.e. the density of X:=weight0 is $exp(-X^2)$, see Figure
<a href="#reference_simulation_harmonic_oscillator.harmonic_oscillator">Gaussian distribution</a>.</p></div>
</li>
</ul></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The figure was obtained with setting <span class="monospaced">max_steps</span> to 10000.</td>
</tr></table>
</div>
<div class="imageblock" id="reference.examples.harmonic_oscillator.density">
<div class="content">
<img src="./doc/userguide/pictures/density_harmonic_oscillator.png" alt="density harmonic oscillator" width="400">
</div>
<div class="title">Figure 8. Gaussian distribution: Histogram of the trajectories obtained by simulating 1D harmonic oscillator with BAOAB sampler.</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reference.optimizers">3.3. Optimizers</h3>
<div class="paragraph"><p>Optimizers in TATi primarily serve the purpose of finding a good starting
position for the samplers.</p></div>
<div class="paragraph"><p>To this end, only a few optimizers are currently implemented.</p></div>
<div class="ulist"><ul>
<li>
<p>
(Stochastic) Gradient Descent
</p>
</li>
<li>
<p>
Gradient Descent with step width calculated following Barzilai-Borwein
</p>
</li>
</ul></div>
<div class="sect3">
<h4 id="reference.optimizers.gd">3.3.1. Gradient Descent</h4>
<div class="paragraph"><p><strong>optimizer</strong>: <span class="monospaced">GradientDescent</span></p></div>
<div class="paragraph"><p>This implementation directly calls upon Tensorflow&#8217;s <span class="monospaced">GradientDescentOptimizer</span>.
It is slightly modified for additional book-keeping of norms of gradients and
the virial (this can be deactivated setting <em>do_accumulates</em> to <strong>False</strong>).</p></div>
<div class="paragraph"><p>The step update is:
$\theta^{n+1} = \theta^{n} - \nabla  U(\theta) \Delta t$,</p></div>
</div>
<div class="sect3">
<h4 id="reference.optimizers.bbgd">3.3.2. Gradient Descent with Barzilai-Borwein step width</h4>
<div class="paragraph"><p><strong>optimizer</strong>: <span class="monospaced">BarzilaiBorweinGradientDescent</span></p></div>
<div class="paragraph"><p>The update step is the same as for <a href="#reference.optimizers.gd">[reference.optimizers.gd]</a>. However, the
learning rate $\Delta t$ is modified as, see <a href="#Barzilai1988">[Barzilai1988]</a>,</p></div>
<div class="paragraph"><p>$\Delta t = \frac{ (\theta^{n} - \theta^{n-1})\cdot(\nabla  U(\theta^{n})- \nabla  U(\theta^{n-1})) }{ |(\nabla  U(\theta^{n})- \nabla  U(\theta^{n-1})|^2 }$.</p></div>
<div class="paragraph"><p>where the learning rate is bounded in the interval [1e-10,1e+10] to ensure
numerical stability. If the above calculation should be invalid, the default
learning rate $\Delta t$ is used.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">At the moment, this is only implemented for <a href="#GD">[GD]</a>, not for <a href="#SGD">[SGD]</a>.
See <a href="#Tan2016">[Tan2016]</a> for a proposed SGD-BB method. It is not suitable for
mini-batching as the secant computation relies on exact gradients.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reference.samplers">3.4. Samplers</h3>
<div class="paragraph"><p>Samplers are discretizations of a given dynamical system, set by an ODE or
SDE. In our case, it is mostly Langevin dynamics. Given that the  sampler
is ergodic we can replace integrals over the whole domain by integrals along
sufficiently long trajectories.</p></div>
<div class="paragraph"><p>Certain asymptotical properties are inherently connected to the respective
dynamics, for example, the average kinetic energy. It can be evaluated as
an average by integrating along the trajecories. However, as it is not possible
to integrate the continuous dynamics directly, we rely on the respective
discretization, the sampler, which naturally introduces an error.</p></div>
<div class="paragraph"><p>This discretization error depends on the chosen <em>finite step width</em> by which
the trajectories are produced. It shows as a finite error to the
asymptotical value regardless of the length of the trajectory. Choosing a
smaller step width will produce a smaller error.</p></div>
<div class="paragraph"><p>This can be observed for the average kinetic energy by looking at a small
example network and dataset and producing sufficiently long trajectories.
If one plots the difference against the known asymptotical value (namely
$\frac{N^{dof}}{2} k_B T$ with temperature <strong>T</strong>, and degrees of
freedom $N^{dof}$) over different step widths in double
logarithmic fashion, one obtains straight lines whose slope depends on the
discretization order.</p></div>
<div class="paragraph"><p>Different samplers have different discretization orders and the order also
depends on the observed quantity.</p></div>
<div class="paragraph"><p>This makes picking the right sampler a critical choice: From a statistical point
of view the most accurate sampler is best, i.e. the one having the highest
discretization order. Also, as all of them have roughly the same
computational cost, it is generally recommended to pick BAOAB which has second
order convergence and even fourth order in the high-friction limit, see
<a href="#Leimkuhler2012">[Leimkuhler2012]</a> for details.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">At the beginning of each the following subsections we give the name of
the respective sampler in order to activate it using the <strong>sampler</strong> keyword,
see <a href="#quickstart.simulation.sampling">[quickstart.simulation.sampling]</a> and <a href="#quickstart.cmdline.sampling">[quickstart.cmdline.sampling]</a>.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In explaining implementation details, we will be using the B,A,O notation
for giving the order of the split analytic integration steps: B means
momentum integration, A stands for position integration and O is associated
with the noise step.</td>
</tr></table>
</div>
<div class="sect3">
<h4 id="reference.samplers.sgld">3.4.1. Stochastic Gradient Langevin Dynamics</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">StochasticGradientLangevinDynamics</span></p></div>
<div class="paragraph"><p>The Stochastic Gradient Langevin Dynamics (SGLD) was proposed by
<a href="#Welling2011">[Welling2011]</a> based on the <a href="#SGD">[SGD]</a>[Stochastic Gradient Descent], which is a
variant of the <a href="#GD">[GD]</a>[Gradient Descent] using only a subset of the dataset
for computing gradients. The central idea behind SGLD was to add an
additional noise term whose magnitude then controls the noise induced by
the approximate gradients. By a suitable reduction of the step width the
method can be shown to be globally convergent. This reduction is typically not
done in practice.</p></div>
<div class="paragraph"><p>Implements a Stochastic Gradient Langevin Dynamics Sampler in the form of a
TensorFlow Optimizer, overriding <span class="monospaced">tensorflow.python.training.Optimizer</span>.</p></div>
<div class="paragraph"><p>The step update is:
$\theta^{n+1} = \theta^{n} - \nabla  U(\theta) \Delta t + \sqrt{\frac{2\Delta t} {\beta}} G^n$,
where
$\beta$ is the inverse temperature coefficient, $\Delta t$ is the (discretization)
step width and $\theta$ is the parameter vector, $U(\theta)$ the energy or loss function,
and $G\sim N (0, 1)$.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 1. Table of parameters for SGLD</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p><strong>SGLD</strong> is very much like <strong>SGD</strong> and <strong>GD</strong> in terms that the <span class="monospaced">step_width</span> needs
to be small enough with respect to the gradient sizes of your problem.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.ccadl">3.4.2. Covariance Controlled Adaptive Langevin</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">CovarianceControlledAdaptiveLangevin</span></p></div>
<div class="paragraph"><p>This is an extension of Stochastic Gradient Descent proposed by
<a href="#Shang2015">[Shang2015]</a>. The key idea is to dissipate the extra heat caused by the
approximate gradients through a suitable thermostat. However, the
discretisation used here is not based on the (first-order)
Euler-Maruyama as <a href="#SGLD">[SGLD]</a> but on <a href="#GLA">[GLA]</a> 2nd order.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 2. Table of parameters for CCAdL</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">friction_constant</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">friction constant $\gamma$ that controls
how much momentum is replaced by random noise each step</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the noise,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">sigma</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">controlling the thermostat</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">sigmaA</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">controlling the thermostat&#8217;s adaptivity</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p><span class="monospaced">sigma</span> and <span class="monospaced">sigmaA</span> are two additional parameters that control the action
of the thermostat. Moreover, we require the same parameters as for <a href="#GLA">[GLA]</a> 2nd
order.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.gla">3.4.3. Geometric Langevin Algorithms</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">Geometric Langevin Algorithms_1stOrder</span>, <span class="monospaced">Geometric Langevin Algorithms_2ndOrder</span></p></div>
<div class="paragraph"><p>GLA results from a first-order splitting between the Hamiltonian and the
Ornstein-Uhlenbeck parts, see <a href="#Leimkuhler2015">[Leimkuhler2015]</a>[section 2.2.3, Leimkuhler 2015] and also
<a href="#Leimkuhler2012">[Leimkuhler2012]</a>. If the Hamiltonian part is discretized with a scheme of second order as in <strong>GLA2</strong>,
it provides second order accuracy at basically no extra
cost.</p></div>
<div class="paragraph"><p>The update step of the parameters for second order GLA is BABO:</p></div>
<div class="paragraph"><p>$B: p^{n+1/2} = p^n -\nabla U(q^n)\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$A: q^{n+1} =q^n+M^{-1}p^{n+1/2}\Delta t$</p></div>
<div class="paragraph"><p>$B: \tilde{p}^{n+1} = p^{n+1/2} -\nabla U(q^{n+1})\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$O: p^{n+1} = \alpha_{\Delta t}\tilde{p}^{n+1}+ \sqrt{\frac{1-\alpha_{\Delta t}^2}{\beta}M}G^n$
where $\alpha_{\Delta t}=exp(-\gamma \Delta t)$ and $G\sim N (0, 1)$.</p></div>
<div class="paragraph"><p>The first order GLA is BAO.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 3. Table of parameters for GLA1 and GLA2</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">friction_constant</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">friction constant $\gamma$ that controls
how much momentum is replaced by random noise each step</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the noise,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>All GLA samplers have two more parameters: <span class="monospaced">inverse_temperature</span> (usually denoted as $\beta$) and <span class="monospaced">friction_constant</span> (usually denoted as $\gamma$). Inverse
temperature controls the average momentum of each parameter while the
friction constant decides over how much of the momentum is replaced by
random noise, i.e. the random walker character of the trajectory.</p></div>
<div class="paragraph"><p>Good values for beta depend on the loss manifold and its barriers and
need to be find by try&amp;error at the moment.</p></div>
<div class="paragraph"><p>As a rough guide, $\gamma=10$ is a good start for the friction
constant. Moreover, when choosing such a friction constant, with
$\beta=1000$ sampling will remain in the starting minimum basin,
while $\beta=1$ exists basins very soon. Note that large noise can
be hidden by a too small friction constant, i.e. they both depend on each
other.</p></div>
</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="reference.samplers.gla.implementation">Implementation notes</h5>
<div class="paragraph"><p>The first order GLA with its BAO sequence of steps is implemented in a
straight-forward fashion. GLA2 with its BABO sequence is more difficult as we
would need to calculate the updated gradients for the second "B" step.
Tensorflow, however, by its code design, only allows gradient (and loss)
evaluation at the very beginning of the sequence.</p></div>
<div class="paragraph"><p>To overcome this, we shuffle the steps in a cyclic fashion to become BOBA.
This can be seen as delaying some steps at iteration $n$ and
performing them at iteration $n+1$. Here, we delay the action of
the "BO" part to the next step.</p></div>
<div class="paragraph"><p>When doing this, we have to take extra care to still calculate all energies
at the correct time: the only "A" step is the last and loss evaluation is at
the right moment. Moreover, both "B" steps now use the same gradient and we
do not need to recalculate. The only minor difficulty is that the kinetic
energy must be evaluated after the "O" step. This is no problem as the
kinetic energy is not computed by Tensorflow and therefore we are not
constrained.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">This cyclic permutation still changes the very first step that is
however typically started from a random position. Hence, this has no effect.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.baoab">3.4.4. BAOAB</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">BAOAB</span></p></div>
<div class="paragraph"><p>BAOAB derives from the basic building blocks A (position update), B
(momentum update), and O (noise update) into which the Langevin system
is split up. Each step is solved in a separate step. Hence, we perform a
B step, then an A step, &#8230; and so on. This scheme has second-order
accuracy and superb overall accuracy with respect to positions. See
<a href="#Leimkuhler2012">[Leimkuhler2012]</a> for more details.</p></div>
<div class="paragraph"><p>The update step of the parameters is:</p></div>
<div class="paragraph"><p>$B: p^{n+1/2} = p^n -\nabla U(q^n)\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$A: q^{n+1} =q^n+M^{-1}p^{n+1/2}\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$O: p^{n+1} = \alpha_{\Delta t}\tilde{p}^{n+1}+ \sqrt{\frac{1-\alpha_{\Delta t}^2}{\beta}M}G^n$</p></div>
<div class="paragraph"><p>$A: q^{n+1} =q^n+M^{-1}p^{n+1/2}\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$B: p^{n+1/2} = p^n -\nabla U(q^n)\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>where $\alpha_{\Delta t}=exp(-\gamma \Delta t)$ and $G\sim N (0, 1)$.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 4. Table of parameters for BAOAB</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">friction_constant</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">friction constant $\gamma$ that controls
how much momentum is replaced by random noise each step</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the noise,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="sect4">
<h5 id="reference.samplers.baoab.implementation">Implementation notes</h5>
<div class="paragraph"><p>With BAOAB we face similar difficulties as with second order GLA: we would need
to recalculate gradients for the second "B" step. Again, we solve this by
cyclic permutation of the steps to become BBAOA. In other words, we delay
the action of the very last "B" step to iteration $n+1$.</p></div>
<div class="paragraph"><p>Now, gradient calculation is at the very beginning and both "B" steps use the
same gradient. Loss calculation is still after the second "A" step. The only
minor difficulty is again calculating the kinetic energy: this needs to occur
in between the two "B" steps. Naturally, this is now also delayed to the next
step. However, as the loss is always computed prior to updating the state
they still match.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.hmc">3.4.5. Hamiltonian Monte Carlo</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">HamiltonianMonteCarlo_1stOrder</span> (modified Euler),
<span class="monospaced">HamiltonianMonteCarlo_2ndOrder</span> (Leapfrog)</p></div>
<div class="paragraph"><p>HMC is based on Hamiltonian dynamics instead of Langevin Dynamics. Noise
only enters when, after the evaluation of an <em>acceptance criterion</em>, the
momenta are redrawn randomly. It has first been proposed by <a href="#Duane1987">[Duane1987]</a>.</p></div>
<div class="paragraph"><p>This Metropolisation ensures that too large step widths will not negatively
affect the sampled distribution. In contrast to the sampling through
Langevin Dynamics there is no bias with respect to the step width. However,
a large step width will potentially cause a higher rejection rate.</p></div>
<div class="paragraph"><p>One further virtue of HMC is when using longer Verlet time integration
legs (<span class="monospaced">hamiltonian_dynamics_time</span> larger than <span class="monospaced">step_width</span>) that the walker
will progress much further than in the case of Langevin Dynamics. This is
because it only uses the gradient. In contrast to samplers based on Langevin
dynamics, it is not performing partly a Brownian motion through the injected
noise. Langevin dynamics can be recovered by using just a single step for
time integration (i.e. a single step is immediately followed by evaluating the
acceptance criterion). This is called "Langevin Monte Carlo" which does however
lack the favorable scaling properties of HMC, i.e. when using multiple steps.
Random walk scales as $d^2$ in the amount of computation time for
a dataset of dimensionality $d$, while HMC scales only as
$d^{5/4}$. LMC on the other hand scales as $d^{4/3}$, see
<a href="#Neal2011">Neal, 2011, section 4.4 and 5.2</a>.</p></div>
<div class="paragraph"><p>However, longer legs again typically cause higher rejection rates. Therefore,
they come at the price of extra computational work in terms of possibly
rejected legs. There are specific rejection rates that are optimal.
Typically they range between 35% ($L\gg 1$) and 43% ($L=1$).
In essence, the rejection rate tells about getting sufficiently far from the
initial state to a truly independent proposal state.
See  <a href="#Neal2011">Neal, 2011, section 5.2</a> for details and in general
<a href="#Neal2011">[Neal2011]</a>  for a very readable, general introduction to HMC in the context
of Markov  Chain Monte Carlo methods.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 5. Table of parameters for HMC</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">hamiltonian_dynamics_time</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">Time used for integrating Hamiltonian
dynamics. This is relative to <span class="monospaced">step_width</span>. For example, if <span class="monospaced">step_width</span> is
<strong>0.05</strong> and this is chosen as <strong>0.05</strong> as well, it will evaluate every other step,
i.e. perform only a single time integration step before evaluating the
acceptance criterion. If you want to evaluate every <strong>n</strong> steps, then use
$n \cdot \Delta t$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the initially
randomly drawn momenta,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">Both the exact amount of Hamiltonian dynamics time integration
steps and the step width are slightly varied (uniformly randomly varied by
a factor in [0.9,1.1] for integration time and in [0.7,1.3] for the step
width) such that not all walkers evaluate at the same step and are
correlated thereby. This also avoids issues with periodicities in the dataset,
see <a href="#Neal2011"> Neal, 2011, end of section 3.2, 3.3, and section 4.2 on trajectory length </a>
and also a good illustration in <a href="#MacKenzie1989">MacKenzie, 1989</a>.</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="reference.samplers.hmc.implementation">Implementation notes</h5>
<div class="paragraph"><p>Tensorflow&#8217;s code base causes some restrictions on the possible implementation
of the samplers as we have already seen.</p></div>
<div class="paragraph"><p>This is even more severe for the Hamiltonian Monte Carlo as it branches a lot
and does not perform the same computations in every iteration as do the
samplers based on Langevin Dynamics.</p></div>
<div class="paragraph"><p>Therefore, we will briefly discuss some of the pecularities. Moreover, we
explain in detail the values in each column of the three output files or
data frames: run info, trajectory, averages.</p></div>
<div class="paragraph"><p><a href="#Neal2011">[Neal2011]</a> gives the following steps for the HMC:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
New values for the momentum $p_n$ variables are randomly drawn from
their Gaussian distribution. The positions $q_n$ do not change.
</p>
</li>
<li>
<p>
A Metropolis update is performed, using Hamiltonian dynamics to propose a new
state $(q_{n+1},p_{n+1})$.
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
Start with an initial state $(q_{n,0},p_{n,0}) := (q_n,p_n)$
</p>
</li>
<li>
<p>
Simulate Hamiltonian Dynamics for $L$ steps using Leapfrog (or some
other volume preserving/reversible method): $(q_{n,L},p_{n,L})$
</p>
</li>
<li>
<p>
Momentum is negated at the leg&#8217;s end: $(q_{n,L},-p_{n,L})$
</p>
</li>
<li>
<p>
Accept the proposed state with probability
$min(1,exp(-U(q_{n,L})+U(q_n)-T(-p_{n,L})+T(p_n)))$
</p>
<div class="olist lowerroman"><ol class="lowerroman">
<li>
<p>
Accept: Set $(q_{n+1},p_{n+1}) := (q_{n,L},-p_{n,L})$
</p>
</li>
<li>
<p>
Reject: Set $(q_{n+1},p_{n+1}) := (q_{n},p_{n})$
</p>
</li>
</ol></div>
</li>
</ol></div>
</li>
</ol></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The negation of momentum is not needed in practice but makes the proposal
symmetric which is relevant for the method&#8217;s analysis.</td>
</tr></table>
</div>
<div class="paragraph"><p>As we see, we need to distinguish between either acceptance evaluation or
time integration. Moreover, we need to branch on whether we accept or reject
the proposed state. Finally, on rejection the initial state is actually
repeated (also in the output and averaging, see
<a href="#Neal2011"> Neal, 2011, section 3.2, paragraph "two steps" </a>).</p></div>
<div class="paragraph"><p>This means that one sampling step will perform (exclusively) either an
evaluation or a single integration step. The sampling function is given the
current step and the next evaluation step. When both are equal, we do not
perform an integration ($q$ does not change) but redraw momenta and
evaluate the criterion (this may change $q$, restoring it to its
old value). For more details on how to implement branching with Tensorflow,
we refer to the <a href="#extensions.samplers.branching">programmer&#8217;s guide</a>.</p></div>
<div class="paragraph"><p>On top of that we have the Tensorflow constraint that the loss and gradient
evaluation occurs always at the very beginning of the sampling step. This
means we need to restore the loss, gradients, virials and other energies
when the proposed state is rejected. Also, the loss always lags behind one
step, i.e. we see $L(q_{n-1})$ and $T(p_n)$. Note that
this does not affect the criterion evaluation as this is an extra step and
the loss is updated in time.</p></div>
<div class="paragraph"><p>In TATi there are two time integration methods implemented for HMC: <em>modified
Euler method</em> and <em>Leapfrog</em> (also called Velocity Verlet).
These are available as <span class="monospaced">HamiltonianMonteCarlo_1stOrder</span> (Euler) and
<span class="monospaced">HamiltonianMonteCarlo_2ndOrder</span> (Leapfrog). If $n$ is the number
of hamiltonian dynamics steps, then the first order method will perform
$n+1$ steps (and gradient evaluations), while the second order
method executes $n+2$ steps. This is because of the aforementioned
constraint in the Tensorflow code base.</p></div>
<div class="paragraph"><p>The extra step for Leapfrog is because we again need to use cyclic permutation
(similar to <a href="#reference.samplers.gla.implementation">GLA2</a>) to turn "BAB"
into "BBA". This entails shifting the very last "B" step into an extra step to
properly evaluate the kinetic energy before evaluating the acceptance
criterion. This is not needed for the modified Euler which performs only "BA".</p></div>
<div class="paragraph"><p>Finally, in the output files or data frames only the final state is seen. We
do not output any of the intermediate states $(q_{n,i}, p_{n,i})$
during integration.</p></div>
<div class="paragraph"><p>As a last remark, the very first step is always an evaluation step that we also
always accept. Ths is to properly initialize certain internal variables. Note
that this initial accept is counted for obtaining the rejection rate.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">If you want to inspect very closely what is going on in each step, use
verbosity level of 2 by either using <span class="monospaced">-vv</span> for the command-line interface
or setting <span class="monospaced">params.verbose = 2</span> in the simulation/python interface and follow
the "DEBUG" output in the terminal.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let us then look at the contents of the files/data frames, where we skip
some columns that are not affected (such as <em>id</em>, &#8230;). Note that the state
seen in those files is always the state <em>after</em> full evaluation of the
acceptance criterion and redrawing of momenta, i.e. just before step "2." in
the algorithm given in <a href="#Neal2011">[Neal2011]</a>.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Run info
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
<em>step</em>: shows the evaluation steps only, e.g., 1 (n=1), 7 (n=2), 13 (n=3) for
L=5 and HMC_1st.
</p>
</li>
<li>
<p>
<em>loss</em>: shows $L(q_n)$, i.e. the loss of the current state, either
accepted or rejected.
</p>
</li>
<li>
<p>
<em>total_energy</em>: shows the total energy $L(q_{n-1,L})+T(-p_{n-1,L})$ of
the proposed state used for the acceptance evaluation. Note that this is <em>not</em>
the sum of <em>loss</em> and <em>kinetic_energy</em>.
</p>
</li>
<li>
<p>
<em>old_total_energy</em>: shows the total energy $L(q_{n-1,0})+T(-p_{n-1,0})$
of the initial state used for the acceptance evaluation.
</p>
</li>
<li>
<p>
<em>kinetic_energy</em>: shows the kinetic energy <em>after</em> momenta have been redrawn,
$T(p_n)$.
</p>
</li>
<li>
<p>
<em>scaled_momentum</em>: shows the scaled momenta <em>after</em> momenta have been redrawn.
</p>
</li>
<li>
<p>
<em>scaled_gradient</em>: shows the gradients scaled by the step width for the
current state $q_n$, either accepted or rejected.
</p>
</li>
<li>
<p>
<em>virial</em>: shows the virial energy for the current state $q_n$
</p>
</li>
<li>
<p>
<em>average_rejection_rate</em>: shows the number of rejected states over the sum of
accepeted and rejected states.
</p>
</li>
</ol></div>
</li>
<li>
<p>
Trajectory
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
<em>step</em>: see run info.
</p>
</li>
<li>
<p>
<em>loss</em>: see run info.
</p>
</li>
<li>
<p>
<em>weight?</em>: one weight parameter of the current state $q_n$, either
accepted or rejected.
</p>
</li>
<li>
<p>
<em>bias?</em>: equivalently, one bias parameter of the current state
$q_n$.
</p>
</li>
</ol></div>
</li>
<li>
<p>
Averages
</p>
<div class="olist loweralpha"><ol class="loweralpha">
<li>
<p>
<em>step</em>: see run info.
</p>
</li>
<li>
<p>
<em>loss</em>: see run info.
</p>
</li>
<li>
<p>
<em>ensemble_average_loss</em>: evaluates
$\frac{ \sum_n L(q_n) exp(-1/T \cdot L(q_n)) }{ \sum_n exp(-1/T \cdot L(q_n)) }$
over the states $q_n$.
</p>
</li>
<li>
<p>
<em>average_kinetic_energy</em>: evaluates $\frac 1 N \sum^N_{n=1} T(p_n)$.
</p>
</li>
<li>
<p>
<em>average_virials</em>: evaluates
$\frac 1 N \sum^N_{n=1} \frac 1 2 | q_n \cdot p_n  |$.
</p>
</li>
<li>
<p>
<em>average_rejection_rate</em>: see run info.
</p>
</li>
</ol></div>
</li>
</ol></div>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.walkerensemble">3.4.6. Ensemble of Walkers</h4>
<div class="paragraph"><p>Ensemble of Walkers uses a collection of walkers that exchange gradient
and parameter information in each step in order to calculate a
preconditioning matrix. This preconditioning allows to explore elongated
minimum basins faster than independent walkers would do alone, see
<a href="#Matthews2018">[Matthews2018]</a>.</p></div>
<div class="paragraph"><p>This is activated by setting the <span class="monospaced">number_walkers</span> to a value larger than
1. Note that <span class="monospaced">covariance_blending</span> controls the magnitude of the
covariance matrix approximation and <span class="monospaced">collapse_after_steps</span> controls
after how many steps the walkers are restarted at the parameter
configuration of the first walker to ensure that the harmonic
approximation still holds.</p></div>
<div class="paragraph"><p>This works for all of the aforementioned samplers as simply the gradient
of each walker is rescaled.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="reference.implementing_sampler">3.5. Simulation module: Implementing a sampler</h3>
<div class="paragraph"><p>We would like to demonstrate how to implement the <a href="#GLA">[GLA]</a> sampler of 2nd order
using the simulation module.</p></div>
<div class="paragraph"><p>Let us look at the four integration steps.</p></div>
<div class="openblock" id="reference.implementing_sampler.gla2">
<div class="title">Geometric Langevin Algorithm 2nd order</div>
<div class="content">
<div class="olist arabic"><ol class="arabic">
<li>
<p>
$p_{n+\frac 1 2} = p_n - \frac {\lambda}{2} \nabla_x L(x_n)$
</p>
</li>
<li>
<p>
$x_{n+1} = x_n + \lambda p_{n+\frac 1 2}$
</p>
</li>
<li>
<p>
$\widehat{p}_{n+1} = p_{n+\frac 1 2} - \frac {\lambda}{2} \nabla_x L(x_{n+1})$
</p>
</li>
<li>
<p>
$p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n$
</p>
</li>
</ol></div>
</div></div>
<div class="paragraph"><p>If you are familar with the <strong>A</strong> (position integration), <strong>B</strong>
(momentum integration), <strong>O</strong> (noise integration) notation, see <a href="#Leimkuhler2012">[Leimkuhler2012]</a>
then you will notice that we have the steps: <strong>BABO</strong>.</p></div>
<div class="sect3">
<h4 id="reference.implementing_sampler.naive_update">3.5.1. Simple update implementation</h4>
<div class="paragraph"><p>Therefore, let us write a python function that works on several <span class="monospaced">numpy</span> arrays
producing a single GLA2 update step by performing the four integration steps
above. To this end, we make use <span class="monospaced">tati.gradients()</span> for computing the gradients
$\nabla_x L(x_{n+1})$.</p></div>
<div class="listingblock">
<div class="title">A first GLA2 update implementation</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">gla2_update_step</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">,</span> momenta<span style="color: #990000">,</span> step_width<span style="color: #990000">,</span> beta<span style="color: #990000">,</span> gamma<span style="color: #990000">):</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  <span style="font-style: italic"><span style="color: #9A1900"># 1. p_{n+\frac 1 2} = p_n - \frac {\lambda}{2} \nabla_x L(x_n)</span></span>
  momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">()</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 2. x_{n+1} = x_n + \lambda p_{n+\frac 1 2}</span></span>
  nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> nn<span style="color: #990000">.</span>parameters <span style="color: #990000">+</span> step_width <span style="color: #990000">*</span> momenta  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 3. \widehat{p}_{n+1} = p_{n+\frac 1 2} - \frac {\lambda}{2} \nabla_x L(x_{n+1})</span></span>
  momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">()</span>  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 4. p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n</span></span>
  alpha <span style="color: #990000">=</span> math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">exp</span></span><span style="color: #990000">(-</span>gamma<span style="color: #990000">*</span>step_width<span style="color: #990000">)</span>
  momenta <span style="color: #990000">=</span> alpha <span style="color: #990000">*</span> momenta <span style="color: #990000">+</span> <span style="color: #990000">\</span>
            math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sqrt</span></span><span style="color: #990000">((</span><span style="color: #993399">1</span><span style="color: #990000">.-</span>math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">pow</span></span><span style="color: #990000">(</span>alpha<span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">.))/</span>beta<span style="color: #990000">)</span> <span style="color: #990000">*</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">standard_normal</span></span><span style="color: #990000">(</span>momenta<span style="color: #990000">.</span>shape<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span>

  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> momenta</tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
In the function header we need access to the <span class="monospaced">tati</span> reference, to the
<span class="monospaced">numpy</span> array containing the <span class="monospaced">momenta</span>. Moreover, we need a few parameters,
namely the step width <span class="monospaced">step_width</span>, the inverse temperature factor <span class="monospaced">beta</span> and
the friction constant <span class="monospaced">gamma</span>.
</p>
</li>
<li>
<p>
First, we perform the <strong>B</strong> step integrating the momenta.
</p>
</li>
<li>
<p>
Next comes the <strong>A</strong> step, integrating positions with the updated momenta.
</p>
</li>
<li>
<p>
Then, we integrate momenta again, <strong>B</strong>.
</p>
</li>
<li>
<p>
Last, we perform the noise integration <strong>O</strong>. First, we compute the value of
$\alpha_n$ and the the momenta are partially reset by the noise.
</p>
</li>
</ol></div>
<div class="paragraph"><p>As the source of noise we have simply used `numpy`s standard normal
distribution.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">It is advisable to fix the seed using <span class="monospaced">numpy.random.seed(426)</span> (or any
other value) to allow for reproducible runs.</td>
</tr></table>
</div>
<div class="paragraph"><p>We could have used <span class="monospaced">nn.momenta</span> for storing momenta. However, this
needs some extra computations for assigning the momenta inside the tensorflow
computational graph. As they are not needed in the graph anyway, we can store
them outside directly.</p></div>
<div class="paragraph"><p>Note that we have been a bit wasteful in the above implementation but very
close to the formulas in <a href="#reference.implementing_sampler">[reference.implementing_sampler]</a>.</p></div>
</div>
<div class="sect3">
<h4 id="reference.implementing_sampler.saving_old_gradients">3.5.2. Saving a gradient evaluation</h4>
<div class="paragraph"><p>We evaluate the gradient twice but actually only one evaluation would have been
needed: The update gradients in step 3. are the same as the gradients in step 1.
on the next iteration.</p></div>
<div class="paragraph"><p>Hence, let us refine the function with respect to this.</p></div>
<div class="listingblock">
<div class="title">GLa2 update implementation with just one gradient evaluation</div>
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">gla2_update_step</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">,</span> momenta<span style="color: #990000">,</span> old_gradients<span style="color: #990000">,</span> step_width<span style="color: #990000">,</span> beta<span style="color: #990000">,</span> gamma<span style="color: #990000">):</span>
    <span style="font-style: italic"><span style="color: #9A1900"># 1. p_{n+\frac 1 2} = p_n - \frac {\lambda}{2} \nabla_x L(x_n)</span></span>
    momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> old_gradients

    <span style="font-style: italic"><span style="color: #9A1900"># 2. x_{n+1} = x_n + \lambda p_{n+\frac 1 2}</span></span>
    nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> nn<span style="color: #990000">.</span>parameters <span style="color: #990000">+</span> step_width <span style="color: #990000">*</span> momenta

    <span style="font-style: italic"><span style="color: #9A1900"># \nabla_x L(x_{n+1})</span></span>
    gradients <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">()</span>

    <span style="font-style: italic"><span style="color: #9A1900"># 3. \widehat{p}_{n+1} = p_{n+\frac 1 2} - \frac {\lambda}{2} \nabla_x L(x_{n+1})</span></span>
    momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> gradients

    <span style="font-style: italic"><span style="color: #9A1900"># 4. p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n</span></span>
    alpha <span style="color: #990000">=</span> math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">exp</span></span><span style="color: #990000">(-</span>gamma<span style="color: #990000">*</span>step_width<span style="color: #990000">)</span>
    momenta <span style="color: #990000">=</span> alpha <span style="color: #990000">*</span> momenta <span style="color: #990000">+</span> <span style="color: #990000">\</span>
              math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sqrt</span></span><span style="color: #990000">((</span><span style="color: #993399">1</span><span style="color: #990000">.-</span>math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">pow</span></span><span style="color: #990000">(</span>alpha<span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">.))/</span>beta<span style="color: #990000">)</span> <span style="color: #990000">*</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">standard_normal</span></span><span style="color: #990000">(</span>momenta<span style="color: #990000">.</span>shape<span style="color: #990000">)</span>

    <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> gradients<span style="color: #990000">,</span> momenta</tt></pre></div></div>
<div class="paragraph"><p>Now, we use <span class="monospaced">old_gradients</span> in step 1. and return the updated gradients such
that it can be given as old gradients in the next call.</p></div>
</div>
<div class="sect3">
<h4 id="reference.implementing_sampler.loop_body">3.5.3. The loop</h4>
<div class="paragraph"><p>Now, we ad the loop body.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> math
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">426</span><span style="color: #990000">)</span>

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
<span style="color: #990000">)</span>

momenta <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">((</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
old_gradients <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">((</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>

<span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="color: #993399">100</span><span style="color: #990000">):</span>  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Current step #"</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>i<span style="color: #990000">))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
    old_gradients<span style="color: #990000">,</span> momenta <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">gla2_update_step</span></span><span style="color: #990000">(</span>
        nn<span style="color: #990000">,</span> momenta<span style="color: #990000">,</span> old_gradients<span style="color: #990000">,</span> step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span> beta<span style="color: #990000">=</span><span style="color: #993399">1e3</span><span style="color: #990000">,</span> gamma<span style="color: #990000">=</span><span style="color: #993399">10</span><span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">())</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span></tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
We instantiate a <span class="monospaced">tati</span> instance as usual, giving it the dataset and using
its default single-layer perceptron topology.
</p>
</li>
<li>
<p>
We create two numpy arrays to contain the momenta and the old gradients.
</p>
</li>
<li>
<p>
We iterate for 100 steps, printing the current step.
</p>
</li>
<li>
<p>
We use the <span class="monospaced">gla2_update_step()</span> function written to perform a single update
step. We store the returned gradients and momenta.
</p>
</li>
<li>
<p>
Finally, we print the loss per step.
</p>
</li>
</ol></div>
</div>
</div>
<div class="sect2">
<h3 id="reference.simulation">3.6. Simulation module reference</h3>
<div class="paragraph"><p>When trying out new methods on sampling or training neural networks, one
needs to play around with the network directly. This may require one or
more of the following operations:</p></div>
<div class="ulist"><ul>
<li>
<p>
Setting up a specific network
</p>
</li>
<li>
<p>
Setting a specific set of parameters
</p>
</li>
<li>
<p>
Requesting the current set of parameters
</p>
</li>
<li>
<p>
Requesting the current momenta
</p>
</li>
<li>
<p>
(Re-)initialize momenta
</p>
</li>
<li>
<p>
Evaluating the loss and/or gradients
</p>
</li>
<li>
<p>
Evaluating the predicted labels for another dataset
</p>
</li>
<li>
<p>
Evaluating accuracy
</p>
</li>
<li>
<p>
Supplying a different dataset
</p>
</li>
</ul></div>
<div class="paragraph"><p>TATi&#8217;s <span class="monospaced">simulation</span> interface readily allows for these and has one beneficial
feature: The network&#8217;s topology is completely hidden with respect to the
set of parameters. To elaborate on this: Tensorflow internally uses
tensors to represent weights between layers. Therefore, its parameters
are organized in tensors. This structure makes it quite difficult to set
or retrieve all parameters at once as their structure depends on the
chosen topology of the neural network. When using TATi then all you see
is a single vector of values containing all weights and biases of the
network. This makes it easy to manipulate or store them.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>However, this ease of use comes at the price of possibly increased
computational complexity as an extra array is needed for the values and
they need to be translated to the internal topology of the network every
time they are modified.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>In other words, this interface is good for trying out a quick-and-dirty
approach to implementing a new method or idea. However, it is not suited
for production runs of training a network. In the latter case it is
recommended to implement your method within Tensorflow, see the programmer&#8217;s
guide that should accompany your installation.</p></div>
<div class="sect3">
<h4 id="quickstart.simulation.tensorflow_internals">3.6.1. Tensorflow internals</h4>
<div class="paragraph"><p>Before we start, there are a few notes on how Tensorflow works
internally that might be helpfup in understanding why things are done
the way they are.</p></div>
<div class="paragraph"><p>Tensorflow internally represents all operations as nodes in a so-called
computational graph. Edges between nodes tell tensorflow which
operations' output is required as input to other operations, e.g. the
ones requested to evaluate. For example, in order to evaluate the loss,
it first needs to look at the dataset and also all weights and biases.
Any variable is also represented as a node in the graph.</p></div>
<div class="paragraph"><p>All actual data is stored in a so-called <em>session</em> object. Evaluations
of nodes in the computational graph are done by giving a list of nodes
to the &#8216;run()` function of this session object. This function usually
requires a so-called <em>feed dict</em>, a dictionary containing any external
values that are referenced through <em>placeholder</em> nodes. When evaluating
nodes, only that part of the feed dict needs to be given that is
required for the nodes&#8217; evaluation. E.g. when assigning values to
parameters through an assign operation using placeholders, we do not
need to specify the dataset</p></div>
<div class="paragraph"><p>This has been very brief and for a more in-depth view into the design of
Tensorflow, we refer to the programmer&#8217;s guide or to the official tensorflow
<a href="https://www.tensorflow.org/tutorial">tutorials</a>.</p></div>
</div>
<div class="sect3">
<h4 id="reference.simulation.help_options">3.6.2. Help on Options</h4>
<div class="paragraph"><p>TATi has quite a number of options that control its behavior with respect
to sampling, optimizing and so on. We briefly remind of how you can request
help to a specific option.
Let us inspect the help for <span class="monospaced">batch_data_files</span>:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">&gt;&gt;&gt;</span> <span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="color: #990000">&gt;&gt;&gt;</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">help</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"batch_data_files"</span><span style="color: #990000">)</span>
Option name<span style="color: #990000">:</span> batch_data_files
Description<span style="color: #990000">:</span> set of files to read input <span style="font-weight: bold"><span style="color: #000080">from</span></span>
Type       <span style="color: #990000">:</span> list of <span style="color: #990000">&lt;</span><span style="font-weight: bold"><span style="color: #0000FF">class</span></span> <span style="color: #FF0000">'str'</span><span style="color: #990000">&gt;</span>
Default    <span style="color: #990000">:</span> <span style="color: #990000">[]</span></tt></pre></div></div>
<div class="paragraph"><p>This will print a description, give the default value and expected type.</p></div>
<div class="paragraph"><p>Moreover, in case you have forgotten the name of one of the options.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">&gt;&gt;&gt;</span> <span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="color: #990000">&gt;&gt;&gt;</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">help</span></span><span style="color: #990000">()</span>
averages_file<span style="color: #990000">:</span>             CSV file name to write ensemble averages information such as average kinetic<span style="color: #990000">,</span> potential<span style="color: #990000">,</span> virial
batch_data_file_type<span style="color: #990000">:</span>      type of the files to read input <span style="font-weight: bold"><span style="color: #000080">from</span></span>
 <span style="color: #990000">&lt;</span>remainder omitted<span style="color: #990000">&gt;</span></tt></pre></div></div>
<div class="paragraph"><p>This will print a general help listing all available options.</p></div>
</div>
<div class="sect3">
<h4 id="reference.simulation.setup">3.6.3. Setup</h4>
<div class="paragraph"><p>Let&#8217;s first look at how to set up the neural network and supply it with
a dataset.</p></div>
<div class="sect4">
<h5 id="reference.simulation.setup.setting_up_network">Setting up the network</h5>
<div class="paragraph"><p>Let&#8217;s first create a neural network. At the moment of writing TATi is
constrained to multi-layer perceptrons but this will soon be extened to
convolutational and other networks.</p></div>
<div class="paragraph"><p>Multi-layer perceptrons are characterized by the number of layers, the
number of nodes per layer and the output function used in each node.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-style: italic"><span style="color: #9A1900"># prepare parameters</span></span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
        batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
        hidden_dimension<span style="color: #990000">=[</span><span style="color: #993399">8</span><span style="color: #990000">,</span> <span style="color: #993399">8</span><span style="color: #990000">],</span>
        hidden_activation<span style="color: #990000">=</span><span style="color: #FF0000">"relu"</span><span style="color: #990000">,</span>
        input_dimension<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
        loss<span style="color: #990000">=</span><span style="color: #FF0000">"mean_squared"</span><span style="color: #990000">,</span>
        output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
        output_dimension<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">,</span>
        seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>In the above example, we specify a neural network of two hidden layers,
each having 8 nodes. We use the "rectified linear" activation function
for these nodes. The output nodes are activated by a linear function.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>At the moment it is not possible to set different activation functions
for individual nodes or between hidden layers.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>For a full (and up-to-date) list of all available activation functions,
please look at TATi/models/model.py, <span class="monospaced">get_activations()</span>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Note that (re-)creating the <span class="monospaced">tati_model</span> instance model will always reset the
computational graph of tensorflow in case you need to add nodes.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="reference.simulation.setup.supply_dataset">Supply dataset as array</h5>
<div class="paragraph"><p>As a next step, we need to supply a dataset $D$. This dataset will
also necessitate a certain amount of input and output nodes.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>At the moment only classification datasets can be set up.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>There are basically two ways of supplying the dataset:</p></div>
<div class="ulist"><ul>
<li>
<p>
from a CSV file
</p>
</li>
<li>
<p>
from an internal (numpy) array
</p>
</li>
</ul></div>
<div class="paragraph"><p>We will give examples for both ways here.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> pandas as pd

<span style="font-style: italic"><span style="color: #9A1900"># e.g. parse dataset from CSV file into pandas frame</span></span>
input_dimension <span style="color: #990000">=</span> <span style="color: #993399">2</span>
output_dimension <span style="color: #990000">=</span> <span style="color: #993399">1</span>
parsed_csv <span style="color: #990000">=</span> pd<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">read_csv</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">,</span> sep<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> header<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># extract feature and label columns as numpy arrays</span></span>
features <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(\</span>
        parsed_csv<span style="color: #990000">.</span>iloc<span style="color: #990000">[:,</span> <span style="color: #993399">0</span><span style="color: #990000">:</span>input_dimension<span style="color: #990000">])</span>
labels <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(\</span>
        parsed_csv<span style="color: #990000">.</span>iloc<span style="color: #990000">[:,</span> <span style="color: #990000">\</span>
        input_dimension<span style="color: #990000">:</span>input_dimension <span style="color: #990000">\</span>
        <span style="color: #990000">+</span> output_dimension<span style="color: #990000">])</span>

<span style="font-style: italic"><span style="color: #9A1900"># supply dataset (this creates the input layer)</span></span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
        batch_size<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
        dataset <span style="color: #990000">=</span> <span style="color: #990000">[</span>features<span style="color: #990000">,</span> labels<span style="color: #990000">]</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>dataset<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>The <span class="monospaced">batch_data_files</span> always needs to be a list as multiple files may
be given. <span class="monospaced">batch_data_file_type</span> gives the type of the files.
Currently <strong>csv</strong> and <strong>tfrecord</strong> are supported choices.</p></div>
<div class="paragraph"><p>Note that you can combine all parameters for specifying the data set files
with the ones characterizing the neural network above to the <span class="monospaced">__init__()</span> call
of <span class="monospaced">TATi.simulation</span>. In that case you do not need to call <span class="monospaced">reset_parameters()</span>.</p></div>
<div class="paragraph"><p>We repeat the example given in
<a href="#quickstart.simulation.sampling.supply_dataset">Supplying Dataset</a>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-style: italic"><span style="color: #9A1900"># prepare parameters</span></span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>dataset<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>There, we read a dataset from a CSV file into a pandas dataframe which
afterwards is converted to a numpy array and then handed over to the
<span class="monospaced">provide_dataset()</span> function of the model interface. Naturally, if the
dataset is present in memory, it can be given right away and we do not
need to parse a CSV file.</p></div>
<div class="paragraph"><p>Note that the essential difference between these two examples is one uses
<span class="monospaced">batch_data_files</span> to request parsing the dataset from file while the other
example assigns the dataset through the <span class="monospaced">dataset</span> parameter.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">You can easily inspect the current batch by looking at <span class="monospaced">nn.dataset</span>.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="reference.simulation.parameters">3.6.4. Parameters</h4>
<div class="paragraph"><p>Next, we look at how to inspect and modify the neural network&#8217;s
parameters.</p></div>
<div class="sect4">
<h5 id="reference.simulation.parameters.requesting_parameters">Requesting parameters</h5>
<div class="paragraph"><p>The network&#8217;s parameters consist of a single vector $w$ where weights
and biases are concatenated in the following fashion: weight matrix to first
layer, weight matrix to second layer, &#8230;, bias vector to first layer. In
other words, first all weights, then all biases.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"There are "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())+</span><span style="color: #FF0000">" parameters."</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Parameters: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>The class <span class="monospaced">tati</span> simply has an internal object <span class="monospaced">parameters</span> which allows
to access them like a numpy array.</p></div>
</div>
<div class="sect4">
<h5 id="reference.simulation.parameters.setting_parameters">Setting parameters</h5>
<div class="paragraph"><p>Setting the parameters is just as easy as requesting them.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># the old parameters</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># set all parameters to zero</span></span>
parameters <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>parameters<span style="color: #990000">)</span>
nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> parameters
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we create the numpy array filled with zeros by requesting the
total number of weight and bias degrees of freedom, i.e. the number of
parameters of the network in <span class="monospaced">num_parameters()</span>. Afterwards, we simply
assign the <span class="monospaced">parameters</span> object to this new array of values.</p></div>
</div>
<div class="sect4">
<h5 id="reference.simulation.parameters.setting_parameters_walkers">Setting parameters of each walker</h5>
<div class="paragraph"><p>Did you know that you can have multiple walkers that <span class="monospaced">fit()</span> or <span class="monospaced">sample()</span>. In
other words, there can be replicated versions of the neural network. Each has
its own set of parameters and may move through the loss manifold individually.</p></div>
<div class="paragraph"><p>The key option to enable this is to set <em>number_walkers</em> larger than <strong>1</strong> in
the options to <span class="monospaced">tati</span>.</p></div>
<div class="paragraph"><p>This will automatically create copies of the network, each having a different
random starting position. Let us take a look at the following example where
we set the parameters of all and individual walkers.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    number_walkers<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># the old parameters</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># set all parameters to zero</span></span>
parameters <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())</span>
nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> parameters
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Parameters are zero: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">))</span>
<span style="font-style: italic"><span style="color: #9A1900"># set parameters of first walker to [1,0,...]</span></span>
parameters<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">]</span> <span style="color: #990000">=</span> <span style="color: #993399">1</span><span style="color: #990000">.</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>parameters<span style="color: #990000">)</span>
nn<span style="color: #990000">.</span>parameters<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">]</span> <span style="color: #990000">=</span> parameters
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Parameters 1st: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">))</span>
<span style="font-style: italic"><span style="color: #9A1900"># set parameters of second walker to [0,1,...]</span></span>
parameters<span style="color: #990000">[</span><span style="color: #993399">1</span><span style="color: #990000">]</span> <span style="color: #990000">=</span> <span style="color: #993399">1</span><span style="color: #990000">.</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>parameters<span style="color: #990000">)</span>
nn<span style="color: #990000">.</span>parameters<span style="color: #990000">[</span><span style="color: #993399">1</span><span style="color: #990000">]</span> <span style="color: #990000">=</span> parameters
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Parameters 2nd: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>parameters<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>As you see, accessing <span class="monospaced">parameters</span> will access all walkers at once. On the
hand, accessing <span class="monospaced">parameters[i]</span> will access only the parameters of walker <span class="monospaced">i</span>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Caution</div>
</td>
<td class="content">You cannot set a single parameter of a walker, i.e.
<span class="monospaced">nn.parameters[0][0] = 1.</span> will not fail but it will also <em>not</em> set the first
parameter of the first walker to <strong>1.</strong>. Essentially, you are setting the first
component of the returned numpy array to <strong>1.</strong> and then discard it immediately
as you hold no reference to it any longer.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Setting single parameters is deliberately not supported as it would only
be possible through obtaining all parameters, setting the single component, and
then assigning all parameters again which is possibly a very costly operation.
This is a tensorflow restriction.</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="reference.simulation.parameters.requesting_momenta">Requesting momenta</h5>
<div class="paragraph"><p>In the same way as requesting the current set of parameters we can also access
the momenta $p$.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"There are "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">())+</span><span style="color: #FF0000">" momenta."</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Momenta: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>momenta<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>This will return a <span class="monospaced">numpy</span> array of the current set of momenta if the sampler
supports it. Otherwise this will raise a <span class="monospaced">ValueError</span>.</p></div>
</div>
<div class="sect4">
<h5 id="reference.simulation.parameters.setting_momenta">Setting and initializing momenta</h5>
<div class="paragraph"><p>Equivalently to requesting momenta, they also can be set just as the set of
parameters.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>

np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>_options<span style="color: #990000">.</span>seed<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># the old momenta</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>momenta<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># set all momenta to normally distributed random values</span></span>
momenta <span style="color: #990000">=</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">standard_normal</span></span><span style="color: #990000">(</span>size<span style="color: #990000">=(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>momenta<span style="color: #990000">)</span>
nn<span style="color: #990000">.</span>momenta <span style="color: #990000">=</span> momenta
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>momenta<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>In this example, we first look at the old momenta and then set them to random
values from a normal distribution.</p></div>
<div class="paragraph"><p>Naturally, this works in the same way for multiple walkers as it worked for
the parameters.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">The <span class="monospaced">simulation</span> interface has a convenience function to reset all
momenta  from a normal distribution with zero mean according to a given
<span class="monospaced">inverse_temperature</span>.</td>
</tr></table>
</div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>

np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>_options<span style="color: #990000">.</span>seed<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># the old momenta</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>momenta<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># reinitialize all momenta</span></span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_momenta</span></span><span style="color: #990000">(</span>inverse_temperature<span style="color: #990000">=</span><span style="color: #993399">10</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>momenta<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>This will reset the momenta such that they match an average temperature of <strong>10</strong>.
In case of multiple walkers each walker will be initialized with different
momenta.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="reference.simulation.evaluation">3.6.5. Evaluation</h4>
<div class="paragraph"><p>Now, we are in the position to evaluate our neural network. Or neural networks,
in case you specified <em>number_walkers</em> larger than <strong>1</strong>, see <a href="#reference.simulation.parameters.setting_parameters_walkers">Setting parameters of each walker</a>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content"><em>walker_index</em> in each of the following functions determines which of
the walkers is evaluated. If no index is given, then all are evaluated, i.e.
a list of values is returned.
If there is just a single walker, the list is reduced to its first component
automatically.</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="reference.simulation.evaluation.loss">Evaluate loss</h5>
<div class="paragraph"><p>Now, all is set to actually evaluate the loss function $L_D(w)$ for
the first time. As a default the mean squared distance $l(x,y) = ||x-y||^2$
is chosen as the loss function. However, by setting the <span class="monospaced">loss</span> in the initial
parameters to <span class="monospaced">tati</span> appropriately, all other loss functions that tensorflow
offers are available, too.
For a full (and up-to-date) list please refer to <span class="monospaced">TATi/models/model.py</span>,
function <span class="monospaced">add_losses()</span>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>This will simply return the loss.</p></div>
</div>
<div class="sect4">
<h5 id="reference.simulation.evaluation.gradients">Evaluate gradients</h5>
<div class="paragraph"><p>Gradient information is similarly important as the loss function itself,
$\nabla_w L_D(w)$.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>Remember that all parameters are vectorized, hence, the
<span class="monospaced">gradients()</span> object returned is actually a numpy array containing
per component the gradient with respect to the specific parameter.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The gradients are given in exactly the same order as the order of
the parameter vector $w$.</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="reference.simulation.evaluation.hessians">Evaluate Hessians</h5>
<div class="paragraph"><p>Apart from gradient information hessians $H_{ij}$ are also available.
Note however that hessians are both very expensive to compute and to setup as
many nodes needed to be added to the computational graph. Therefore, in
the initial parameters to <span class="monospaced">tati</span> you need to explicitly state
<span class="monospaced">do_hessians=True</span> in order to activate their creation!</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    do_hessians<span style="color: #990000">=</span>True<span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">hessians</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>Again remember that all parameters are vectorized, hence, the
<span class="monospaced">hessians()</span> object returned is actually a numpy array containing
per component the gradient with respect to the specific parameter.</p></div>
</div>
<div class="sect4">
<h5 id="reference.simulation.evaluation.accuracy">Evaluate accuracy</h5>
<div class="paragraph"><p>Evaluating accuracy is as simple as evaluating the loss.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span>
<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">score</span></span><span style="color: #990000">())</span></tt></pre></div></div>
<div class="paragraph"><p>The accuracy is simply comparing the signs of predicted label and label given
in the dataset over all the batch or dataset.  In other words in the binary
classification setting we expect labels as $\{-1,1\}$.
For multi-class classification labels are simply in $\{0,1\}$ and
we compare which two classes have the largest output component, i.e. the
likeliest to match. Again, this is reduced by taking the mean square over the
whole dataset or batch.</p></div>
</div>
<div class="sect4">
<h5 id="reference.simulation.evaluation.predict">Evaluate predicted labels</h5>
<div class="paragraph"><p>Naturally, obtaining the predictions is now just as simple.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">427</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">427</span><span style="color: #990000">)</span>
other_features <span style="color: #990000">=</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random</span></span><span style="color: #990000">((</span><span style="color: #993399">2</span><span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">predict</span></span><span style="color: #990000">(</span>other_features<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>You have to supply a set of (new) features on which to evaluate the
predictions. Here, we simply use a set of random values.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="reference.simulation.datasets">3.6.6. Datasets</h4>
<div class="paragraph"><p>Last but not least, how to change the dataset when it was already
specified?</p></div>
<div class="sect4">
<h5 id="reference.simulation.datasets.change">Change the dataset</h5>
<div class="paragraph"><p>All evaluating takes place on the same dataset, once a network is trained.
We have already seen how to get predictions for a different dataset.
However, we might want to see its performance on a test or validation dataset.</p></div>
<div class="paragraph"><p>There are again two different ways because of the two different modes of
feeding the dataset: from file and from an array.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>batch_size<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">)</span>
nn<span style="color: #990000">.</span>dataset <span style="color: #990000">=</span> <span style="color: #990000">[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">]</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>dataset<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>In the first example, we set the <span class="monospaced">dataset</span> object of <span class="monospaced">tati</span> to a different
(list of) files.
This will automatically reset the input pipeline and prepare everything for
feeding the new dataset.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-style: italic"><span style="color: #9A1900"># create random (but fixes) 2d dataset, labels in {-1,1}</span></span>
num_items <span style="color: #990000">=</span> <span style="color: #993399">2</span>
np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">427</span><span style="color: #990000">)</span>
features <span style="color: #990000">=</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random</span></span><span style="color: #990000">((</span>num_items<span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">))</span>
labels <span style="color: #990000">=</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random_integers</span></span><span style="color: #990000">(</span><span style="color: #993399">0</span><span style="color: #990000">,</span><span style="color: #993399">1</span><span style="color: #990000">,</span> <span style="color: #990000">(</span>num_items<span style="color: #990000">,</span><span style="color: #993399">1</span><span style="color: #990000">))*</span><span style="color: #993399">2</span><span style="color: #990000">-</span><span style="color: #993399">1</span>

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">()</span>
nn<span style="color: #990000">.</span>dataset <span style="color: #990000">=</span> <span style="color: #990000">[</span>features<span style="color: #990000">,</span> labels<span style="color: #990000">]</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span>dataset<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Switching to (another) dataset from an internal numpy array we again access
the <span class="monospaced">dataset</span> array. Only this time we assign it to the numpy array.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>You must not change the input or output dimension as the network itself
is fixed.</p></div>
</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Note that changing the dataset actually modifies some nodes in
Tensorflow&#8217;s computational graph. This principally makes things a bit
slower as the session object has already been created. Simply keep this
in mind if slowness is suddenly bothering. You could store <span class="monospaced">parameters</span>,
reset the <span class="monospaced">tati</span> object and restore them to avoid this.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reference.parallelization">3.7. A note on parallelization</h3>
<div class="paragraph"><p>Internally, Tensorflow uses a computational graph to represent all
operations. Nodes in the graph represent computations and their results
and edges represent dependencies between these values, i.e. some may act
as input to operations resulting in certain output.</p></div>
<div class="paragraph"><p>Because of this internal representation Tensorflow has two kind of
parallelisms:</p></div>
<div class="ulist"><ul>
<li>
<p>
inter ops
</p>
</li>
<li>
<p>
intra ops
</p>
</li>
</ul></div>
<div class="paragraph"><p>Each is connected to its its own thread pool. Both the command-line and
the Python interface let you pick the number of threads per pool. If 0
is stated (default), then the number of threads is picked automatically.</p></div>
<div class="paragraph"><p>In general, <em>inter_ops_threads</em> refers to multiple cores performing
matrix multiplication or reduction operations together.
<em>intra_ops_threads</em> seems to be connected to executing multiple nodes
in parallel that are independent of each other but this is guessing at
the moment.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph"><p>When setting <em>inter_ops_threads</em> <span class="monospaced">unequal</span> to 1, then subsequent runs
may produce different results, i.e. results are no longer strictly
reproducible. According to Tensorflow this is because reduction
operations such as <span class="monospaced">reduce_sum()</span> run non-deterministically on multiple
cores for sake of speed.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect2">
<h3 id="reference.reproducibility">3.8. A note on reproducibility</h3>
<div class="paragraph"><p>In many of the examples in the quickstart tutorials we have set a <em>seed</em> value
to enforce reproducible runs.</p></div>
<div class="paragraph"><p>We have gone through great lengths to make sure that runs using the same set
of options yield the same output on every evocation.</p></div>
<div class="paragraph"><p>Tensorflow is not fully reproducible per se. Its internal random number seeds
change when the computational graph changes. Its reduction operations are
non-deterministic. The latter can be overcome by setting <em>inter_ops_threads</em> to
<em>1</em>, which take away some of the parallelization for the sake of
reproducibility. The former is taken care of by TATi itself. We make sure to
set the random number seeds deterministically to ensure that values are
unchanged even if the graph is slightly changed.</p></div>
<div class="paragraph"><p>If we find that this should not be the case, please file an issue, see
<a href="#introduction.feedback">[introduction.feedback]</a>.</p></div>
</div>
<div class="sect2">
<h3 id="reference.performance">3.9. Notes on Performance</h3>
<div class="paragraph"><p>Performance is everything in the world of neural network training. Codes and
machines are measured by how fast they perform in images/second when training
AlexNet or other networks on the ImageNet dataset, see <a href="https://www.tensorflow.org/performance/benchmarks">Tensorflow Benchmarks</a>.</p></div>
<div class="paragraph"><p>We worked hard to ensure that whatever Tensorflow offers in performance is also
seen when using TATi. In order to guide the user in what to expect and what to
do when these expectations are not met, we invite to go through this section.</p></div>
<div class="paragraph"><p>In general, performance hinges <strong>critically</strong> on the input pipeline. In other
words, it depends very much on how fast a specific machine setup can feed the
dataset into the input layer of the neural network.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In our examples both datasets and networks are very small. This causes
the sequential parts of tensorflow to overwhelm any kind of parallel execution.</td>
</tr></table>
</div>
<div class="paragraph"><p>Typically, these datasets are stored as a set of files residing on disk. Note
that reading from disk is very slow compared to reading from memory. Hence, the
first step is to read the dataset from disk and this will completely dominate
the computational load at the beginning.</p></div>
<div class="paragraph"><p>If the dataset is small enough to completely fit in memory, TATi will uses
Tensorflow&#8217;s <em>caching</em> to speed up the operations. This will become noticeable
after the first epoch, i.e. when all batches of the dataset have been processed
exactly once. Caching delivers at least a tenfold increase in learning speed,
depending on wer hard drive setup.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="title">In memory pipeline</div>
<div class="paragraph"><p>If the dataset fits in memory, it is advised to use the <span class="monospaced">InMemoryPipeline</span>
by setting the appropriate options in <span class="monospaced">tati</span> instantiation, see
<a href="#quickstart.simulation">[quickstart.simulation]</a>. This is especially true for small batch sizes and
CPU-only hardware. On GPU-assisted systems the in-memory pipeline should not
be used.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
        <span style="font-style: italic"><span style="color: #9A1900"># ...</span></span>
        in_memory_pipeline <span style="color: #990000">=</span> True<span style="color: #990000">,</span>
        <span style="font-style: italic"><span style="color: #9A1900"># ...</span></span>
<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>When using the command-line interface, add the respective option, see <a href="#quickstart.cmdline">[quickstart.cmdline]</a>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">...</span>
  --in_memory_pipeline <span style="color: #993399">1</span> <span style="color: #990000">\</span>
<span style="color: #990000">...</span></tt></pre></div></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>Furthermore, TATi uses Tensorflow&#8217;s prefetching to interleave feeding and
training operations. This will take effect roughly after the second epoch.
Prefetching will show an increase by another factor of 2.</p></div>
<div class="paragraph"><p>A typical runtime profile is given in Figure <a href="#references.performance.runtime_comparison_cpu">[references.performance.runtime_comparison_cpu]</a>
where we show the time spent for every 10 steps over the whole history. This is
done by simply plotting the <em>time_per_nth_step</em> column from the run file against
the <em>step</em> column.
There, we have used the <a href="#BAOAB">[BAOAB]</a> sampler. Initially, there is a large peak
caused by the necessary parsing of the dataset from disk. This is followed by a
period where the caching is effective and runtime per nth step has dropped
dramatically. From this time on, Tensorflow will be able to make use of parallel
threads for training. Then, we see another drop when prefetching kicks in.</p></div>
<div class="imageblock" id="references.performance.runtime_comparison_cpu">
<div class="content">
<img src="./doc/userguide/pictures/time_per_nth_step_hidden_dimension-hash_912b074-dimension_5000-batch_size_100-semilogy-2018-06-27.png" alt="runtime comparison" width="600">
</div>
<div class="title">Figure 9. Runtime comparison, CPU: Core i7, network with a single hidden layer and various numbers of nodes on a random MNIST dataset</div>
</div>
<div class="paragraph"><p>Note that Tensorflow has been designed to use GPU cards such as offered by
NVIDIA (and also Google&#8217;s own domain-specific chip called Tensor Proccessing
Unit). If such a GPU card is employed, the actual linear algebra operations
necessary for the gradient calculation and weight and bias updates during
training will become negligible except for very large networks (1e6 dof and
beyond).</p></div>
<div class="paragraph"><p>In <a href="#references.performance.runtime_comparison_gpu">[references.performance.runtime_comparison_gpu]</a> we give the same runtime
profile as before. In contrast to before, the simulation is now done on a
system with 2 NVIDIA V100 cards. Comparing this to figure <a href="#references.performance.runtime_comparison_cpu">[references.performance.runtime_comparison_cpu]</a>
we notice that now all curves associated to different number of nodes in the
hidden layer (<strong>hidden_dimension</strong>) basically lie on top of each other. In the
runtime profile on CPUs alone there is a clear trend for networks with more
degrees of freedom to significantly require more time per training step. We
conclude that with these networks (784 input nodes, 10 output nodes,
<strong>hidden_dimension</strong> hidden nodes, i.e. ~1e6 dof) the V100s do not see full load,
yet.</p></div>
<div class="imageblock" id="references.performance.runtime_comparison_gpu">
<div class="content">
<img src="./doc/userguide/pictures/time_per_nth_step_hidden_dimension-dimension_16000-batch_size_1000-2018-06-28.png" alt="runtime comparison" width="600">
</div>
<div class="title">Figure 10. Runtime comparison, GPU: 2x V100 cards, network with a single hidden layer and various numbers of nodes on a random MNIST dataset</div>
</div>
<div class="sect3">
<h4 id="reference.performance.hints">3.9.1. General advice</h4>
<div class="paragraph"><p>We conclude this section with some general advice on resolving performance issues:</p></div>
<div class="ulist"><ul>
<li>
<p>
trajectory files become very big with large networks. Do not write the
  trajectory if you do not really need it. If you do need it, then write only
        every tenth or better every hundredth step (option <em>every_nth</em>).
</p>
</li>
<li>
<p>
summaries (option <em>summaries_path</em>) are very costly as well. Write them only
  for debugging purposes.
</p>
</li>
<li>
<p>
if you can afford the stochastic noise, use a small batch size (option
  <em>batch_size</em>).
</p>
</li>
<li>
<p>
use hessians (option <em>do_hessians</em>) only for really small networks.
</p>
</li>
<li>
<p>
covariance computations are ${O}(M^2)$ in the number of
  parameters <strong>M</strong>. Rather use diffusion_map analysis which is squared only in the
        number of trajectory steps.
</p>
</li>
<li>
<p>
measure the time needed for a few sampling/training steps, see the
  <strong>time_per_nth_step</strong> in the run info dataframes/files. A very good value
  is 300 steps per second. If you find you are much worse than this (on otherwise
        able hardware), then start tweaking parameters.
</p>
</li>
<li>
<p>
Parallel load is not everything. The dataset pipeline will produce much more
  parallel load but is not necessarily faster than the in-memory pipeline. The
        essential measure is <strong>steps/second</strong>.
</p>
</li>
<li>
<p>
In case you do not need any extra information such as norm of gradients,
  noise, momentum, kinetic energy and so forth, then you may set
        <em>do_accumulates</em> to False (or 0 on the cmd-line). This will deactivate these
        extra calculations, rendering the associated columns in the averages and run
        info files/data frames zero. This gives about 20-25% decrease in runtime
        depending on the network and dataset/batch size.
</p>
</li>
</ul></div>
</div>
</div>
<div class="sect2">
<h3 id="reference.miscellaneous">3.10. Miscellaneous</h3>
<div class="sect3">
<h4 id="reference.miscellaneous.progress_bar">3.10.1. Displaying a progress bar</h4>
<div class="paragraph"><p>For longer simulation runs it is desirable to obtain an estimate after a
few steps of the time required for the entire run.</p></div>
<div class="paragraph"><p>This is possible using the <span class="monospaced">progress</span> option. Specified to 1 or True it
will produce a progress bar showing the total number of steps, the
iterations per second, the elapsed time since start and the estimated
time till finish.</p></div>
<div class="paragraph"><p>This features requires the <a href="https://github.com/tqdm/tqdm">tqdm</a> package.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>On the debug verbosity level per output step also an estimate of the
remaining run time is given.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.miscellaneous.summaries">3.10.2. Tensorflow summaries</h4>
<div class="paragraph"><p>Tensorflow delivers a powerful instrument for inspecting the inner
workings of its computational graph: TensorBoard.</p></div>
<div class="paragraph"><p>This tool allows also to inspect values such as the activation
histogram, the loss and accuracy and many other parameters and values
internal to TATi.</p></div>
<div class="paragraph"><p>Supplying a path <span class="monospaced">/foo/bar</span> present in the file system using the
<span class="monospaced">summaries_path</span> variable, summaries are automatically written to the
path and can be inspected with the following call to tensorboard.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>tensorboard --logdir /foo/bar</tt></pre></div></div>
<div class="paragraph"><p>The tensorboard essentially comprises a web server for rendering the
nodes of the graph and figures of the inspected values inside a web page.
On execution it provides a URL that needs to be entered in any
web browser to access the web page.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The accumulation and writing of the summaries has quite an impact on
TATi&#8217;s overall performance and is therefore switched off by default.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_acknowledgements">Acknowledgements</h2>
<div class="sectionbody">
<div class="paragraph"><p>Thanks to all users of the code!</p></div>
</div>
</div>
<div class="sect1">
<h2 id="bibliography">Literature</h2>
<div class="sectionbody">
<div class="ulist"><ul>
<li>
<p>
<a id="Barzilai1988"></a> Barzilai, J., &amp; Borwein, J. M. (1988).
Two-Point Step Size Gradient Methods.
IMA Journal of Numerical Analysis, 8(1), 141–148.
<a href="http://doi.org/10.1093/imanum/8.1.141">http://doi.org/10.1093/imanum/8.1.141</a>
</p>
</li>
<li>
<p>
<a id="Bishop2006"></a> Bishop, Christopher M. (2006).
Pattern Recognition and Machine Learning.
Springer Scienice+Business Media, LLC.
<a href="https://www.springer.com/de/book/9780387310732">https://www.springer.com/de/book/9780387310732</a>
</p>
</li>
<li>
<p>
<a id="Bungartz2004"></a> Bungartz, H. J., &amp; Griebel, M. (2004).
Sparse grids.
Acta Numerica, 13, 147–269.
<a href="http://doi.org/10.1017/S0962492904000182">http://doi.org/10.1017/S0962492904000182</a>
</p>
</li>
<li>
<p>
<a id="Coifman2006"></a> Coifman, R. R., &amp; Lafon, S. (2006).
Diffusion maps.
Applied and Computational Harmonic Analysis, 21(1), 5–30.
<a href="https://doi.org/10.1016/j.acha.2006.04.006">https://doi.org/10.1016/j.acha.2006.04.006</a>
</p>
</li>
<li>
<p>
<a id="Duane1987"></a> Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987).
Hybrid Monte Carlo.
Physics Letters B, 195(2), 216–222.
<a href="http://doi.org/10.1016/0370-2693(87)91197-X">http://doi.org/10.1016/0370-2693(87)91197-X</a>
</p>
</li>
<li>
<p>
<a id="Goodman2010"></a> Goodman, J., &amp; Weare, J. (2010).
Ensemble samplers with affine invariance.
Communications in Applied Mathematics and Computational Science, 5(1), 65–80.
<a href="http://doi.org/10.2140/camcos.2010.5.65">http://doi.org/10.2140/camcos.2010.5.65</a>
</p>
</li>
<li>
<p>
<a id="LeCun2006"></a> LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., &amp; Huang, F. J. (2006).
A Tutorial on Energy-Based Learning.
Predicting Structured Data, 191–246.
<a href="http://doi.org/10.1198/tech.2008.s913">http://doi.org/10.1198/tech.2008.s913</a>
</p>
</li>
<li>
<p>
<a id="Leimkuhler2012"></a> Leimkuhler, B., &amp; Matthews, C. (2012).
Rational Construction of Stochastic Numerical Methods for Molecular Sampling.
Applied Mathematics Research EXpress, 2013(1), 34–56.
<a href="http://doi.org/10.1093/amrx/abs010">http://doi.org/10.1093/amrx/abs010</a>
</p>
</li>
<li>
<p>
<a id="Leimkuhler2015"></a> Leimkuhler, B., Matthews, C., &amp; Stoltz, G. (2015).
The computation of averages from equilibrium and nonequilibrium Langevin molecular dynamics.
IMA Journal of Numerical Analysis, 1–55.
<a href="http://doi.org/10.1093/imanum/dru056">http://doi.org/10.1093/imanum/dru056</a>
</p>
</li>
<li>
<p>
<a id="MacKenzie1989"></a> Mackenzie, P. B. (1989).
An improved hybrid Monte Carlo method.
Physics Letters B, 226(3–4), 369–371.
<a href="https://doi.org/10.1016/0370-2693(89)91212-4">https://doi.org/10.1016/0370-2693(89)91212-4</a>
</p>
</li>
<li>
<p>
<a id="Matthews2018"></a> Matthews, C., Weare, J., &amp; Leimkuhler, B. (2018).
Ensemble preconditioning for Markov chain Monte Carlo simulation.
Statistics and Computing, 28(2), 277–290.
<a href="http://doi.org/10.1007/s11222-017-9730-1">http://doi.org/10.1007/s11222-017-9730-1</a>
</p>
</li>
<li>
<p>
<a id="Neal2011"></a> Neal, R. M. (2011).
MCMC Using Hamiltonian Dynamics.
In Handbook of Markov Chain Monte Carlo (pp. 113–162).
</p>
</li>
<li>
<p>
<a id="Pflueger2010"></a> Pflüger, D. (2010).
Spatially Adaptive Sparse Grids for High-Dimensional Problems.
PhD thesis. Technische Universität München.
<a href="http://www5.in.tum.de/pub/pflueger10spatially.pdf">http://www5.in.tum.de/pub/pflueger10spatially.pdf</a>
</p>
</li>
<li>
<p>
<a id="Shang2015"></a> Shang, X., Zhu, Z., Leimkuhler, B., &amp; Storkey, A. J. (2015).
Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling.
In C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett (Ed.),
Advances in Neural Information Processing Systems 28 (pp. 37–45).
Curran Associates, Inc.
<a href="http://doi.org/10.1515/jip-2012-0071">http://doi.org/10.1515/jip-2012-0071</a>
</p>
</li>
<li>
<p>
<a id="Tan2016"></a> Tan, C., Ma, S., Dai, Y.-H., &amp; Qian, Y. (2016).
Barzilai-Borwein Step Size for Stochastic Gradient Descent, 1–17.
Retrieved from <a href="http://arxiv.org/abs/1605.04131">http://arxiv.org/abs/1605.04131</a>
</p>
</li>
<li>
<p>
<a id="Trstanova2016"></a> Trstanova, Z. (2016).
Mathematical and Algorithmic Analysis of Modified Langevin Dynamics.
</p>
</li>
<li>
<p>
<a id="Welling2011"></a> Welling, M., &amp; Teh, Y.-W. (2011).
Bayesian Learning via Stochastic Gradient Langevin Dynamics.
Proceedings of the 28th International Conference on Machine Learning, 681–688.
<a href="http://doi.org/10.1515/jip-2012-0071">http://doi.org/10.1515/jip-2012-0071</a>
</p>
</li>
</ul></div>
</div>
</div>
<div class="sect1">
<h2 id="glossary">Glossary</h2>
<div class="sectionbody">
<div class="ulist"><ul>
<li>
<p>
<a id="BAOAB"></a> <strong>BAOAB</strong>
</p>
<div class="paragraph"><p>BAOAB is the short-form for the order of the exact solution steps in the
splitting of the Langevin Dynamics SDE: B means momentum update, A is the
position update, and O is the random noise update. It has 2nd order convergence
properties, showing even 4th order super-convergence in the context of
high friction, see <a href="#Leimkuhler2012">[Leimkuhler2012]</a>.</p></div>
</li>
<li>
<p>
<a id="CCAdL"></a> <strong>Covariance Controlled Adaptive Langevin</strong> (CCAdL)
</p>
<div class="paragraph"><p>This is an extension of <a href="#SGD">[SGD]</a> that uses a thermostat to dissipate the
extra noise through approximate gradients from the system.</p></div>
</li>
<li>
<p>
<a id="GD"></a> <strong>Gradient Descent</strong> (GD)
</p>
<div class="paragraph"><p>An iterative, first-order optimization that use the negative gradient
times a step width to converge towards the minimum.</p></div>
</li>
<li>
<p>
<a id="GLA"></a> <strong>Geometric Langevin Algorithm</strong> (GLA)
</p>
<div class="paragraph"><p>This family of samplers results from a first-order splitting between the
Hamiltonian and the Ornstein-Uhlenbeck parts. It provides up to
second-order accuracy. In the package we have implemented both the 1st
and 2nd order variant. GLA2nd is among the most accurate samplers,
especially when it comes to accuracy of momenta. It is surpassed by
BAOAB, particularly for positions.</p></div>
</li>
<li>
<p>
<a id="HMC"></a> <strong>Hamiltonian Monte Carlo</strong> (HMC)
</p>
<div class="paragraph"><p>Instead of Langevin Dynamics this sampler relies on Hamiltonian
Dynamics. After a specific number of trajectory steps an acceptance
criterion is evaluated. Afterwards momenta are drawn randomly. Hence,
here noise comes into play at distinct intervals while for the other
samplers noise enters gradually in every step.</p></div>
</li>
<li>
<p>
<a id="SGD"></a> <strong>Stochastic Gradient Descent</strong> (SGD)
</p>
<div class="paragraph"><p>A variant of <a href="#GD">[GD]</a> where not the whole dataset is used for the gradient
computation but only a smaller part. This lightens the computational
complexity and adds some noise to the iteration as gradients are only
approximate. However, given redundancy in the dataset this noise is
often welcome and helps in overcoming barriers in the non-convex
minimization problem.</p></div>
<div class="paragraph"><p>See also <a href="#GD">[GD]</a>.</p></div>
</li>
<li>
<p>
<a id="SGLD"></a> <strong>Stochastic Gradient Langevin Dynamics</strong> (SGLD)
</p>
<div class="paragraph"><p>A variant of SGD where the approximate gradients are not only source of
noise but an additional noise term is added whose magnitude controls the
noise from the gradients.</p></div>
<div class="paragraph"><p>See also <a href="#SGD">[SGD]</a>.</p></div>
</li>
</ul></div>
</div>
</div>
</div>
<div id="footnotes"><hr></div>
<div id="footer">
<div id="footer-text">
Version v0.9.5-2-g887d52f<br>
Last updated
 2019-02-06 15:04:05 CET
</div>
</div>
</body>
</html>
