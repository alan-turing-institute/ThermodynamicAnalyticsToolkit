<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="AsciiDoc 8.6.10">
<meta name="description" content="TATi is a software suite based on tensorflow that brings enhanced sampling methods based on Langevin Dynamics and Hamiltonian Dynamics to neural network training.">
<meta name="keywords" content="neural networks, loss, loss manifold, sampling, exploration">
<title>Thermodynamic Analytics Toolkit (TATi)</title>
<style type="text/css">
/* Shared CSS for AsciiDoc xhtml11 and html5 backends */

/* Default font. */
body {
  font-family: Georgia,serif;
}

/* Title font. */
h1, h2, h3, h4, h5, h6,
div.title, caption.title,
thead, p.table.header,
#toctitle,
#author, #revnumber, #revdate, #revremark,
#footer {
  font-family: Arial,Helvetica,sans-serif;
}

body {
  margin: 1em 5% 1em 5%;
}

a {
  color: blue;
  text-decoration: underline;
}
a:visited {
  color: fuchsia;
}

em {
  font-style: italic;
  color: navy;
}

strong {
  font-weight: bold;
  color: #083194;
}

h1, h2, h3, h4, h5, h6 {
  color: #527bbd;
  margin-top: 1.2em;
  margin-bottom: 0.5em;
  line-height: 1.3;
}

h1, h2, h3 {
  border-bottom: 2px solid silver;
}
h2 {
  padding-top: 0.5em;
}
h3 {
  float: left;
}
h3 + * {
  clear: left;
}
h5 {
  font-size: 1.0em;
}

div.sectionbody {
  margin-left: 0;
}

hr {
  border: 1px solid silver;
}

p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

ul, ol, li > p {
  margin-top: 0;
}
ul > li     { color: #aaa; }
ul > li > * { color: black; }

.monospaced, code, pre {
  font-family: "Courier New", Courier, monospace;
  font-size: inherit;
  color: navy;
  padding: 0;
  margin: 0;
}
pre {
  white-space: pre-wrap;
}

#author {
  color: #527bbd;
  font-weight: bold;
  font-size: 1.1em;
}
#email {
}
#revnumber, #revdate, #revremark {
}

#footer {
  font-size: small;
  border-top: 2px solid silver;
  padding-top: 0.5em;
  margin-top: 4.0em;
}
#footer-text {
  float: left;
  padding-bottom: 0.5em;
}
#footer-badges {
  float: right;
  padding-bottom: 0.5em;
}

#preamble {
  margin-top: 1.5em;
  margin-bottom: 1.5em;
}
div.imageblock, div.exampleblock, div.verseblock,
div.quoteblock, div.literalblock, div.listingblock, div.sidebarblock,
div.admonitionblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.admonitionblock {
  margin-top: 2.0em;
  margin-bottom: 2.0em;
  margin-right: 10%;
  color: #606060;
}

div.content { /* Block element content. */
  padding: 0;
}

/* Block element titles. */
div.title, caption.title {
  color: #527bbd;
  font-weight: bold;
  text-align: left;
  margin-top: 1.0em;
  margin-bottom: 0.5em;
}
div.title + * {
  margin-top: 0;
}

td div.title:first-child {
  margin-top: 0.0em;
}
div.content div.title:first-child {
  margin-top: 0.0em;
}
div.content + div.title {
  margin-top: 0.0em;
}

div.sidebarblock > div.content {
  background: #ffffee;
  border: 1px solid #dddddd;
  border-left: 4px solid #f0f0f0;
  padding: 0.5em;
}

div.listingblock > div.content {
  border: 1px solid #dddddd;
  border-left: 5px solid #f0f0f0;
  background: #f8f8f8;
  padding: 0.5em;
}

div.quoteblock, div.verseblock {
  padding-left: 1.0em;
  margin-left: 1.0em;
  margin-right: 10%;
  border-left: 5px solid #f0f0f0;
  color: #888;
}

div.quoteblock > div.attribution {
  padding-top: 0.5em;
  text-align: right;
}

div.verseblock > pre.content {
  font-family: inherit;
  font-size: inherit;
}
div.verseblock > div.attribution {
  padding-top: 0.75em;
  text-align: left;
}
/* DEPRECATED: Pre version 8.2.7 verse style literal block. */
div.verseblock + div.attribution {
  text-align: left;
}

div.admonitionblock .icon {
  vertical-align: top;
  font-size: 1.1em;
  font-weight: bold;
  text-decoration: underline;
  color: #527bbd;
  padding-right: 0.5em;
}
div.admonitionblock td.content {
  padding-left: 0.5em;
  border-left: 3px solid #dddddd;
}

div.exampleblock > div.content {
  border-left: 3px solid #dddddd;
  padding-left: 0.5em;
}

div.imageblock div.content { padding-left: 0; }
span.image img { border-style: none; vertical-align: text-bottom; }
a.image:visited { color: white; }

dl {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
dt {
  margin-top: 0.5em;
  margin-bottom: 0;
  font-style: normal;
  color: navy;
}
dd > *:first-child {
  margin-top: 0.1em;
}

ul, ol {
    list-style-position: outside;
}
ol.arabic {
  list-style-type: decimal;
}
ol.loweralpha {
  list-style-type: lower-alpha;
}
ol.upperalpha {
  list-style-type: upper-alpha;
}
ol.lowerroman {
  list-style-type: lower-roman;
}
ol.upperroman {
  list-style-type: upper-roman;
}

div.compact ul, div.compact ol,
div.compact p, div.compact p,
div.compact div, div.compact div {
  margin-top: 0.1em;
  margin-bottom: 0.1em;
}

tfoot {
  font-weight: bold;
}
td > div.verse {
  white-space: pre;
}

div.hdlist {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
div.hdlist tr {
  padding-bottom: 15px;
}
dt.hdlist1.strong, td.hdlist1.strong {
  font-weight: bold;
}
td.hdlist1 {
  vertical-align: top;
  font-style: normal;
  padding-right: 0.8em;
  color: navy;
}
td.hdlist2 {
  vertical-align: top;
}
div.hdlist.compact tr {
  margin: 0;
  padding-bottom: 0;
}

.comment {
  background: yellow;
}

.footnote, .footnoteref {
  font-size: 0.8em;
}

span.footnote, span.footnoteref {
  vertical-align: super;
}

#footnotes {
  margin: 20px 0 20px 0;
  padding: 7px 0 0 0;
}

#footnotes div.footnote {
  margin: 0 0 5px 0;
}

#footnotes hr {
  border: none;
  border-top: 1px solid silver;
  height: 1px;
  text-align: left;
  margin-left: 0;
  width: 20%;
  min-width: 100px;
}

div.colist td {
  padding-right: 0.5em;
  padding-bottom: 0.3em;
  vertical-align: top;
}
div.colist td img {
  margin-top: 0.3em;
}

@media print {
  #footer-badges { display: none; }
}

#toc {
  margin-bottom: 2.5em;
}

#toctitle {
  color: #527bbd;
  font-size: 1.1em;
  font-weight: bold;
  margin-top: 1.0em;
  margin-bottom: 0.1em;
}

div.toclevel0, div.toclevel1, div.toclevel2, div.toclevel3, div.toclevel4 {
  margin-top: 0;
  margin-bottom: 0;
}
div.toclevel2 {
  margin-left: 2em;
  font-size: 0.9em;
}
div.toclevel3 {
  margin-left: 4em;
  font-size: 0.9em;
}
div.toclevel4 {
  margin-left: 6em;
  font-size: 0.9em;
}

span.aqua { color: aqua; }
span.black { color: black; }
span.blue { color: blue; }
span.fuchsia { color: fuchsia; }
span.gray { color: gray; }
span.green { color: green; }
span.lime { color: lime; }
span.maroon { color: maroon; }
span.navy { color: navy; }
span.olive { color: olive; }
span.purple { color: purple; }
span.red { color: red; }
span.silver { color: silver; }
span.teal { color: teal; }
span.white { color: white; }
span.yellow { color: yellow; }

span.aqua-background { background: aqua; }
span.black-background { background: black; }
span.blue-background { background: blue; }
span.fuchsia-background { background: fuchsia; }
span.gray-background { background: gray; }
span.green-background { background: green; }
span.lime-background { background: lime; }
span.maroon-background { background: maroon; }
span.navy-background { background: navy; }
span.olive-background { background: olive; }
span.purple-background { background: purple; }
span.red-background { background: red; }
span.silver-background { background: silver; }
span.teal-background { background: teal; }
span.white-background { background: white; }
span.yellow-background { background: yellow; }

span.big { font-size: 2em; }
span.small { font-size: 0.6em; }

span.underline { text-decoration: underline; }
span.overline { text-decoration: overline; }
span.line-through { text-decoration: line-through; }

div.unbreakable { page-break-inside: avoid; }


/*
 * xhtml11 specific
 *
 * */

div.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.tableblock > table {
  border: 3px solid #527bbd;
}
thead, p.table.header {
  font-weight: bold;
  color: #527bbd;
}
p.table {
  margin-top: 0;
}
/* Because the table frame attribute is overriden by CSS in most browsers. */
div.tableblock > table[frame="void"] {
  border-style: none;
}
div.tableblock > table[frame="hsides"] {
  border-left-style: none;
  border-right-style: none;
}
div.tableblock > table[frame="vsides"] {
  border-top-style: none;
  border-bottom-style: none;
}


/*
 * html5 specific
 *
 * */

table.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
thead, p.tableblock.header {
  font-weight: bold;
  color: #527bbd;
}
p.tableblock {
  margin-top: 0;
}
table.tableblock {
  border-width: 3px;
  border-spacing: 0px;
  border-style: solid;
  border-color: #527bbd;
  border-collapse: collapse;
}
th.tableblock, td.tableblock {
  border-width: 1px;
  padding: 4px;
  border-style: solid;
  border-color: #527bbd;
}

table.tableblock.frame-topbot {
  border-left-style: hidden;
  border-right-style: hidden;
}
table.tableblock.frame-sides {
  border-top-style: hidden;
  border-bottom-style: hidden;
}
table.tableblock.frame-none {
  border-style: hidden;
}

th.tableblock.halign-left, td.tableblock.halign-left {
  text-align: left;
}
th.tableblock.halign-center, td.tableblock.halign-center {
  text-align: center;
}
th.tableblock.halign-right, td.tableblock.halign-right {
  text-align: right;
}

th.tableblock.valign-top, td.tableblock.valign-top {
  vertical-align: top;
}
th.tableblock.valign-middle, td.tableblock.valign-middle {
  vertical-align: middle;
}
th.tableblock.valign-bottom, td.tableblock.valign-bottom {
  vertical-align: bottom;
}


/*
 * manpage specific
 *
 * */

body.manpage h1 {
  padding-top: 0.5em;
  padding-bottom: 0.5em;
  border-top: 2px solid silver;
  border-bottom: 2px solid silver;
}
body.manpage h2 {
  border-style: none;
}
body.manpage div.sectionbody {
  margin-left: 3em;
}

@media print {
  body.manpage div#toc { display: none; }
}


@media screen {
  body {
    max-width: 50em; /* approximately 80 characters wide */
    margin-left: 16em;
  }

  #toc {
    position: fixed;
    top: 0;
    left: 0;
    bottom: 0;
    width: 13em;
    padding: 0.5em;
    padding-bottom: 1.5em;
    margin: 0;
    overflow: auto;
    border-right: 3px solid #f8f8f8;
    background-color: white;
  }

  #toc .toclevel1 {
    margin-top: 0.5em;
  }

  #toc .toclevel2 {
    margin-top: 0.25em;
    display: list-item;
    color: #aaaaaa;
  }

  #toctitle {
    margin-top: 0.5em;
  }
}
</style>
<script type="text/javascript">
/*<![CDATA[*/
var asciidoc = {  // Namespace.

/////////////////////////////////////////////////////////////////////
// Table Of Contents generator
/////////////////////////////////////////////////////////////////////

/* Author: Mihai Bazon, September 2002
 * http://students.infoiasi.ro/~mishoo
 *
 * Table Of Content generator
 * Version: 0.4
 *
 * Feel free to use this script under the terms of the GNU General Public
 * License, as long as you do not remove or alter this notice.
 */

 /* modified by Troy D. Hanson, September 2006. License: GPL */
 /* modified by Stuart Rackham, 2006, 2009. License: GPL */

// toclevels = 1..4.
toc: function (toclevels) {

  function getText(el) {
    var text = "";
    for (var i = el.firstChild; i != null; i = i.nextSibling) {
      if (i.nodeType == 3 /* Node.TEXT_NODE */) // IE doesn't speak constants.
        text += i.data;
      else if (i.firstChild != null)
        text += getText(i);
    }
    return text;
  }

  function TocEntry(el, text, toclevel) {
    this.element = el;
    this.text = text;
    this.toclevel = toclevel;
  }

  function tocEntries(el, toclevels) {
    var result = new Array;
    var re = new RegExp('[hH]([1-'+(toclevels+1)+'])');
    // Function that scans the DOM tree for header elements (the DOM2
    // nodeIterator API would be a better technique but not supported by all
    // browsers).
    var iterate = function (el) {
      for (var i = el.firstChild; i != null; i = i.nextSibling) {
        if (i.nodeType == 1 /* Node.ELEMENT_NODE */) {
          var mo = re.exec(i.tagName);
          if (mo && (i.getAttribute("class") || i.getAttribute("className")) != "float") {
            result[result.length] = new TocEntry(i, getText(i), mo[1]-1);
          }
          iterate(i);
        }
      }
    }
    iterate(el);
    return result;
  }

  var toc = document.getElementById("toc");
  if (!toc) {
    return;
  }

  // Delete existing TOC entries in case we're reloading the TOC.
  var tocEntriesToRemove = [];
  var i;
  for (i = 0; i < toc.childNodes.length; i++) {
    var entry = toc.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div'
     && entry.getAttribute("class")
     && entry.getAttribute("class").match(/^toclevel/))
      tocEntriesToRemove.push(entry);
  }
  for (i = 0; i < tocEntriesToRemove.length; i++) {
    toc.removeChild(tocEntriesToRemove[i]);
  }

  // Rebuild TOC entries.
  var entries = tocEntries(document.getElementById("content"), toclevels);
  for (var i = 0; i < entries.length; ++i) {
    var entry = entries[i];
    if (entry.element.id == "")
      entry.element.id = "_toc_" + i;
    var a = document.createElement("a");
    a.href = "#" + entry.element.id;
    a.appendChild(document.createTextNode(entry.text));
    var div = document.createElement("div");
    div.appendChild(a);
    div.className = "toclevel" + entry.toclevel;
    toc.appendChild(div);
  }
  if (entries.length == 0)
    toc.parentNode.removeChild(toc);
},


/////////////////////////////////////////////////////////////////////
// Footnotes generator
/////////////////////////////////////////////////////////////////////

/* Based on footnote generation code from:
 * http://www.brandspankingnew.net/archive/2005/07/format_footnote.html
 */

footnotes: function () {
  // Delete existing footnote entries in case we're reloading the footnodes.
  var i;
  var noteholder = document.getElementById("footnotes");
  if (!noteholder) {
    return;
  }
  var entriesToRemove = [];
  for (i = 0; i < noteholder.childNodes.length; i++) {
    var entry = noteholder.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div' && entry.getAttribute("class") == "footnote")
      entriesToRemove.push(entry);
  }
  for (i = 0; i < entriesToRemove.length; i++) {
    noteholder.removeChild(entriesToRemove[i]);
  }

  // Rebuild footnote entries.
  var cont = document.getElementById("content");
  var spans = cont.getElementsByTagName("span");
  var refs = {};
  var n = 0;
  for (i=0; i<spans.length; i++) {
    if (spans[i].className == "footnote") {
      n++;
      var note = spans[i].getAttribute("data-note");
      if (!note) {
        // Use [\s\S] in place of . so multi-line matches work.
        // Because JavaScript has no s (dotall) regex flag.
        note = spans[i].innerHTML.match(/\s*\[([\s\S]*)]\s*/)[1];
        spans[i].innerHTML =
          "[<a id='_footnoteref_" + n + "' href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
        spans[i].setAttribute("data-note", note);
      }
      noteholder.innerHTML +=
        "<div class='footnote' id='_footnote_" + n + "'>" +
        "<a href='#_footnoteref_" + n + "' title='Return to text'>" +
        n + "</a>. " + note + "</div>";
      var id =spans[i].getAttribute("id");
      if (id != null) refs["#"+id] = n;
    }
  }
  if (n == 0)
    noteholder.parentNode.removeChild(noteholder);
  else {
    // Process footnoterefs.
    for (i=0; i<spans.length; i++) {
      if (spans[i].className == "footnoteref") {
        var href = spans[i].getElementsByTagName("a")[0].getAttribute("href");
        href = href.match(/#.*/)[0];  // Because IE return full URL.
        n = refs[href];
        spans[i].innerHTML =
          "[<a href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
      }
    }
  }
},

install: function(toclevels) {
  var timerId;

  function reinstall() {
    asciidoc.footnotes();
    if (toclevels) {
      asciidoc.toc(toclevels);
    }
  }

  function reinstallAndRemoveTimer() {
    clearInterval(timerId);
    reinstall();
  }

  timerId = setInterval(reinstall, 500);
  if (document.addEventListener)
    document.addEventListener("DOMContentLoaded", reinstallAndRemoveTimer, false);
  else
    window.onload = reinstallAndRemoveTimer;
}

}
asciidoc.install(2);
/*]]>*/
</script>
<script type="text/javascript">
/*<![CDATA[*/
/*
LaTeXMathML.js
==============

This file, in this form, is due to Douglas Woodall, June 2006.
It contains JavaScript functions to convert (most simple) LaTeX
math notation to Presentation MathML.  It was obtained by
downloading the file ASCIIMathML.js from
	http://www1.chapman.edu/~jipsen/mathml/asciimathdownload/
and modifying it so that it carries out ONLY those conversions
that would be carried out in LaTeX.  A description of the original
file, with examples, can be found at
	www1.chapman.edu/~jipsen/mathml/asciimath.html
	ASCIIMathML: Math on the web for everyone

Here is the header notice from the original file:

ASCIIMathML.js
==============
This file contains JavaScript functions to convert ASCII math notation
to Presentation MathML. The conversion is done while the (X)HTML page
loads, and should work with Firefox/Mozilla/Netscape 7+ and Internet
Explorer 6+MathPlayer (http://www.dessci.com/en/products/mathplayer/).
Just add the next line to your (X)HTML page with this file in the same folder:
This is a convenient and inexpensive solution for authoring MathML.

Version 1.4.7 Dec 15, 2005, (c) Peter Jipsen http://www.chapman.edu/~jipsen
Latest version at http://www.chapman.edu/~jipsen/mathml/ASCIIMathML.js
For changes see http://www.chapman.edu/~jipsen/mathml/asciimathchanges.txt
If you use it on a webpage, please send the URL to jipsen@chapman.edu

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
General Public License (at http://www.gnu.org/copyleft/gpl.html)
for more details.

LaTeXMathML.js (ctd)
==============

The instructions for use are the same as for the original
ASCIIMathML.js, except that of course the line you add to your
file should be
Or use absolute path names if the file is not in the same folder
as your (X)HTML page.
*/

var checkForMathML = true;   // check if browser can display MathML
var notifyIfNoMathML = true; // display note if no MathML capability
var alertIfNoMathML = false;  // show alert box if no MathML capability
// was "red":
var mathcolor = "";	     // change it to "" (to inherit) or any other color
// was "serif":
var mathfontfamily = "";      // change to "" to inherit (works in IE)
                              // or another family (e.g. "arial")
var showasciiformulaonhover = true; // helps students learn ASCIIMath
/*
// Commented out by DRW -- not now used -- see DELIMITERS (twice) near the end
var displaystyle = false;     // puts limits above and below large operators
var decimalsign = ".";        // change to "," if you like, beware of `(1,2)`!
var AMdelimiter1 = "`", AMescape1 = "\\\\`"; // can use other characters
var AMdelimiter2 = "$", AMescape2 = "\\\\\\$", AMdelimiter2regexp = "\\$";
var doubleblankmathdelimiter = false; // if true,  x+1  is equal to `x+1`
                                      // for IE this works only in <!--   -->
//var separatetokens;// has been removed (email me if this is a problem)
*/
var isIE = document.createElementNS==null;

if (document.getElementById==null)
  alert("This webpage requires a recent browser such as\
\nMozilla/Netscape 7+ or Internet Explorer 6+MathPlayer")

// all further global variables start with "AM"

function AMcreateElementXHTML(t) {
  if (isIE) return document.createElement(t);
  else return document.createElementNS("http://www.w3.org/1999/xhtml",t);
}

function AMnoMathMLNote() {
  var nd = AMcreateElementXHTML("h3");
  nd.setAttribute("align","center")
  nd.appendChild(AMcreateElementXHTML("p"));
  nd.appendChild(document.createTextNode("To view the "));
  var an = AMcreateElementXHTML("a");
  an.appendChild(document.createTextNode("LaTeXMathML"));
  an.setAttribute("href","http://www.maths.nott.ac.uk/personal/drw/lm.html");
  nd.appendChild(an);
  nd.appendChild(document.createTextNode(" notation use Internet Explorer 6+"));
  an = AMcreateElementXHTML("a");
  an.appendChild(document.createTextNode("MathPlayer"));
  an.setAttribute("href","http://www.dessci.com/en/products/mathplayer/download.htm");
  nd.appendChild(an);
  nd.appendChild(document.createTextNode(" or Netscape/Mozilla/Firefox"));
  nd.appendChild(AMcreateElementXHTML("p"));
  return nd;
}

function AMisMathMLavailable() {
  if (navigator.appName.slice(0,8)=="Netscape")
    if (navigator.appVersion.slice(0,1)>="5") return null;
    else return AMnoMathMLNote();
  else if (navigator.appName.slice(0,9)=="Microsoft")
    try {
        var ActiveX = new ActiveXObject("MathPlayer.Factory.1");
        return null;
    } catch (e) {
        return AMnoMathMLNote();
    }
  else return AMnoMathMLNote();
}

// character lists for Mozilla/Netscape fonts
var AMcal = [0xEF35,0x212C,0xEF36,0xEF37,0x2130,0x2131,0xEF38,0x210B,0x2110,0xEF39,0xEF3A,0x2112,0x2133,0xEF3B,0xEF3C,0xEF3D,0xEF3E,0x211B,0xEF3F,0xEF40,0xEF41,0xEF42,0xEF43,0xEF44,0xEF45,0xEF46];
var AMfrk = [0xEF5D,0xEF5E,0x212D,0xEF5F,0xEF60,0xEF61,0xEF62,0x210C,0x2111,0xEF63,0xEF64,0xEF65,0xEF66,0xEF67,0xEF68,0xEF69,0xEF6A,0x211C,0xEF6B,0xEF6C,0xEF6D,0xEF6E,0xEF6F,0xEF70,0xEF71,0x2128];
var AMbbb = [0xEF8C,0xEF8D,0x2102,0xEF8E,0xEF8F,0xEF90,0xEF91,0x210D,0xEF92,0xEF93,0xEF94,0xEF95,0xEF96,0x2115,0xEF97,0x2119,0x211A,0x211D,0xEF98,0xEF99,0xEF9A,0xEF9B,0xEF9C,0xEF9D,0xEF9E,0x2124];

var CONST = 0, UNARY = 1, BINARY = 2, INFIX = 3, LEFTBRACKET = 4,
    RIGHTBRACKET = 5, SPACE = 6, UNDEROVER = 7, DEFINITION = 8,
    TEXT = 9, BIG = 10, LONG = 11, STRETCHY = 12, MATRIX = 13; // token types

var AMsqrt = {input:"\\sqrt",	tag:"msqrt", output:"sqrt",	ttype:UNARY},
  AMroot = {input:"\\root",	tag:"mroot", output:"root",	ttype:BINARY},
  AMfrac = {input:"\\frac",	tag:"mfrac", output:"/",	ttype:BINARY},
  AMover = {input:"\\stackrel", tag:"mover", output:"stackrel", ttype:BINARY},
  AMatop = {input:"\\atop",	tag:"mfrac", output:"",		ttype:INFIX},
  AMchoose = {input:"\\choose", tag:"mfrac", output:"",		ttype:INFIX},
  AMsub  = {input:"_",		tag:"msub",  output:"_",	ttype:INFIX},
  AMsup  = {input:"^",		tag:"msup",  output:"^",	ttype:INFIX},
  AMtext = {input:"\\mathrm",	tag:"mtext", output:"text",	ttype:TEXT},
  AMmbox = {input:"\\mbox",	tag:"mtext", output:"mbox",	ttype:TEXT};

// Commented out by DRW to prevent 1/2 turning into a 2-line fraction
// AMdiv   = {input:"/",	 tag:"mfrac", output:"/",    ttype:INFIX},
// Commented out by DRW so that " prints literally in equations
// AMquote = {input:"\"",	 tag:"mtext", output:"mbox", ttype:TEXT};

var AMsymbols = [
//Greek letters
{input:"\\alpha",	tag:"mi", output:"\u03B1", ttype:CONST},
{input:"\\beta",	tag:"mi", output:"\u03B2", ttype:CONST},
{input:"\\gamma",	tag:"mi", output:"\u03B3", ttype:CONST},
{input:"\\delta",	tag:"mi", output:"\u03B4", ttype:CONST},
{input:"\\epsilon",	tag:"mi", output:"\u03B5", ttype:CONST},
{input:"\\varepsilon",  tag:"mi", output:"\u025B", ttype:CONST},
{input:"\\zeta",	tag:"mi", output:"\u03B6", ttype:CONST},
{input:"\\eta",		tag:"mi", output:"\u03B7", ttype:CONST},
{input:"\\theta",	tag:"mi", output:"\u03B8", ttype:CONST},
{input:"\\vartheta",	tag:"mi", output:"\u03D1", ttype:CONST},
{input:"\\iota",	tag:"mi", output:"\u03B9", ttype:CONST},
{input:"\\kappa",	tag:"mi", output:"\u03BA", ttype:CONST},
{input:"\\lambda",	tag:"mi", output:"\u03BB", ttype:CONST},
{input:"\\mu",		tag:"mi", output:"\u03BC", ttype:CONST},
{input:"\\nu",		tag:"mi", output:"\u03BD", ttype:CONST},
{input:"\\xi",		tag:"mi", output:"\u03BE", ttype:CONST},
{input:"\\pi",		tag:"mi", output:"\u03C0", ttype:CONST},
{input:"\\varpi",	tag:"mi", output:"\u03D6", ttype:CONST},
{input:"\\rho",		tag:"mi", output:"\u03C1", ttype:CONST},
{input:"\\varrho",	tag:"mi", output:"\u03F1", ttype:CONST},
{input:"\\varsigma",	tag:"mi", output:"\u03C2", ttype:CONST},
{input:"\\sigma",	tag:"mi", output:"\u03C3", ttype:CONST},
{input:"\\tau",		tag:"mi", output:"\u03C4", ttype:CONST},
{input:"\\upsilon",	tag:"mi", output:"\u03C5", ttype:CONST},
{input:"\\phi",		tag:"mi", output:"\u03C6", ttype:CONST},
{input:"\\varphi",	tag:"mi", output:"\u03D5", ttype:CONST},
{input:"\\chi",		tag:"mi", output:"\u03C7", ttype:CONST},
{input:"\\psi",		tag:"mi", output:"\u03C8", ttype:CONST},
{input:"\\omega",	tag:"mi", output:"\u03C9", ttype:CONST},
{input:"\\Gamma",	tag:"mo", output:"\u0393", ttype:CONST},
{input:"\\Delta",	tag:"mo", output:"\u0394", ttype:CONST},
{input:"\\Theta",	tag:"mo", output:"\u0398", ttype:CONST},
{input:"\\Lambda",	tag:"mo", output:"\u039B", ttype:CONST},
{input:"\\Xi",		tag:"mo", output:"\u039E", ttype:CONST},
{input:"\\Pi",		tag:"mo", output:"\u03A0", ttype:CONST},
{input:"\\Sigma",	tag:"mo", output:"\u03A3", ttype:CONST},
{input:"\\Upsilon",	tag:"mo", output:"\u03A5", ttype:CONST},
{input:"\\Phi",		tag:"mo", output:"\u03A6", ttype:CONST},
{input:"\\Psi",		tag:"mo", output:"\u03A8", ttype:CONST},
{input:"\\Omega",	tag:"mo", output:"\u03A9", ttype:CONST},

//fractions
{input:"\\frac12",	tag:"mo", output:"\u00BD", ttype:CONST},
{input:"\\frac14",	tag:"mo", output:"\u00BC", ttype:CONST},
{input:"\\frac34",	tag:"mo", output:"\u00BE", ttype:CONST},
{input:"\\frac13",	tag:"mo", output:"\u2153", ttype:CONST},
{input:"\\frac23",	tag:"mo", output:"\u2154", ttype:CONST},
{input:"\\frac15",	tag:"mo", output:"\u2155", ttype:CONST},
{input:"\\frac25",	tag:"mo", output:"\u2156", ttype:CONST},
{input:"\\frac35",	tag:"mo", output:"\u2157", ttype:CONST},
{input:"\\frac45",	tag:"mo", output:"\u2158", ttype:CONST},
{input:"\\frac16",	tag:"mo", output:"\u2159", ttype:CONST},
{input:"\\frac56",	tag:"mo", output:"\u215A", ttype:CONST},
{input:"\\frac18",	tag:"mo", output:"\u215B", ttype:CONST},
{input:"\\frac38",	tag:"mo", output:"\u215C", ttype:CONST},
{input:"\\frac58",	tag:"mo", output:"\u215D", ttype:CONST},
{input:"\\frac78",	tag:"mo", output:"\u215E", ttype:CONST},

//binary operation symbols
{input:"\\pm",		tag:"mo", output:"\u00B1", ttype:CONST},
{input:"\\mp",		tag:"mo", output:"\u2213", ttype:CONST},
{input:"\\triangleleft",tag:"mo", output:"\u22B2", ttype:CONST},
{input:"\\triangleright",tag:"mo",output:"\u22B3", ttype:CONST},
{input:"\\cdot",	tag:"mo", output:"\u22C5", ttype:CONST},
{input:"\\star",	tag:"mo", output:"\u22C6", ttype:CONST},
{input:"\\ast",		tag:"mo", output:"\u002A", ttype:CONST},
{input:"\\times",	tag:"mo", output:"\u00D7", ttype:CONST},
{input:"\\div",		tag:"mo", output:"\u00F7", ttype:CONST},
{input:"\\circ",	tag:"mo", output:"\u2218", ttype:CONST},
//{input:"\\bullet",	  tag:"mo", output:"\u2219", ttype:CONST},
{input:"\\bullet",	tag:"mo", output:"\u2022", ttype:CONST},
{input:"\\oplus",	tag:"mo", output:"\u2295", ttype:CONST},
{input:"\\ominus",	tag:"mo", output:"\u2296", ttype:CONST},
{input:"\\otimes",	tag:"mo", output:"\u2297", ttype:CONST},
{input:"\\bigcirc",	tag:"mo", output:"\u25CB", ttype:CONST},
{input:"\\oslash",	tag:"mo", output:"\u2298", ttype:CONST},
{input:"\\odot",	tag:"mo", output:"\u2299", ttype:CONST},
{input:"\\land",	tag:"mo", output:"\u2227", ttype:CONST},
{input:"\\wedge",	tag:"mo", output:"\u2227", ttype:CONST},
{input:"\\lor",		tag:"mo", output:"\u2228", ttype:CONST},
{input:"\\vee",		tag:"mo", output:"\u2228", ttype:CONST},
{input:"\\cap",		tag:"mo", output:"\u2229", ttype:CONST},
{input:"\\cup",		tag:"mo", output:"\u222A", ttype:CONST},
{input:"\\sqcap",	tag:"mo", output:"\u2293", ttype:CONST},
{input:"\\sqcup",	tag:"mo", output:"\u2294", ttype:CONST},
{input:"\\uplus",	tag:"mo", output:"\u228E", ttype:CONST},
{input:"\\amalg",	tag:"mo", output:"\u2210", ttype:CONST},
{input:"\\bigtriangleup",tag:"mo",output:"\u25B3", ttype:CONST},
{input:"\\bigtriangledown",tag:"mo",output:"\u25BD", ttype:CONST},
{input:"\\dag",		tag:"mo", output:"\u2020", ttype:CONST},
{input:"\\dagger",	tag:"mo", output:"\u2020", ttype:CONST},
{input:"\\ddag",	tag:"mo", output:"\u2021", ttype:CONST},
{input:"\\ddagger",	tag:"mo", output:"\u2021", ttype:CONST},
{input:"\\lhd",		tag:"mo", output:"\u22B2", ttype:CONST},
{input:"\\rhd",		tag:"mo", output:"\u22B3", ttype:CONST},
{input:"\\unlhd",	tag:"mo", output:"\u22B4", ttype:CONST},
{input:"\\unrhd",	tag:"mo", output:"\u22B5", ttype:CONST},


//BIG Operators
{input:"\\sum",		tag:"mo", output:"\u2211", ttype:UNDEROVER},
{input:"\\prod",	tag:"mo", output:"\u220F", ttype:UNDEROVER},
{input:"\\bigcap",	tag:"mo", output:"\u22C2", ttype:UNDEROVER},
{input:"\\bigcup",	tag:"mo", output:"\u22C3", ttype:UNDEROVER},
{input:"\\bigwedge",	tag:"mo", output:"\u22C0", ttype:UNDEROVER},
{input:"\\bigvee",	tag:"mo", output:"\u22C1", ttype:UNDEROVER},
{input:"\\bigsqcap",	tag:"mo", output:"\u2A05", ttype:UNDEROVER},
{input:"\\bigsqcup",	tag:"mo", output:"\u2A06", ttype:UNDEROVER},
{input:"\\coprod",	tag:"mo", output:"\u2210", ttype:UNDEROVER},
{input:"\\bigoplus",	tag:"mo", output:"\u2A01", ttype:UNDEROVER},
{input:"\\bigotimes",	tag:"mo", output:"\u2A02", ttype:UNDEROVER},
{input:"\\bigodot",	tag:"mo", output:"\u2A00", ttype:UNDEROVER},
{input:"\\biguplus",	tag:"mo", output:"\u2A04", ttype:UNDEROVER},
{input:"\\int",		tag:"mo", output:"\u222B", ttype:CONST},
{input:"\\oint",	tag:"mo", output:"\u222E", ttype:CONST},

//binary relation symbols
{input:":=",		tag:"mo", output:":=",	   ttype:CONST},
{input:"\\lt",		tag:"mo", output:"<",	   ttype:CONST},
{input:"\\gt",		tag:"mo", output:">",	   ttype:CONST},
{input:"\\ne",		tag:"mo", output:"\u2260", ttype:CONST},
{input:"\\neq",		tag:"mo", output:"\u2260", ttype:CONST},
{input:"\\le",		tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\leq",		tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\leqslant",	tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\ge",		tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\geq",		tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\geqslant",	tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\equiv",	tag:"mo", output:"\u2261", ttype:CONST},
{input:"\\ll",		tag:"mo", output:"\u226A", ttype:CONST},
{input:"\\gg",		tag:"mo", output:"\u226B", ttype:CONST},
{input:"\\doteq",	tag:"mo", output:"\u2250", ttype:CONST},
{input:"\\prec",	tag:"mo", output:"\u227A", ttype:CONST},
{input:"\\succ",	tag:"mo", output:"\u227B", ttype:CONST},
{input:"\\preceq",	tag:"mo", output:"\u227C", ttype:CONST},
{input:"\\succeq",	tag:"mo", output:"\u227D", ttype:CONST},
{input:"\\subset",	tag:"mo", output:"\u2282", ttype:CONST},
{input:"\\supset",	tag:"mo", output:"\u2283", ttype:CONST},
{input:"\\subseteq",	tag:"mo", output:"\u2286", ttype:CONST},
{input:"\\supseteq",	tag:"mo", output:"\u2287", ttype:CONST},
{input:"\\sqsubset",	tag:"mo", output:"\u228F", ttype:CONST},
{input:"\\sqsupset",	tag:"mo", output:"\u2290", ttype:CONST},
{input:"\\sqsubseteq",  tag:"mo", output:"\u2291", ttype:CONST},
{input:"\\sqsupseteq",  tag:"mo", output:"\u2292", ttype:CONST},
{input:"\\sim",		tag:"mo", output:"\u223C", ttype:CONST},
{input:"\\simeq",	tag:"mo", output:"\u2243", ttype:CONST},
{input:"\\approx",	tag:"mo", output:"\u2248", ttype:CONST},
{input:"\\cong",	tag:"mo", output:"\u2245", ttype:CONST},
{input:"\\Join",	tag:"mo", output:"\u22C8", ttype:CONST},
{input:"\\bowtie",	tag:"mo", output:"\u22C8", ttype:CONST},
{input:"\\in",		tag:"mo", output:"\u2208", ttype:CONST},
{input:"\\ni",		tag:"mo", output:"\u220B", ttype:CONST},
{input:"\\owns",	tag:"mo", output:"\u220B", ttype:CONST},
{input:"\\propto",	tag:"mo", output:"\u221D", ttype:CONST},
{input:"\\vdash",	tag:"mo", output:"\u22A2", ttype:CONST},
{input:"\\dashv",	tag:"mo", output:"\u22A3", ttype:CONST},
{input:"\\models",	tag:"mo", output:"\u22A8", ttype:CONST},
{input:"\\perp",	tag:"mo", output:"\u22A5", ttype:CONST},
{input:"\\smile",	tag:"mo", output:"\u2323", ttype:CONST},
{input:"\\frown",	tag:"mo", output:"\u2322", ttype:CONST},
{input:"\\asymp",	tag:"mo", output:"\u224D", ttype:CONST},
{input:"\\notin",	tag:"mo", output:"\u2209", ttype:CONST},

//matrices
{input:"\\begin{eqnarray}",	output:"X",	ttype:MATRIX, invisible:true},
{input:"\\begin{array}",	output:"X",	ttype:MATRIX, invisible:true},
{input:"\\\\",			output:"}&{",	ttype:DEFINITION},
{input:"\\end{eqnarray}",	output:"}}",	ttype:DEFINITION},
{input:"\\end{array}",		output:"}}",	ttype:DEFINITION},

//grouping and literal brackets -- ieval is for IE
{input:"\\big",	   tag:"mo", output:"X", atval:"1.2", ieval:"2.2", ttype:BIG},
{input:"\\Big",	   tag:"mo", output:"X", atval:"1.6", ieval:"2.6", ttype:BIG},
{input:"\\bigg",   tag:"mo", output:"X", atval:"2.2", ieval:"3.2", ttype:BIG},
{input:"\\Bigg",   tag:"mo", output:"X", atval:"2.9", ieval:"3.9", ttype:BIG},
{input:"\\left",   tag:"mo", output:"X", ttype:LEFTBRACKET},
{input:"\\right",  tag:"mo", output:"X", ttype:RIGHTBRACKET},
{input:"{",	   output:"{", ttype:LEFTBRACKET,  invisible:true},
{input:"}",	   output:"}", ttype:RIGHTBRACKET, invisible:true},

{input:"(",	   tag:"mo", output:"(",      atval:"1", ttype:STRETCHY},
{input:"[",	   tag:"mo", output:"[",      atval:"1", ttype:STRETCHY},
{input:"\\lbrack", tag:"mo", output:"[",      atval:"1", ttype:STRETCHY},
{input:"\\{",	   tag:"mo", output:"{",      atval:"1", ttype:STRETCHY},
{input:"\\lbrace", tag:"mo", output:"{",      atval:"1", ttype:STRETCHY},
{input:"\\langle", tag:"mo", output:"\u2329", atval:"1", ttype:STRETCHY},
{input:"\\lfloor", tag:"mo", output:"\u230A", atval:"1", ttype:STRETCHY},
{input:"\\lceil",  tag:"mo", output:"\u2308", atval:"1", ttype:STRETCHY},

// rtag:"mi" causes space to be inserted before a following sin, cos, etc.
// (see function AMparseExpr() )
{input:")",	  tag:"mo",output:")",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"]",	  tag:"mo",output:"]",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rbrack",tag:"mo",output:"]",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\}",	  tag:"mo",output:"}",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rbrace",tag:"mo",output:"}",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rangle",tag:"mo",output:"\u232A", rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rfloor",tag:"mo",output:"\u230B", rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rceil", tag:"mo",output:"\u2309", rtag:"mi",atval:"1",ttype:STRETCHY},

// "|", "\\|", "\\vert" and "\\Vert" modified later: lspace = rspace = 0em
{input:"|",		tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\|",		tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"\\vert",	tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\Vert",	tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"\\mid",		tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\parallel",	tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"/",		tag:"mo", output:"/",	atval:"1.01", ttype:STRETCHY},
{input:"\\backslash",	tag:"mo", output:"\u2216", atval:"1", ttype:STRETCHY},
{input:"\\setminus",	tag:"mo", output:"\\",	   ttype:CONST},

//miscellaneous symbols
{input:"\\!",	  tag:"mspace", atname:"width", atval:"-0.167em", ttype:SPACE},
{input:"\\,",	  tag:"mspace", atname:"width", atval:"0.167em", ttype:SPACE},
{input:"\\>",	  tag:"mspace", atname:"width", atval:"0.222em", ttype:SPACE},
{input:"\\:",	  tag:"mspace", atname:"width", atval:"0.222em", ttype:SPACE},
{input:"\\;",	  tag:"mspace", atname:"width", atval:"0.278em", ttype:SPACE},
{input:"~",	  tag:"mspace", atname:"width", atval:"0.333em", ttype:SPACE},
{input:"\\quad",  tag:"mspace", atname:"width", atval:"1em", ttype:SPACE},
{input:"\\qquad", tag:"mspace", atname:"width", atval:"2em", ttype:SPACE},
//{input:"{}",		  tag:"mo", output:"\u200B", ttype:CONST}, // zero-width
{input:"\\prime",	tag:"mo", output:"\u2032", ttype:CONST},
{input:"'",		tag:"mo", output:"\u02B9", ttype:CONST},
{input:"''",		tag:"mo", output:"\u02BA", ttype:CONST},
{input:"'''",		tag:"mo", output:"\u2034", ttype:CONST},
{input:"''''",		tag:"mo", output:"\u2057", ttype:CONST},
{input:"\\ldots",	tag:"mo", output:"\u2026", ttype:CONST},
{input:"\\cdots",	tag:"mo", output:"\u22EF", ttype:CONST},
{input:"\\vdots",	tag:"mo", output:"\u22EE", ttype:CONST},
{input:"\\ddots",	tag:"mo", output:"\u22F1", ttype:CONST},
{input:"\\forall",	tag:"mo", output:"\u2200", ttype:CONST},
{input:"\\exists",	tag:"mo", output:"\u2203", ttype:CONST},
{input:"\\Re",		tag:"mo", output:"\u211C", ttype:CONST},
{input:"\\Im",		tag:"mo", output:"\u2111", ttype:CONST},
{input:"\\aleph",	tag:"mo", output:"\u2135", ttype:CONST},
{input:"\\hbar",	tag:"mo", output:"\u210F", ttype:CONST},
{input:"\\ell",		tag:"mo", output:"\u2113", ttype:CONST},
{input:"\\wp",		tag:"mo", output:"\u2118", ttype:CONST},
{input:"\\emptyset",	tag:"mo", output:"\u2205", ttype:CONST},
{input:"\\infty",	tag:"mo", output:"\u221E", ttype:CONST},
{input:"\\surd",	tag:"mo", output:"\\sqrt{}", ttype:DEFINITION},
{input:"\\partial",	tag:"mo", output:"\u2202", ttype:CONST},
{input:"\\nabla",	tag:"mo", output:"\u2207", ttype:CONST},
{input:"\\triangle",	tag:"mo", output:"\u25B3", ttype:CONST},
{input:"\\therefore",	tag:"mo", output:"\u2234", ttype:CONST},
{input:"\\angle",	tag:"mo", output:"\u2220", ttype:CONST},
//{input:"\\\\ ",	  tag:"mo", output:"\u00A0", ttype:CONST},
{input:"\\diamond",	tag:"mo", output:"\u22C4", ttype:CONST},
//{input:"\\Diamond",	  tag:"mo", output:"\u25CA", ttype:CONST},
{input:"\\Diamond",	tag:"mo", output:"\u25C7", ttype:CONST},
{input:"\\neg",		tag:"mo", output:"\u00AC", ttype:CONST},
{input:"\\lnot",	tag:"mo", output:"\u00AC", ttype:CONST},
{input:"\\bot",		tag:"mo", output:"\u22A5", ttype:CONST},
{input:"\\top",		tag:"mo", output:"\u22A4", ttype:CONST},
{input:"\\square",	tag:"mo", output:"\u25AB", ttype:CONST},
{input:"\\Box",		tag:"mo", output:"\u25A1", ttype:CONST},
{input:"\\wr",		tag:"mo", output:"\u2240", ttype:CONST},

//standard functions
//Note UNDEROVER *must* have tag:"mo" to work properly
{input:"\\arccos", tag:"mi", output:"arccos", ttype:UNARY, func:true},
{input:"\\arcsin", tag:"mi", output:"arcsin", ttype:UNARY, func:true},
{input:"\\arctan", tag:"mi", output:"arctan", ttype:UNARY, func:true},
{input:"\\arg",	   tag:"mi", output:"arg",    ttype:UNARY, func:true},
{input:"\\cos",	   tag:"mi", output:"cos",    ttype:UNARY, func:true},
{input:"\\cosh",   tag:"mi", output:"cosh",   ttype:UNARY, func:true},
{input:"\\cot",	   tag:"mi", output:"cot",    ttype:UNARY, func:true},
{input:"\\coth",   tag:"mi", output:"coth",   ttype:UNARY, func:true},
{input:"\\csc",	   tag:"mi", output:"csc",    ttype:UNARY, func:true},
{input:"\\deg",	   tag:"mi", output:"deg",    ttype:UNARY, func:true},
{input:"\\det",	   tag:"mi", output:"det",    ttype:UNARY, func:true},
{input:"\\dim",	   tag:"mi", output:"dim",    ttype:UNARY, func:true}, //CONST?
{input:"\\exp",	   tag:"mi", output:"exp",    ttype:UNARY, func:true},
{input:"\\gcd",	   tag:"mi", output:"gcd",    ttype:UNARY, func:true}, //CONST?
{input:"\\hom",	   tag:"mi", output:"hom",    ttype:UNARY, func:true},
{input:"\\inf",	      tag:"mo", output:"inf",	 ttype:UNDEROVER},
{input:"\\ker",	   tag:"mi", output:"ker",    ttype:UNARY, func:true},
{input:"\\lg",	   tag:"mi", output:"lg",     ttype:UNARY, func:true},
{input:"\\lim",	      tag:"mo", output:"lim",	 ttype:UNDEROVER},
{input:"\\liminf",    tag:"mo", output:"liminf", ttype:UNDEROVER},
{input:"\\limsup",    tag:"mo", output:"limsup", ttype:UNDEROVER},
{input:"\\ln",	   tag:"mi", output:"ln",     ttype:UNARY, func:true},
{input:"\\log",	   tag:"mi", output:"log",    ttype:UNARY, func:true},
{input:"\\max",	      tag:"mo", output:"max",	 ttype:UNDEROVER},
{input:"\\min",	      tag:"mo", output:"min",	 ttype:UNDEROVER},
{input:"\\Pr",	   tag:"mi", output:"Pr",     ttype:UNARY, func:true},
{input:"\\sec",	   tag:"mi", output:"sec",    ttype:UNARY, func:true},
{input:"\\sin",	   tag:"mi", output:"sin",    ttype:UNARY, func:true},
{input:"\\sinh",   tag:"mi", output:"sinh",   ttype:UNARY, func:true},
{input:"\\sup",	      tag:"mo", output:"sup",	 ttype:UNDEROVER},
{input:"\\tan",	   tag:"mi", output:"tan",    ttype:UNARY, func:true},
{input:"\\tanh",   tag:"mi", output:"tanh",   ttype:UNARY, func:true},

//arrows
{input:"\\gets",		tag:"mo", output:"\u2190", ttype:CONST},
{input:"\\leftarrow",		tag:"mo", output:"\u2190", ttype:CONST},
{input:"\\to",			tag:"mo", output:"\u2192", ttype:CONST},
{input:"\\rightarrow",		tag:"mo", output:"\u2192", ttype:CONST},
{input:"\\leftrightarrow",	tag:"mo", output:"\u2194", ttype:CONST},
{input:"\\uparrow",		tag:"mo", output:"\u2191", ttype:CONST},
{input:"\\downarrow",		tag:"mo", output:"\u2193", ttype:CONST},
{input:"\\updownarrow",		tag:"mo", output:"\u2195", ttype:CONST},
{input:"\\Leftarrow",		tag:"mo", output:"\u21D0", ttype:CONST},
{input:"\\Rightarrow",		tag:"mo", output:"\u21D2", ttype:CONST},
{input:"\\Leftrightarrow",	tag:"mo", output:"\u21D4", ttype:CONST},
{input:"\\iff", tag:"mo", output:"~\\Longleftrightarrow~", ttype:DEFINITION},
{input:"\\Uparrow",		tag:"mo", output:"\u21D1", ttype:CONST},
{input:"\\Downarrow",		tag:"mo", output:"\u21D3", ttype:CONST},
{input:"\\Updownarrow",		tag:"mo", output:"\u21D5", ttype:CONST},
{input:"\\mapsto",		tag:"mo", output:"\u21A6", ttype:CONST},
{input:"\\longleftarrow",	tag:"mo", output:"\u2190", ttype:LONG},
{input:"\\longrightarrow",	tag:"mo", output:"\u2192", ttype:LONG},
{input:"\\longleftrightarrow",	tag:"mo", output:"\u2194", ttype:LONG},
{input:"\\Longleftarrow",	tag:"mo", output:"\u21D0", ttype:LONG},
{input:"\\Longrightarrow",	tag:"mo", output:"\u21D2", ttype:LONG},
{input:"\\Longleftrightarrow",  tag:"mo", output:"\u21D4", ttype:LONG},
{input:"\\longmapsto",		tag:"mo", output:"\u21A6", ttype:CONST},
							// disaster if LONG

//commands with argument
AMsqrt, AMroot, AMfrac, AMover, AMsub, AMsup, AMtext, AMmbox, AMatop, AMchoose,
//AMdiv, AMquote,

//diacritical marks
{input:"\\acute",	tag:"mover",  output:"\u00B4", ttype:UNARY, acc:true},
//{input:"\\acute",	  tag:"mover",  output:"\u0317", ttype:UNARY, acc:true},
//{input:"\\acute",	  tag:"mover",  output:"\u0301", ttype:UNARY, acc:true},
//{input:"\\grave",	  tag:"mover",  output:"\u0300", ttype:UNARY, acc:true},
//{input:"\\grave",	  tag:"mover",  output:"\u0316", ttype:UNARY, acc:true},
{input:"\\grave",	tag:"mover",  output:"\u0060", ttype:UNARY, acc:true},
{input:"\\breve",	tag:"mover",  output:"\u02D8", ttype:UNARY, acc:true},
{input:"\\check",	tag:"mover",  output:"\u02C7", ttype:UNARY, acc:true},
{input:"\\dot",		tag:"mover",  output:".",      ttype:UNARY, acc:true},
{input:"\\ddot",	tag:"mover",  output:"..",     ttype:UNARY, acc:true},
//{input:"\\ddot",	  tag:"mover",  output:"\u00A8", ttype:UNARY, acc:true},
{input:"\\mathring",	tag:"mover",  output:"\u00B0", ttype:UNARY, acc:true},
{input:"\\vec",		tag:"mover",  output:"\u20D7", ttype:UNARY, acc:true},
{input:"\\overrightarrow",tag:"mover",output:"\u20D7", ttype:UNARY, acc:true},
{input:"\\overleftarrow",tag:"mover", output:"\u20D6", ttype:UNARY, acc:true},
{input:"\\hat",		tag:"mover",  output:"\u005E", ttype:UNARY, acc:true},
{input:"\\widehat",	tag:"mover",  output:"\u0302", ttype:UNARY, acc:true},
{input:"\\tilde",	tag:"mover",  output:"~",      ttype:UNARY, acc:true},
//{input:"\\tilde",	  tag:"mover",  output:"\u0303", ttype:UNARY, acc:true},
{input:"\\widetilde",	tag:"mover",  output:"\u02DC", ttype:UNARY, acc:true},
{input:"\\bar",		tag:"mover",  output:"\u203E", ttype:UNARY, acc:true},
{input:"\\overbrace",	tag:"mover",  output:"\u23B4", ttype:UNARY, acc:true},
{input:"\\overline",	tag:"mover",  output:"\u00AF", ttype:UNARY, acc:true},
{input:"\\underbrace",  tag:"munder", output:"\u23B5", ttype:UNARY, acc:true},
{input:"\\underline",	tag:"munder", output:"\u00AF", ttype:UNARY, acc:true},
//{input:"underline",	tag:"munder", output:"\u0332", ttype:UNARY, acc:true},

//typestyles and fonts
{input:"\\displaystyle",tag:"mstyle",atname:"displaystyle",atval:"true", ttype:UNARY},
{input:"\\textstyle",tag:"mstyle",atname:"displaystyle",atval:"false", ttype:UNARY},
{input:"\\scriptstyle",tag:"mstyle",atname:"scriptlevel",atval:"1", ttype:UNARY},
{input:"\\scriptscriptstyle",tag:"mstyle",atname:"scriptlevel",atval:"2", ttype:UNARY},
{input:"\\textrm", tag:"mstyle", output:"\\mathrm", ttype: DEFINITION},
{input:"\\mathbf", tag:"mstyle", atname:"mathvariant", atval:"bold", ttype:UNARY},
{input:"\\textbf", tag:"mstyle", atname:"mathvariant", atval:"bold", ttype:UNARY},
{input:"\\mathit", tag:"mstyle", atname:"mathvariant", atval:"italic", ttype:UNARY},
{input:"\\textit", tag:"mstyle", atname:"mathvariant", atval:"italic", ttype:UNARY},
{input:"\\mathtt", tag:"mstyle", atname:"mathvariant", atval:"monospace", ttype:UNARY},
{input:"\\texttt", tag:"mstyle", atname:"mathvariant", atval:"monospace", ttype:UNARY},
{input:"\\mathsf", tag:"mstyle", atname:"mathvariant", atval:"sans-serif", ttype:UNARY},
{input:"\\mathbb", tag:"mstyle", atname:"mathvariant", atval:"double-struck", ttype:UNARY, codes:AMbbb},
{input:"\\mathcal",tag:"mstyle", atname:"mathvariant", atval:"script", ttype:UNARY, codes:AMcal},
{input:"\\mathfrak",tag:"mstyle",atname:"mathvariant", atval:"fraktur",ttype:UNARY, codes:AMfrk}
];

function compareNames(s1,s2) {
  if (s1.input > s2.input) return 1
  else return -1;
}

var AMnames = []; //list of input symbols

function AMinitSymbols() {
  AMsymbols.sort(compareNames);
  for (i=0; i<AMsymbols.length; i++) AMnames[i] = AMsymbols[i].input;
}

var AMmathml = "http://www.w3.org/1998/Math/MathML";

function AMcreateElementMathML(t) {
  if (isIE) return document.createElement("m:"+t);
  else return document.createElementNS(AMmathml,t);
}

function AMcreateMmlNode(t,frag) {
//  var node = AMcreateElementMathML(name);
  if (isIE) var node = document.createElement("m:"+t);
  else var node = document.createElementNS(AMmathml,t);
  node.appendChild(frag);
  return node;
}

function newcommand(oldstr,newstr) {
  AMsymbols = AMsymbols.concat([{input:oldstr, tag:"mo", output:newstr,
                                 ttype:DEFINITION}]);
}

function AMremoveCharsAndBlanks(str,n) {
//remove n characters and any following blanks
  var st;
  st = str.slice(n);
  for (var i=0; i<st.length && st.charCodeAt(i)<=32; i=i+1);
  return st.slice(i);
}

function AMposition(arr, str, n) {
// return position >=n where str appears or would be inserted
// assumes arr is sorted
  if (n==0) {
    var h,m;
    n = -1;
    h = arr.length;
    while (n+1<h) {
      m = (n+h) >> 1;
      if (arr[m]<str) n = m; else h = m;
    }
    return h;
  } else
    for (var i=n; i<arr.length && arr[i]<str; i++);
  return i; // i=arr.length || arr[i]>=str
}

function AMgetSymbol(str) {
//return maximal initial substring of str that appears in names
//return null if there is none
  var k = 0; //new pos
  var j = 0; //old pos
  var mk; //match pos
  var st;
  var tagst;
  var match = "";
  var more = true;
  for (var i=1; i<=str.length && more; i++) {
    st = str.slice(0,i); //initial substring of length i
    j = k;
    k = AMposition(AMnames, st, j);
    if (k<AMnames.length && str.slice(0,AMnames[k].length)==AMnames[k]){
      match = AMnames[k];
      mk = k;
      i = match.length;
    }
    more = k<AMnames.length && str.slice(0,AMnames[k].length)>=AMnames[k];
  }
  AMpreviousSymbol=AMcurrentSymbol;
  if (match!=""){
    AMcurrentSymbol=AMsymbols[mk].ttype;
    return AMsymbols[mk];
  }
  AMcurrentSymbol=CONST;
  k = 1;
  st = str.slice(0,1); //take 1 character
  if ("0"<=st && st<="9") tagst = "mn";
  else tagst = (("A">st || st>"Z") && ("a">st || st>"z")?"mo":"mi");
/*
// Commented out by DRW (not fully understood, but probably to do with
// use of "/" as an INFIX version of "\\frac", which we don't want):
//}
//if (st=="-" && AMpreviousSymbol==INFIX) {
//  AMcurrentSymbol = INFIX;  //trick "/" into recognizing "-" on second parse
//  return {input:st, tag:tagst, output:st, ttype:UNARY, func:true};
//}
*/
  return {input:st, tag:tagst, output:st, ttype:CONST};
}


/*Parsing ASCII math expressions with the following grammar
v ::= [A-Za-z] | greek letters | numbers | other constant symbols
u ::= sqrt | text | bb | other unary symbols for font commands
b ::= frac | root | stackrel	binary symbols
l ::= { | \left			left brackets
r ::= } | \right		right brackets
S ::= v | lEr | uS | bSS	Simple expression
I ::= S_S | S^S | S_S^S | S	Intermediate expression
E ::= IE | I/I			Expression
Each terminal symbol is translated into a corresponding mathml node.*/

var AMpreviousSymbol,AMcurrentSymbol;

function AMparseSexpr(str) { //parses str and returns [node,tailstr,(node)tag]
  var symbol, node, result, result2, i, st,// rightvert = false,
    newFrag = document.createDocumentFragment();
  str = AMremoveCharsAndBlanks(str,0);
  symbol = AMgetSymbol(str);             //either a token or a bracket or empty
  if (symbol == null || symbol.ttype == RIGHTBRACKET)
    return [null,str,null];
  if (symbol.ttype == DEFINITION) {
    str = symbol.output+AMremoveCharsAndBlanks(str,symbol.input.length);
    symbol = AMgetSymbol(str);
    if (symbol == null || symbol.ttype == RIGHTBRACKET)
      return [null,str,null];
  }
  str = AMremoveCharsAndBlanks(str,symbol.input.length);
  switch (symbol.ttype) {
  case SPACE:
    node = AMcreateElementMathML(symbol.tag);
    node.setAttribute(symbol.atname,symbol.atval);
    return [node,str,symbol.tag];
  case UNDEROVER:
    if (isIE) {
      if (symbol.input.substr(0,4) == "\\big") {   // botch for missing symbols
	str = "\\"+symbol.input.substr(4)+str;	   // make \bigcup = \cup etc.
	symbol = AMgetSymbol(str);
	symbol.ttype = UNDEROVER;
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
      }
    }
    return [AMcreateMmlNode(symbol.tag,
			document.createTextNode(symbol.output)),str,symbol.tag];
  case CONST:
    var output = symbol.output;
    if (isIE) {
      if (symbol.input == "'")
	output = "\u2032";
      else if (symbol.input == "''")
	output = "\u2033";
      else if (symbol.input == "'''")
	output = "\u2033\u2032";
      else if (symbol.input == "''''")
	output = "\u2033\u2033";
      else if (symbol.input == "\\square")
	output = "\u25A1";	// same as \Box
      else if (symbol.input.substr(0,5) == "\\frac") {
						// botch for missing fractions
	var denom = symbol.input.substr(6,1);
	if (denom == "5" || denom == "6") {
	  str = symbol.input.replace(/\\frac/,"\\frac ")+str;
	  return [node,str,symbol.tag];
	}
      }
    }
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(output));
    return [node,str,symbol.tag];
  case LONG:  // added by DRW
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    node.setAttribute("minsize","1.5");
    node.setAttribute("maxsize","1.5");
    node = AMcreateMmlNode("mover",node);
    node.appendChild(AMcreateElementMathML("mspace"));
    return [node,str,symbol.tag];
  case STRETCHY:  // added by DRW
    if (isIE && symbol.input == "\\backslash")
	symbol.output = "\\";	// doesn't expand, but then nor does "\u2216"
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    if (symbol.input == "|" || symbol.input == "\\vert" ||
	symbol.input == "\\|" || symbol.input == "\\Vert") {
	  node.setAttribute("lspace","0em");
	  node.setAttribute("rspace","0em");
    }
    node.setAttribute("maxsize",symbol.atval);  // don't allow to stretch here
    if (symbol.rtag != null)
      return [node,str,symbol.rtag];
    else
      return [node,str,symbol.tag];
  case BIG:  // added by DRW
    var atval = symbol.atval;
    if (isIE)
      atval = symbol.ieval;
    symbol = AMgetSymbol(str);
    if (symbol == null)
	return [null,str,null];
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    if (isIE) {		// to get brackets to expand
      var space = AMcreateElementMathML("mspace");
      space.setAttribute("height",atval+"ex");
      node = AMcreateMmlNode("mrow",node);
      node.appendChild(space);
    } else {		// ignored in IE
      node.setAttribute("minsize",atval);
      node.setAttribute("maxsize",atval);
    }
    return [node,str,symbol.tag];
  case LEFTBRACKET:   //read (expr+)
    if (symbol.input == "\\left") { // left what?
      symbol = AMgetSymbol(str);
      if (symbol != null) {
	if (symbol.input == ".")
	  symbol.invisible = true;
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
      }
    }
    result = AMparseExpr(str,true,false);
    if (symbol==null ||
	(typeof symbol.invisible == "boolean" && symbol.invisible))
      node = AMcreateMmlNode("mrow",result[0]);
    else {
      node = AMcreateMmlNode("mo",document.createTextNode(symbol.output));
      node = AMcreateMmlNode("mrow",node);
      node.appendChild(result[0]);
    }
    return [node,result[1],result[2]];
  case MATRIX:	 //read (expr+)
    if (symbol.input == "\\begin{array}") {
      var mask = "";
      symbol = AMgetSymbol(str);
      str = AMremoveCharsAndBlanks(str,0);
      if (symbol == null)
	mask = "l";
      else {
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
	if (symbol.input != "{")
	  mask = "l";
	else do {
	  symbol = AMgetSymbol(str);
	  if (symbol != null) {
	    str = AMremoveCharsAndBlanks(str,symbol.input.length);
	    if (symbol.input != "}")
	      mask = mask+symbol.input;
	  }
	} while (symbol != null && symbol.input != "" && symbol.input != "}");
      }
      result = AMparseExpr("{"+str,true,true);
//    if (result[0]==null) return [AMcreateMmlNode("mo",
//			   document.createTextNode(symbol.input)),str];
      node = AMcreateMmlNode("mtable",result[0]);
      mask = mask.replace(/l/g,"left ");
      mask = mask.replace(/r/g,"right ");
      mask = mask.replace(/c/g,"center ");
      node.setAttribute("columnalign",mask);
      node.setAttribute("displaystyle","false");
      if (isIE)
	return [node,result[1],null];
// trying to get a *little* bit of space around the array
// (IE already includes it)
      var lspace = AMcreateElementMathML("mspace");
      lspace.setAttribute("width","0.167em");
      var rspace = AMcreateElementMathML("mspace");
      rspace.setAttribute("width","0.167em");
      var node1 = AMcreateMmlNode("mrow",lspace);
      node1.appendChild(node);
      node1.appendChild(rspace);
      return [node1,result[1],null];
    } else {	// eqnarray
      result = AMparseExpr("{"+str,true,true);
      node = AMcreateMmlNode("mtable",result[0]);
      if (isIE)
	node.setAttribute("columnspacing","0.25em"); // best in practice?
      else
	node.setAttribute("columnspacing","0.167em"); // correct (but ignored?)
      node.setAttribute("columnalign","right center left");
      node.setAttribute("displaystyle","true");
      node = AMcreateMmlNode("mrow",node);
      return [node,result[1],null];
    }
  case TEXT:
      if (str.charAt(0)=="{") i=str.indexOf("}");
      else i = 0;
      if (i==-1)
		 i = str.length;
      st = str.slice(1,i);
      if (st.charAt(0) == " ") {
	node = AMcreateElementMathML("mspace");
	node.setAttribute("width","0.33em");	// was 1ex
	newFrag.appendChild(node);
      }
      newFrag.appendChild(
        AMcreateMmlNode(symbol.tag,document.createTextNode(st)));
      if (st.charAt(st.length-1) == " ") {
	node = AMcreateElementMathML("mspace");
	node.setAttribute("width","0.33em");	// was 1ex
	newFrag.appendChild(node);
      }
      str = AMremoveCharsAndBlanks(str,i+1);
      return [AMcreateMmlNode("mrow",newFrag),str,null];
  case UNARY:
      result = AMparseSexpr(str);
      if (result[0]==null) return [AMcreateMmlNode(symbol.tag,
                             document.createTextNode(symbol.output)),str];
      if (typeof symbol.func == "boolean" && symbol.func) { // functions hack
	st = str.charAt(0);
//	if (st=="^" || st=="_" || st=="/" || st=="|" || st==",") {
	if (st=="^" || st=="_" || st==",") {
	  return [AMcreateMmlNode(symbol.tag,
		    document.createTextNode(symbol.output)),str,symbol.tag];
        } else {
	  node = AMcreateMmlNode("mrow",
	   AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output)));
	  if (isIE) {
	    var space = AMcreateElementMathML("mspace");
	    space.setAttribute("width","0.167em");
	    node.appendChild(space);
	  }
	  node.appendChild(result[0]);
	  return [node,result[1],symbol.tag];
        }
      }
      if (symbol.input == "\\sqrt") {		// sqrt
	if (isIE) {	// set minsize, for \surd
	  var space = AMcreateElementMathML("mspace");
	  space.setAttribute("height","1.2ex");
	  space.setAttribute("width","0em");	// probably no effect
	  node = AMcreateMmlNode(symbol.tag,result[0])
//	  node.setAttribute("minsize","1");	// ignored
//	  node = AMcreateMmlNode("mrow",node);  // hopefully unnecessary
	  node.appendChild(space);
	  return [node,result[1],symbol.tag];
	} else
	  return [AMcreateMmlNode(symbol.tag,result[0]),result[1],symbol.tag];
      } else if (typeof symbol.acc == "boolean" && symbol.acc) {   // accent
        node = AMcreateMmlNode(symbol.tag,result[0]);
	var output = symbol.output;
	if (isIE) {
		if (symbol.input == "\\hat")
			output = "\u0302";
		else if (symbol.input == "\\widehat")
			output = "\u005E";
		else if (symbol.input == "\\bar")
			output = "\u00AF";
		else if (symbol.input == "\\grave")
			output = "\u0300";
		else if (symbol.input == "\\tilde")
			output = "\u0303";
	}
	var node1 = AMcreateMmlNode("mo",document.createTextNode(output));
	if (symbol.input == "\\vec" || symbol.input == "\\check")
						// don't allow to stretch
	    node1.setAttribute("maxsize","1.2");
		 // why doesn't "1" work?  \vec nearly disappears in firefox
	if (isIE && symbol.input == "\\bar")
	    node1.setAttribute("maxsize","0.5");
	if (symbol.input == "\\underbrace" || symbol.input == "\\underline")
	  node1.setAttribute("accentunder","true");
	else
	  node1.setAttribute("accent","true");
	node.appendChild(node1);
	if (symbol.input == "\\overbrace" || symbol.input == "\\underbrace")
	  node.ttype = UNDEROVER;
	return [node,result[1],symbol.tag];
      } else {			      // font change or displaystyle command
        if (!isIE && typeof symbol.codes != "undefined") {
          for (i=0; i<result[0].childNodes.length; i++)
            if (result[0].childNodes[i].nodeName=="mi" || result[0].nodeName=="mi") {
              st = (result[0].nodeName=="mi"?result[0].firstChild.nodeValue:
                              result[0].childNodes[i].firstChild.nodeValue);
              var newst = [];
              for (var j=0; j<st.length; j++)
                if (st.charCodeAt(j)>64 && st.charCodeAt(j)<91) newst = newst +
                  String.fromCharCode(symbol.codes[st.charCodeAt(j)-65]);
                else newst = newst + st.charAt(j);
              if (result[0].nodeName=="mi")
                result[0]=AMcreateElementMathML("mo").
                          appendChild(document.createTextNode(newst));
              else result[0].replaceChild(AMcreateElementMathML("mo").
          appendChild(document.createTextNode(newst)),result[0].childNodes[i]);
            }
        }
        node = AMcreateMmlNode(symbol.tag,result[0]);
        node.setAttribute(symbol.atname,symbol.atval);
	if (symbol.input == "\\scriptstyle" ||
	    symbol.input == "\\scriptscriptstyle")
		node.setAttribute("displaystyle","false");
	return [node,result[1],symbol.tag];
      }
  case BINARY:
    result = AMparseSexpr(str);
    if (result[0]==null) return [AMcreateMmlNode("mo",
			   document.createTextNode(symbol.input)),str,null];
    result2 = AMparseSexpr(result[1]);
    if (result2[0]==null) return [AMcreateMmlNode("mo",
			   document.createTextNode(symbol.input)),str,null];
    if (symbol.input=="\\root" || symbol.input=="\\stackrel")
      newFrag.appendChild(result2[0]);
    newFrag.appendChild(result[0]);
    if (symbol.input=="\\frac") newFrag.appendChild(result2[0]);
    return [AMcreateMmlNode(symbol.tag,newFrag),result2[1],symbol.tag];
  case INFIX:
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    return [AMcreateMmlNode("mo",document.createTextNode(symbol.output)),
	str,symbol.tag];
  default:
    return [AMcreateMmlNode(symbol.tag,        //its a constant
	document.createTextNode(symbol.output)),str,symbol.tag];
  }
}

function AMparseIexpr(str) {
  var symbol, sym1, sym2, node, result, tag, underover;
  str = AMremoveCharsAndBlanks(str,0);
  sym1 = AMgetSymbol(str);
  result = AMparseSexpr(str);
  node = result[0];
  str = result[1];
  tag = result[2];
  symbol = AMgetSymbol(str);
  if (symbol.ttype == INFIX) {
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    result = AMparseSexpr(str);
    if (result[0] == null) // show box in place of missing argument
      result[0] = AMcreateMmlNode("mo",document.createTextNode("\u25A1"));
    str = result[1];
    tag = result[2];
    if (symbol.input == "_" || symbol.input == "^") {
      sym2 = AMgetSymbol(str);
      tag = null;	// no space between x^2 and a following sin, cos, etc.
// This is for \underbrace and \overbrace
      underover = ((sym1.ttype == UNDEROVER) || (node.ttype == UNDEROVER));
//    underover = (sym1.ttype == UNDEROVER);
      if (symbol.input == "_" && sym2.input == "^") {
        str = AMremoveCharsAndBlanks(str,sym2.input.length);
        var res2 = AMparseSexpr(str);
	str = res2[1];
	tag = res2[2];  // leave space between x_1^2 and a following sin etc.
        node = AMcreateMmlNode((underover?"munderover":"msubsup"),node);
        node.appendChild(result[0]);
        node.appendChild(res2[0]);
      } else if (symbol.input == "_") {
	node = AMcreateMmlNode((underover?"munder":"msub"),node);
        node.appendChild(result[0]);
      } else {
	node = AMcreateMmlNode((underover?"mover":"msup"),node);
        node.appendChild(result[0]);
      }
      node = AMcreateMmlNode("mrow",node); // so sum does not stretch
    } else {
      node = AMcreateMmlNode(symbol.tag,node);
      if (symbol.input == "\\atop" || symbol.input == "\\choose")
	node.setAttribute("linethickness","0ex");
      node.appendChild(result[0]);
      if (symbol.input == "\\choose")
	node = AMcreateMmlNode("mfenced",node);
    }
  }
  return [node,str,tag];
}

function AMparseExpr(str,rightbracket,matrix) {
  var symbol, node, result, i, tag,
  newFrag = document.createDocumentFragment();
  do {
    str = AMremoveCharsAndBlanks(str,0);
    result = AMparseIexpr(str);
    node = result[0];
    str = result[1];
    tag = result[2];
    symbol = AMgetSymbol(str);
    if (node!=undefined) {
      if ((tag == "mn" || tag == "mi") && symbol!=null &&
	typeof symbol.func == "boolean" && symbol.func) {
			// Add space before \sin in 2\sin x or x\sin x
	  var space = AMcreateElementMathML("mspace");
	  space.setAttribute("width","0.167em");
	  node = AMcreateMmlNode("mrow",node);
	  node.appendChild(space);
      }
      newFrag.appendChild(node);
    }
  } while ((symbol.ttype != RIGHTBRACKET)
        && symbol!=null && symbol.output!="");
  tag = null;
  if (symbol.ttype == RIGHTBRACKET) {
    if (symbol.input == "\\right") { // right what?
      str = AMremoveCharsAndBlanks(str,symbol.input.length);
      symbol = AMgetSymbol(str);
      if (symbol != null && symbol.input == ".")
	symbol.invisible = true;
      if (symbol != null)
	tag = symbol.rtag;
    }
    if (symbol!=null)
      str = AMremoveCharsAndBlanks(str,symbol.input.length); // ready to return
    var len = newFrag.childNodes.length;
    if (matrix &&
      len>0 && newFrag.childNodes[len-1].nodeName == "mrow" && len>1 &&
      newFrag.childNodes[len-2].nodeName == "mo" &&
      newFrag.childNodes[len-2].firstChild.nodeValue == "&") { //matrix
	var pos = []; // positions of ampersands
        var m = newFrag.childNodes.length;
        for (i=0; matrix && i<m; i=i+2) {
          pos[i] = [];
          node = newFrag.childNodes[i];
	  for (var j=0; j<node.childNodes.length; j++)
	    if (node.childNodes[j].firstChild.nodeValue=="&")
	      pos[i][pos[i].length]=j;
        }
	var row, frag, n, k, table = document.createDocumentFragment();
	for (i=0; i<m; i=i+2) {
	  row = document.createDocumentFragment();
	  frag = document.createDocumentFragment();
	  node = newFrag.firstChild; // <mrow> -&-&...&-&- </mrow>
	  n = node.childNodes.length;
	  k = 0;
	  for (j=0; j<n; j++) {
	    if (typeof pos[i][k] != "undefined" && j==pos[i][k]){
	      node.removeChild(node.firstChild); //remove &
	      row.appendChild(AMcreateMmlNode("mtd",frag));
	      k++;
	    } else frag.appendChild(node.firstChild);
	  }
	  row.appendChild(AMcreateMmlNode("mtd",frag));
	  if (newFrag.childNodes.length>2) {
	    newFrag.removeChild(newFrag.firstChild); //remove <mrow> </mrow>
	    newFrag.removeChild(newFrag.firstChild); //remove <mo>&</mo>
	  }
	  table.appendChild(AMcreateMmlNode("mtr",row));
	}
	return [table,str];
    }
    if (typeof symbol.invisible != "boolean" || !symbol.invisible) {
      node = AMcreateMmlNode("mo",document.createTextNode(symbol.output));
      newFrag.appendChild(node);
    }
  }
  return [newFrag,str,tag];
}

function AMparseMath(str) {
  var result, node = AMcreateElementMathML("mstyle");
  if (mathcolor != "") node.setAttribute("mathcolor",mathcolor);
  if (mathfontfamily != "") node.setAttribute("fontfamily",mathfontfamily);
  node.appendChild(AMparseExpr(str.replace(/^\s+/g,""),false,false)[0]);
  node = AMcreateMmlNode("math",node);
  if (showasciiformulaonhover)                      //fixed by djhsu so newline
    node.setAttribute("title",str.replace(/\s+/g," "));//does not show in Gecko
  if (mathfontfamily != "" && (isIE || mathfontfamily != "serif")) {
    var fnode = AMcreateElementXHTML("font");
    fnode.setAttribute("face",mathfontfamily);
    fnode.appendChild(node);
    return fnode;
  }
  return node;
}

function AMstrarr2docFrag(arr, linebreaks) {
  var newFrag=document.createDocumentFragment();
  var expr = false;
  for (var i=0; i<arr.length; i++) {
    if (expr) newFrag.appendChild(AMparseMath(arr[i]));
    else {
      var arri = (linebreaks ? arr[i].split("\n\n") : [arr[i]]);
      newFrag.appendChild(AMcreateElementXHTML("span").
      appendChild(document.createTextNode(arri[0])));
      for (var j=1; j<arri.length; j++) {
        newFrag.appendChild(AMcreateElementXHTML("p"));
        newFrag.appendChild(AMcreateElementXHTML("span").
        appendChild(document.createTextNode(arri[j])));
      }
    }
    expr = !expr;
  }
  return newFrag;
}

function AMprocessNodeR(n, linebreaks) {
  var mtch, str, arr, frg, i;
  if (n.childNodes.length == 0) {
   if ((n.nodeType!=8 || linebreaks) &&
    n.parentNode.nodeName!="form" && n.parentNode.nodeName!="FORM" &&
    n.parentNode.nodeName!="textarea" && n.parentNode.nodeName!="TEXTAREA" &&
    n.parentNode.nodeName!="pre" && n.parentNode.nodeName!="PRE") {
    str = n.nodeValue;
    if (!(str == null)) {
      str = str.replace(/\r\n\r\n/g,"\n\n");
      str = str.replace(/\x20+/g," ");
      str = str.replace(/\s*\r\n/g," ");
// DELIMITERS:
      mtch = (str.indexOf("\$")==-1 ? false : true);
      str = str.replace(/([^\\])\$/g,"$1 \$");
      str = str.replace(/^\$/," \$");	// in case \$ at start of string
      arr = str.split(" \$");
      for (i=0; i<arr.length; i++)
	arr[i]=arr[i].replace(/\\\$/g,"\$");
      if (arr.length>1 || mtch) {
        if (checkForMathML) {
          checkForMathML = false;
          var nd = AMisMathMLavailable();
          AMnoMathML = nd != null;
          if (AMnoMathML && notifyIfNoMathML)
            if (alertIfNoMathML)
              alert("To view the ASCIIMathML notation use Internet Explorer 6 +\nMathPlayer (free from www.dessci.com)\n\
                or Firefox/Mozilla/Netscape");
            else AMbody.insertBefore(nd,AMbody.childNodes[0]);
        }
        if (!AMnoMathML) {
          frg = AMstrarr2docFrag(arr,n.nodeType==8);
          var len = frg.childNodes.length;
          n.parentNode.replaceChild(frg,n);
          return len-1;
        } else return 0;
      }
    }
   } else return 0;
  } else if (n.nodeName!="math") {
    for (i=0; i<n.childNodes.length; i++)
      i += AMprocessNodeR(n.childNodes[i], linebreaks);
  }
  return 0;
}

function AMprocessNode(n, linebreaks, spanclassAM) {
  var frag,st;
  if (spanclassAM!=null) {
    frag = document.getElementsByTagName("span")
    for (var i=0;i<frag.length;i++)
      if (frag[i].className == "AM")
        AMprocessNodeR(frag[i],linebreaks);
  } else {
    try {
      st = n.innerHTML;
    } catch(err) {}
// DELIMITERS:
    if (st==null || st.indexOf("\$")!=-1)
      AMprocessNodeR(n,linebreaks);
  }
  if (isIE) { //needed to match size and font of formula to surrounding text
    frag = document.getElementsByTagName('math');
    for (var i=0;i<frag.length;i++) frag[i].update()
  }
}

var AMbody;
var AMnoMathML = false, AMtranslated = false;

function translate(spanclassAM) {
  if (!AMtranslated) { // run this only once
    AMtranslated = true;
    AMinitSymbols();
    AMbody = document.getElementsByTagName("body")[0];
    AMprocessNode(AMbody, false, spanclassAM);
  }
}

if (isIE) { // avoid adding MathPlayer info explicitly to each webpage
  document.write("<object id=\"mathplayer\"\
  classid=\"clsid:32F66A20-7614-11D4-BD11-00104BD3F987\"></object>");
  document.write("<?import namespace=\"m\" implementation=\"#mathplayer\"?>");
}

// GO1.1 Generic onload by Brothercake
// http://www.brothercake.com/
//onload function (replaces the onload="translate()" in the <body> tag)
function generic()
{
  translate();
};
//setup onload function
if(typeof window.addEventListener != 'undefined')
{
  //.. gecko, safari, konqueror and standard
  window.addEventListener('load', generic, false);
}
else if(typeof document.addEventListener != 'undefined')
{
  //.. opera 7
  document.addEventListener('load', generic, false);
}
else if(typeof window.attachEvent != 'undefined')
{
  //.. win/ie
  window.attachEvent('onload', generic);
}
//** remove this condition to degrade older browsers
else
{
  //.. mac/ie5 and anything else that gets this far
  //if there's an existing onload function
  if(typeof window.onload == 'function')
  {
    //store it
    var existing = onload;
    //add new onload handler
    window.onload = function()
    {
      //call existing onload function
      existing();
      //call generic onload function
      generic();
    };
  }
  else
  {
    //setup onload function
    window.onload = generic;
  }
}
/*]]>*/
</script>
</head>
<body class="book">
<div id="header">
<h1>Thermodynamic Analytics Toolkit (TATi)</h1>
<span id="author">Frederik Heber</span><br>
<span id="email" class="monospaced">&lt;<a href="mailto:frederik.heber@gmail.com">frederik.heber@gmail.com</a>&gt;</span><br>
<span id="revnumber">version v0.9.1-0-g994aa50,</span>
<span id="revdate">2018-08-17</span>
<div id="toc">
  <div id="toctitle">Table of Contents</div>
  <noscript><p><b>JavaScript must be enabled in your browser to display the table of contents.</b></p></noscript>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="imageblock" style="text-align:center;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/./pictures/tati_logo.png" alt="TATi logo" width="700">
</div>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">2018-08-17 thermodynamicanalyticstoolkit: v0.9.1-0-g994aa50</div>
<div class="paragraph"><p>TATi is a software suite written in Python based on <a href="https://www.tensorflow.org/">tensorflow</a>'s
Python API. It brings advanced sampling methods (GLA1 and GLA2, BAOAB, HMC) to
<em>neural network training</em>. Its <strong>tools</strong> allow to assess the loss manifold&#8217;s
topology that depends on the employed neural network and the dataset. Moreover,
its <strong>simulation</strong> module makes applying present sampling Python codes in the
context of neural networks easy and straight-forward. The goal of the software
is to enable the user to analyze and adapt the network employed for a specific
classification problem to best fit her or his needs.</p></div>
<div class="paragraph"><p>TATi has received financial support from a seed funding grant and through a
Rutherford fellowship from the Alan Turing Institute in London (R-SIS-003,
R-RUT-001) and EPSRC grant no. EP/P006175/1 (Data Driven Coarse Graining using
Space-Time Diffusion Maps, B. Leimkuhler PI).</p></div>
<div class="paragraph"><p><em>Frederik Heber</em></p></div>
</div></div>
</div>
</div>
<div class="sect1">
<h2 id="introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph"><p>As an image says more than a thousand words, here is a mindmap showing all of
TATi&#8217;s main features.</p></div>
<div class="imageblock" style="text-align:right;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/tati-mindmap.png" alt="TATi mindmap" width="500">
</div>
<div class="title">Figure 1. Mindmap of all TATi features</div>
</div>
<div class="paragraph"><p>If you know what all of this means, you might probably be interested. If not,
then read on to the next section.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">TATi is growing, see the
 <a href="roadmap.html">roadmap</a> 
for things to come.</td>
</tr></table>
</div>
<div class="sect2">
<h3 id="introduction.needtoknow">1.1. Before you start</h3>
<div class="paragraph"><p>In the following we assume that you, the reader, has a general
familiarity with neural networks. You should know what a classification
problem is, what an associated dataset for (supervised) learning needs
to contain. You should know about what weights and biases in a neural
network are and what the loss function does. You should also have a
rough idea of what optimization is and that gradients with respect to
the chosen loss function can be obtained through so-called
backpropagation.</p></div>
<div class="paragraph"><p>If you are <em>not</em> familiar with the above terminology, then please first
read the section on <a href="#reference.concepts">Concepts.</a></p></div>
</div>
<div class="sect2">
<h3 id="introduction.whatis">1.2. What is ThermodynamicAnalyticsToolkit?</h3>
<div class="paragraph"><p>Typically, optimzation in neural network training employs methods such
as Gradient Descent, Stochastic Gradient Descent or derived methods
using momentum such as ADAM and so on. The loss function itself may be
convex, however, the loss manifold given a large dataset is not convex
in general. Hence, these methods will only find local minima. Naturally,
multiple random starting positions are used and the winner takes it all.
However, as there exponentially many minima this way we will never be able
to glean any knowledge on how they are situated with respect to one another.
We will never know how the network&#8217;s topology, the number of nodes and so on
influences it.
Moreover, the eventually trained set of parameters of the neural network
is never optimal. Not even when taking the generalization error into account.
However, some simplicity must be hidden in those loss manifolds of neural
networks: given enough data and processing power, they work marvelously even
in spite of all these shortcomings and one may wonder why.</p></div>
<div class="paragraph"><p>The essential idea behind this program package is that we use sampling
instead of optimization: we are not interested in the local minimum
closest to some random initial configuration and be done. Instead we aim
at finding all of the minima and all possible barriers in between by
treating the loss function as a high-dimensional potential and the
weights and biases of the neural network as one-dimensional particles in
a stochastic differential equation, namely Langevin Dynamics.</p></div>
<div class="paragraph"><p>There is not need to panic: You do not need to know anything about these
kinds of equations when using the program. However, rest assured that
all statistical properties derived using trajectories obtained through
these equations are meaningful.</p></div>
<div class="paragraph"><p>The hope is to elucidate the marvel behind the wondrous performance of
neural networks, maybe to obtain even better parametrizations or obtain
the same in cheaper ways, and also to gather means of optimizing the
neural network&#8217;s topology given a specific dataset to train.</p></div>
<div class="paragraph"><p>In essence, this program suite provides samplers using
<a href="https://www.tensorflow.org/">TensorFlow</a> and analysis tools to extract
specific statistical quantities from the computed particle trajectories.</p></div>
<div class="imageblock">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/tati_tools-architecture.png" alt="pictures/tati_tools-architecture.png" width="500">
</div>
<div class="title">Figure 2. Architecture of TATi&#8217;s tools</div>
</div>
<div class="paragraph"><p>Moreover, it can also be used as a stand-alone Python module that allows
to easily make use of present sampling Python code within the context of
neural networks.</p></div>
<div class="imageblock">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/tati_simulations-architecture.png" alt="pictures/tati_simulations-architecture.png" width="500">
</div>
<div class="title">Figure 3. Architecture of the <strong>simulation</strong> module</div>
</div>
<div class="paragraph"><p>It can be used both as python module and as stand-alone command-line
tools. The former is called the <strong>simulation</strong> module and will probably be your
first contact point with TATi.</p></div>
</div>
<div class="sect2">
<h3 id="introduction.installation">1.3. Installation</h3>
<div class="paragraph"><p>In the following we explain the installation procedure to get
ThermodynamicAnalyticsToolkit up and running.</p></div>
<div class="sect3">
<h4 id="introduction.installation.requirements">1.3.1. Installation requirements</h4>
<div class="paragraph"><p>This program suite is implemented using python3 and the development
mainly focused on Linux (development machine used Ubuntu 14.04 up to 18.04). At
the moment other operation systems are not supported but may still work.</p></div>
<div class="paragraph"><p>It has the following non-trivial dependencies:</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="https://www.tensorflow.org/">TensorFlow</a>: version 1.4.1 till currently 1.6 supported
</p>
</li>
<li>
<p>
<a href="https://www.numpy.org/">Numpy</a>:
</p>
</li>
<li>
<p>
<a href="https://pandas.pydata.org/">Pandas</a>
</p>
</li>
<li>
<p>
<a href="https://scikit-learn.org/">sklearn</a>
</p>
</li>
</ul></div>
<div class="paragraph"><p>Note that most of these packages can be easily installed using either
the repository tool (using some linux derivate such as Ubuntu), e.g.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>sudo apt install python3-numpy</pre>
</div></div>
<div class="paragraph"><p>or via <strong>pip3</strong>, i.e.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>pip3 install numpy</pre>
</div></div>
<div class="paragraph"><p>Moreover, the following packages are not ultimately required but
examples or tests may depend on them:</p></div>
<div class="ulist"><ul>
<li>
<p>
<a href="http://matplotlib.org/">matplotlib</a>
</p>
</li>
<li>
<p>
<a href="https://www.sqlite.org">sqlite3</a>
</p>
</li>
<li>
<p>
gawk
</p>
</li>
</ul></div>
<div class="paragraph"><p>The documentation is written in <a href="https://asciidoc.org/">AsciiDoc</a> and
requires a suitable package to compile to HTML or create a PDF, e.g., using
dblatex</p></div>
<div class="ulist"><ul>
<li>
<p>
asciidoc
</p>
</li>
<li>
<p>
dblatex
</p>
</li>
</ul></div>
<div class="paragraph"><p>Finally, for the diffusion map analysis we recommend using the pydiffmap
package, see <a href="https://github.com/DiffusionMapsAcademics/pyDiffMap">https://github.com/DiffusionMapsAcademics/pyDiffMap</a>.</p></div>
<div class="paragraph"><p>In our setting what typically worked best was to use <a href="https://anaconda.org/">anaconda</a>
in the following manner:</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>conda create -n tensorflow python=3.5 -y
conda install -n tensorflow -y \
     tensorflow numpy scipy pandas scikit-learn matplotlib</pre>
</div></div>
<div class="paragraph"><p>In case your machine has GPU hardware for tensorflow, replace
&#8220;tensorflow&#8221; by &#8220;tensorflow-gpu&#8221;.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Note that on systems with typical core i7 architecture recompiling
tensorflow from source provided only very small runtime gains in our
tests which in most cases do not support the extra effort. You may find
it necessary for tackling really large networks and datasets and
especially if you desire using Intel&#8217;s MKL library for the CPU-based
linear algebra computations.</td>
</tr></table>
</div>
<div class="paragraph"><p>Henceforth, we assume that there is a working tensorflow on your system,
i.e. inside the python3 shell</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>import tensorflow as tf</pre>
</div></div>
<div class="paragraph"><p>does <em>not</em> throw an error.</p></div>
<div class="paragraph"><p>Moreover,</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>a=tf.constant("Hello world")
sess=tf.Session()
sess.run(a)</pre>
</div></div>
<div class="paragraph"><p>should print &#8220;Hello world&#8221; or something similar.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
<div class="paragraph"><p>You can check the version of your <strong>tensorflow</strong> installation at any time
by inspecting <span class="monospaced">print(tf.__version__)</span>.</p></div>
<div class="paragraph"><p>Similarly, TATi&#8217;s version can be obtained through</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>import tati

print(tati.__version__)</pre>
</div></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="introduction.installation.procedure">1.3.2. Installation procedure</h4>
<div class="paragraph"><p>Installation comes in two flavours: Either a tarball or a cloned repository.</p></div>
<div class="paragraph"><p>The tarball releases are recommended if you only plan to use TATi and do not
intend modifying its code. If, however, you need to use a development branch,
then you have to clone from the repository.</p></div>
<div class="paragraph"><p>In general, this package is distributed via autotools, "compiled" and installed via
automake. If you are familiar with this set of tools, there should be no
problem. If not, please refer to the text <span class="monospaced">INSTALL</span> file that is included
in the tarball.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Only the tarball contains a precompiled PDF. The cloned repository
contains only the HTML pages.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="introduction.installation.procedure.tarball">1.3.3. From Tarball</h4>
<div class="paragraph"><p>Unpack the archive, assuming its suffix is <span class="monospaced">.bz2</span>.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>tar -jxvf thermodynamicanalyticstoolkit-${revnumber}.tar.bz2</pre>
</div></div>
<div class="paragraph"><p>If the ending is <span class="monospaced">.gz</span>, you need to unpack by</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>tar -zxvf thermodynamicanalyticstoolkit-${revnumber}.tar.gz</pre>
</div></div>
<div class="paragraph"><p>Enter the directory</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>cd thermodynamicanalyticstoolkit</pre>
</div></div>
<div class="paragraph"><p>Continue then in section <a href="#configure_make_install">Configure, make, install</a>.</p></div>
</div>
<div class="sect3">
<h4 id="introduction.installation.repository">1.3.4. From cloned repository</h4>
<div class="paragraph"><p>While the tarball does not require any autotools packages installed on your
system, the cloned repository does. You need the following packages:</p></div>
<div class="ulist"><ul>
<li>
<p>
autotools
</p>
</li>
<li>
<p>
automake
</p>
</li>
</ul></div>
<div class="paragraph"><p>To prepare code in the working directory, enter</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>./bootstrap.sh</pre>
</div></div>
</div>
<div class="sect3">
<h4 id="introduction.installation.configure_make_install">1.3.5. Configure, make, make install</h4>
<div class="paragraph"><p>Next, we recommend to build the toolkit not in the source folder but in an
extra folder, e.g., &#8220;build64&#8221;. In the autotools lingo this is called an
<em>out-of-source</em> build. It prevents cluttering of the source folder.
Naturally, you may pick any name (and actually any location on your
computer) as you see fit.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>mkdir build64
cd build64
../configure --prefix="somepath" -C PYTHON="path to python3"
make
make install</pre>
</div></div>
<div class="paragraph"><p>More importantly, please replace &#8220;somepath&#8221; and &#8220;path to python3&#8221; by
the desired installation path and the full path to the <span class="monospaced">python3</span>
executable on your system.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>In case of having used <em>anaconda</em> for the installation of required packages,
then you need to use</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>$HOME/.conda/envs/tensorflow/bin/python3</pre>
</div></div>
<div class="paragraph"><p>for the respective command, where <span class="monospaced">$HOME</span> is your home folder. This assumes
that your anaconda environment is named <strong>tensorflow</strong> as in the example
installation steps above.</p></div>
</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>We recommend executing (after <span class="monospaced">make install</span> was run)</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>make -j4 check</pre>
</div></div>
<div class="paragraph"><p>additionally. This will execute every test on the extensive testsuite
and report any errors. None should fail. If all fail, a possible cause
might be a not working tensorflow installation. If some fail, please
contact the author.
The extra argument <strong>-j4</strong> instructs <span class="monospaced">make</span> to use four threads in parallel for
testing. Use as many as you have cores on your machine.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="introduction.license">1.4. License</h3>
<div class="paragraph"><p>As long as no other license statement is given,
ThermodynamicAnalyticsToolkit is free for use under the GNU Public
License (GPL) Version 3 (see <a href="https://www.gnu.org/licenses/gpl-3.0.en.html">https://www.gnu.org/licenses/gpl-3.0.en.html</a> for
full text).</p></div>
</div>
<div class="sect2">
<h3 id="introduction.disclaimer">1.5. Disclaimer</h3>
<div class="quoteblock">
<div class="content">
<div class="paragraph"><p>Because the program is licensed free of charge, there is not warranty
for the program, to the extent permitted by applicable law. Except when
otherwise stated in writing in the copyright holders and/or other
parties provide the program "as is" without warranty of any kind, either
expressed or implied. Including, but not limited to, the implied
warranties of merchantability and fitness for a particular purpose. The
entire risk as to the quality and performance of the program is with
you. Should the program prove defective, you assume the cost of all
necessary servicing, repair, or correction.</p></div>
</div>
<div class="attribution">
<em>https://www.gnu.org/licenses/gpl-3.0.en.html</em><br>
&#8212; section 11 of the GPLv3 license
</div></div>
</div>
<div class="sect2">
<h3 id="introduction.feedback">1.6. Feedback</h3>
<div class="paragraph"><p>If you encounter any bugs, errors, or would like to submit feature request,
please write to <a href="mailto:">frederik.heber@gmail.com</a> or open an issue at <a href="https://github.com/alan-turing-institute/ThermodynamicAnalyticsToolkit">GitHub</a>.
The author is especially thankful for any description of all related events
prior to occurrence of the error and auxiliary files. More explicitly, the
<strong>following information is crucial</strong> in enabling assistance:</p></div>
<div class="ulist"><ul>
<li>
<p>
<strong>operating system</strong> and version, e.g., Ubuntu 16.04
</p>
</li>
<li>
<p>
<strong>Tensorflow version</strong>, e.g., TF 1.6
</p>
</li>
<li>
<p>
<strong>TATi version</strong> (or respective branch on GitHub), e.g., TATi 0.8
</p>
</li>
<li>
<p>
steps that lead to the error, possibly with <strong>sample Python code</strong>
</p>
</li>
</ul></div>
<div class="paragraph"><p>Please mind sensible space restrictions of email attachments.</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="quickstart">2. Quickstart</h2>
<div class="sectionbody">
<div class="paragraph"><p>Before we come to actually using TATi, we want to set the stage with a little
trivial example: We will look at a very simple classification task and see how
it is solved using neural networks.</p></div>
<div class="sect2">
<h3 id="quickstart.sampling">2.1. Sampling in neural networks</h3>
<div class="imageblock" id="quickstart.dataset" style="text-align:center;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/dataset_two_clusters.png" alt="pictures/dataset_two_clusters.png" width="400">
</div>
<div class="title">Figure 4. Dataset: "Two Clusters" dataset consisting of two normally distributed point clouds in two dimensions</div>
</div>
<div class="paragraph"><p>Assume we are given a very simple data set as depicted in
<a href="#quickstart.dataset">Dataset</a>. The goal is to
classify all red and blue dots into two different classes. This problem
is quite simple to solve: a line in the two-dimensional space can easily
separate the two classes.</p></div>
<div class="paragraph"><p>A very simple neural network, a perceptron, is all we need: it uses two
inputs nodes, namely each coordinate component, $x_{1}$ and
$x_{2}$, and a single output node with an activation
function $f$ whose sign gives the class the input item
belongs to. The network is given in
<a href="#quickstart.perceptron">Network</a>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">A little while later we will see that an even simpler network suffices to
classify the dataset well, which is intrinsically one-dimensional.</td>
</tr></table>
</div>
<div class="imageblock" id="quickstart.perceptron" style="text-align:center;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/simple_single_layer_perceptron.png" alt="pictures/simple_single_layer_perceptron.png" width="200">
</div>
<div class="title">Figure 5. Network: Single-layer perceptron with weights and biases</div>
</div>
<div class="sect3">
<h4 id="_traing_a_neural_network">2.1.1. Traing a Neural Network</h4>
<div class="paragraph"><p>In the following we want to use the mean square loss, i.e. the euclidean
distance between the output from the network and the expected values per
item, as the network&#8217;s loss function. The loss depends implicitly on the
dataset and explicitly on the weights and biases associated with the
network. In our case, we have two weights for the two edges between
input nodes, $w_{1}$ and $w_{2}$, and the output
node and a single bias attached to the output node $b$.</p></div>
<div class="paragraph"><p>The general goal of training is to find the set of parameters that achieve the
lowest loss. To this end, we use the gradient that is readily obtained from the
neural netwotk through backpropagation of the analytically known derivatives of
the activation functions. The parameters are modified using Gradient Descent
until the gradient becomes zero (or we stop before that).</p></div>
</div>
<div class="sect3">
<h4 id="_what_is_sampling">2.1.2. What is Sampling?</h4>
<div class="paragraph"><p>Sampling typically has quite a different perspective: There, we look at a
system of particles that have two internal properties: <em>location</em> and
<em>momentum</em>. The location is simply their current value, that changes through
its momentum over time. The momentum again changes because the particle are
affected by a potential. The system is described by a so-called
Hamilton operator that gives rise to its same named dynamics. If noise is
additionally taken into account, then instead we look at Langevin Dynamics.
<em>Noise is essential</em>: It is connected to the concept of <em>temperature</em> as
the average squared momenta of each particle, the kinetic energy. High
temperature therefore means large amounts of noise, while low temperature is
associated with small amounts.
Temperature is connected to the concept of heat bath that is a reservoir of
kinetic energy allowing the particles to overcome barriers.</p></div>
<div class="paragraph"><p>Returning to the neural networks, the role of the particles is taken up by
the degrees of freedom of the system: weights and biases. The loss
function is called the <em>potential</em> and it is accompanied by a <em>kinetic
energy</em> that is simply the sum of all squared momenta. Adding Momentum
to Optimizers in neural networks is a concept known already and inspired
by physics. There, it counteracts areas of the loss function where it is
essentially flat and the gradient therefore close to zero.</p></div>
<div class="paragraph"><p>Sampling produces trajectories of particles moving along the manifold.
Integrals along these trajectories, if they are long enough, are
equivalent to integrating over the whole manifold, if the system is
ergodic.  And this is the key point!
Essentially, it enables us to replace integrals over the whole (and possibly
very high-dimensional domain) by a one-dimensional integral along the
trajectory. This reduces dimensional complexity significantly if not all areas
in the high-dimensional domain contribute equally to the integral, if the
contribution from many areas is negligible (think: weight $\exp(-l(x))$
in integrand with a high value for $l(x)$). All we need to do is have
the sampling produce trajectories only in the areas of interest.</p></div>
<div class="paragraph"><p>By using sampling we mean to discover more of the loss manifold than
just the closest local minimum, namely all minima. This excludes all regions
with large loss function values. In other words, we would like to sample
in such a way as only to stay in regions of the loss manifold associated
with small values. Generating trajectories by dynamics where the
negative of the gradient acts as a driving force onto each particle
automatically brings them into regions where the loss' value is small.
However, in general all possible minima locations will not form a connected
region on the loss manifold. These minima regions may be separated by barriers
which are needed to overcome. We distinguish two kinds,</p></div>
<div class="ulist"><ul>
<li>
<p>
entropic barriers,
</p>
</li>
<li>
<p>
and enthalpic barriers.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Both of which are conceptually very simple. The enthalpic barrier is
simply a ridge that is very high where the particles would need a large
momentum to overcome it. Entropic barriers on the other hand are
passages very small in volume that are simply very difficult to find. In
order to overcome barriers of the first kind, higher temperatures
suffice. For the second type of barrier, this is not so easy.
Metaphorically speaking, we are looking for a possibly very small door like
Alice in Wonderland.</p></div>
</div>
<div class="sect3">
<h4 id="_what_does_the_landscape_look_like">2.1.3. What Does the Landscape Look Like?</h4>
<div class="paragraph"><p>Let us have a closer look at a very simple loss landscape. In Figure
<a href="#quickstart.landscape.neuralnetwork">Permutation symmetry</a> we
look at a very simple network of a single input node, with a single
hidden layer containing just one node and a single output layer.
Activation function is linear everywhere. We set the output node&#8217;s and
hidden node&#8217;s bias to zero. The dataset contains two cluster of points,
one (label -1) centered at -2, another (label +1) centered at 2 which is
essentially the one given in Figure <a href="#quickstart.dataset">Dataset</a> if
projected onto x or y axis. Any product of the two degrees of freedom of the
network, namely its two weights, equal to unity will classify the data
well.</p></div>
<div class="paragraph" id="quickstart.network"><div class="title">Permutation symmetry: Neural network with permutation symmetry to provoke multiple minima</div><p><span class="image">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/neuralnetwork_permutation_symmetry.png" alt="pictures/neuralnetwork_permutation_symmetry.png" width="500">
</span></p></div>
<div class="paragraph"><p>In Figure <a href="#quickstart.landscape.loss">Loss manifold</a> we then
turn to the loss landscape depending on either weight. We see two minima
basins both of hyperbole or "banana" shape. There is a clear (enthalpic)
potential barrier in between.</p></div>
<div class="paragraph"><p>In the figure we also give a trajectory as squiggly black line. Here, we have
chosen such an (inverse) temperature value such that it is able to pass the
potential barrier and reach the other minima basin. As we have mentioned before,
higher temperature helps to overcome enthalpic barriers.</p></div>
<div class="paragraph" id="quickstart.landscape.loss"><div class="title">Loss manifold: Loss landscape with an example trajectory</div><p><span class="image">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/losslandscape_permutation_symmetry.png" alt="scaledwidth=45.0%">
</span></p></div>
<div class="paragraph"><p>This quick description of the problem of sampling in the context of
neural networks in data science should have acquainted you with some of the
physical concepts underlying the idea of sampling. It hopefully has prepared
you for the following quickstart tutorial on how to actually use
ThermodynamicAnalyticsToolkit to perform sampling.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.simulation">2.2. Using module simulation</h3>
<div class="paragraph"><p>As promised before, the first contact point in this quickstart tutorial with
TATi is the <span class="monospaced">simulation</span> module. It has been designed explicitly for
ease-of-use and to contain any functionality required for the sampling approach.
However, it can to training as well as you will see. Typically, everything is
achieved through two or three commands: One to setup TATi by handing it a dash
of options, then calling a function to <span class="monospaced">fit()</span> or <span class="monospaced">sample()</span>. In the very end
of this quickstart you will learn how to implement your first sampler using this
interface.</p></div>
<div class="paragraph"><p>If you have installed the package in the folder <span class="monospaced">/foo</span>, i.e. we have
a folder <span class="monospaced">TATi</span> with a file <span class="monospaced">simulation.py</span> residing in there, then you
probably need to add it to the <span class="monospaced">PYTHONPATH</span> as follows</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>PYTHONPATH=/foo python3</pre>
</div></div>
<div class="paragraph"><p>In this shell, you may import the sampling part of the package as
follows</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>import TATi.simulation as tati</pre>
</div></div>
<div class="paragraph"><p>This will import the <span class="monospaced">Simulation</span> interface class as the shortcut <span class="monospaced">tati</span> from
the file mentioned before. This class contains a set of convenience functions
that hides all the complexity of setting up of input pipelines and networks.
Accessing the loss function, gradients and alike or training and sampling can
be done in just a few keystrokes.</p></div>
<div class="paragraph"><p>In order to make your own python scripts executable and know about the
correct (possibly non-standard) path to ThermodynamicAnalyticsToolkit, place
the following two lines at the veryg beginning of your script:</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>import sys
sys.path.insert(1,"&lt;path_to_TATi&gt;/lib/python3.5/site-packages/")</pre>
</div></div>
<div class="paragraph"><p>where <span class="monospaced">&lt;path_to_TATi&gt;</span> needs to be replaced by your specific
installation path and <span class="monospaced">python3.5</span> needs to be replaced if you are using a
different python version. However, for the examples in this quickstart tutorial
it is not necessary if you use <span class="monospaced">PYTHONPATH</span>.</p></div>
<div class="sect3">
<h4 id="quickstart.simulation.notation">2.2.1. Notation</h4>
<div class="paragraph"><p>In the following, we will use the following notation:</p></div>
<div class="ulist"><ul>
<li>
<p>
dataset: $D = \{X,Y\}$ with features $X=\{x_d\}$ and labels
$Y=\{y_d\}$
</p>
</li>
<li>
<p>
batch of the dataset: $D_i=\{X_i, Y_i\}$
</p>
</li>
<li>
<p>
network parameters: $w=\{w_1, \ldots, w_M\}$
</p>
</li>
<li>
<p>
momenta of network parameters: $p=\{p_1, \ldots, p_M\}$
</p>
</li>
<li>
<p>
neural network function: $F_w(x)$
</p>
</li>
<li>
<p>
loss function: $L_D(w) = \sum_i l(F_w(x_i), y_i)$ with a loss
$l(x,y)$
</p>
</li>
<li>
<p>
gradients: $\nabla_w L_D(w)$
</p>
</li>
<li>
<p>
Hessians: $H_{ij} = \partial_{w_i} \partial_{w_j} L_D(w)$
</p>
</li>
</ul></div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.general">2.2.2. Instantiating TATi</h4>
<div class="paragraph"><p>The first thing in all the following example we will do is instantiate the
<span class="monospaced">tati</span> class.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
  <span style="font-style: italic"><span style="color: #9A1900"># comma-separated list of options</span></span>
<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Although it is the <span class="monospaced">simulation</span> module, we "nickname" it <span class="monospaced">tati</span> in the following
and hence will simply refer to this instance as <span class="monospaced">tati.</span></p></div>
<div class="paragraph"><p>This class takes a list of options in its construction or <span class="monospaced">__init__()</span> call.
These options inform it about the dataset to use, the specific network topology,
what sampler or optimizer to use and its parameters and so on.</p></div>
<div class="paragraph"><p>To see how this works, we will first need a dataset to work on.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">All of the examples below can also be found in the folders
<span class="monospaced">doc/userguide/python</span>, <span class="monospaced">doc/userguide/simulation</span>, and <span class="monospaced">doc/userguide/simulation/complex</span>.</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.general.help_options">Help on Options</h5>
<div class="paragraph"><p><span class="monospaced">tati</span> has quite a number of options that control its behavior. You can
request help to a specific option.
Let us inspect the help for <span class="monospaced">batch_data_files</span>:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">&gt;&gt;&gt;</span> <span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="color: #990000">&gt;&gt;&gt;</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">help</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"batch_data_files"</span><span style="color: #990000">)</span>
Option name<span style="color: #990000">:</span> batch_data_files
Description<span style="color: #990000">:</span> set of files to read input <span style="font-weight: bold"><span style="color: #000080">from</span></span>
Type       <span style="color: #990000">:</span> list of <span style="color: #990000">&lt;</span><span style="font-weight: bold"><span style="color: #0000FF">class</span></span> <span style="color: #FF0000">'str'</span><span style="color: #990000">&gt;</span>
Default    <span style="color: #990000">:</span> <span style="color: #990000">[]</span></tt></pre></div></div>
<div class="paragraph"><p>This will print a description, give the default value and expected type.</p></div>
<div class="paragraph"><p>Moreover, in case you have forgotten the name of one of the options.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">&gt;&gt;&gt;</span> <span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>simulation as tati
<span style="color: #990000">&gt;&gt;&gt;</span> tati<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">help</span></span><span style="color: #990000">()</span>
averages_file<span style="color: #990000">:</span>             CSV file name to write ensemble averages information such as average kinetic<span style="color: #990000">,</span> potential<span style="color: #990000">,</span> virial
batch_data_file_type<span style="color: #990000">:</span>      type of the files to read input <span style="font-weight: bold"><span style="color: #000080">from</span></span>
 <span style="color: #990000">&lt;</span>remainder omitted<span style="color: #990000">&gt;</span></tt></pre></div></div>
<div class="paragraph"><p>This will print a general help listing all available options.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.setup">2.2.3. Setup</h4>
<div class="paragraph"><p>In the following we will first be creating a dataset to work on. This example
code will be the most extensive one. All following ones are rather short and
straight-forward.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.writing_data">2.2.4. Preparing a dataset</h4>
<div class="paragraph"><p>Therefore, let us prepare the dataset, see the Figure <a href="#quickstart.dataset">Dataset</a>,
for our following experiments.</p></div>
<div class="paragraph"><p>At the moment, datasets are parsed from Comma Separated Values (CSV)
or Tensorflow&#8217;s own TFRecord files or can be provided in-memory from numpy
arrays. In order for the following examples on optimization and sampling to
work, we need such a data file containing features and labels.</p></div>
<div class="paragraph"><p>TATi provides a few simple dataset generators contained in the class
<span class="monospaced">ClassificationDatasets</span>.</p></div>
<div class="paragraph"><p>One option therefore is to use the TATiDatasetWriter that provides access to
<span class="monospaced">ClassificationDatasets</span>, see <a href="#quickstart.cmdline.writing_dataset">Writing a dataset</a>.
However, we can do the same using python as well. This should give you an idea
that you are not constrained to the <span class="monospaced">simulation</span> part of the Python interface,
see the reference on the general Python interface where we go through the
same examples without importing <span class="monospaced">simulation</span>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>datasets<span style="color: #990000">.</span>classificationdatasets <span style="color: #990000">\</span>
    <span style="font-weight: bold"><span style="color: #000080">import</span></span> ClassificationDatasets as DatasetGenerator

<span style="font-weight: bold"><span style="color: #000080">import</span></span> csv
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">426</span><span style="color: #990000">)</span>

dataset_generator <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">DatasetGenerator</span></span><span style="color: #990000">()</span>
xs<span style="color: #990000">,</span> ys <span style="color: #990000">=</span> dataset_generator<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">generate</span></span><span style="color: #990000">(</span>
    dimension<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    noise<span style="color: #990000">=</span><span style="color: #993399">0.01</span><span style="color: #990000">,</span>
    data_type<span style="color: #990000">=</span>dataset_generator<span style="color: #990000">.</span>TWOCLUSTERS<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># always shuffle data set is good practice</span></span>
randomize <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">arange</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #000000">len</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">))</span>
np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">shuffle</span></span><span style="color: #990000">(</span>randomize<span style="color: #990000">)</span>
xs<span style="color: #990000">[:]</span> <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">array</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">)[</span>randomize<span style="color: #990000">]</span>
ys<span style="color: #990000">[:]</span> <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">array</span></span><span style="color: #990000">(</span>ys<span style="color: #990000">)[</span>randomize<span style="color: #990000">]</span>

with <span style="font-weight: bold"><span style="color: #000000">open</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">,</span> <span style="color: #FF0000">'w'</span><span style="color: #990000">,</span> newline<span style="color: #990000">=</span><span style="color: #FF0000">''</span><span style="color: #990000">)</span> as data_file<span style="color: #990000">:</span>
    csv_writer <span style="color: #990000">=</span> csv<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">writer</span></span><span style="color: #990000">(</span>data_file<span style="color: #990000">,</span> delimiter<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
                            quotechar<span style="color: #990000">=</span><span style="color: #FF0000">'"'</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
                            quoting<span style="color: #990000">=</span>csv<span style="color: #990000">.</span>QUOTE_MINIMAL<span style="color: #990000">)</span>
    header <span style="color: #990000">=</span> <span style="color: #990000">[</span><span style="color: #FF0000">"x"</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>i<span style="color: #990000">+</span><span style="color: #993399">1</span><span style="color: #990000">)</span> <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #000000">len</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">]))]+[</span><span style="color: #FF0000">"label"</span><span style="color: #990000">]</span>
    csv_writer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">writerow</span></span><span style="color: #990000">(</span>header<span style="color: #990000">)</span>
    <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> x<span style="color: #990000">,</span> y <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">zip</span></span><span style="color: #990000">(</span>xs<span style="color: #990000">,</span> ys<span style="color: #990000">):</span>
        csv_writer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">writerow</span></span><span style="color: #990000">(</span>
            <span style="color: #990000">[</span><span style="color: #FF0000">'{:{width}.{precision}e}'</span><span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">format</span></span><span style="color: #990000">(</span>val<span style="color: #990000">,</span> width<span style="color: #990000">=</span><span style="color: #993399">8</span><span style="color: #990000">,</span>
                                              precision<span style="color: #990000">=</span><span style="color: #993399">8</span><span style="color: #990000">)</span>
             <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> val <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">list</span></span><span style="color: #990000">(</span>x<span style="color: #990000">)]</span> <span style="color: #990000">\</span>
            <span style="color: #990000">+</span> <span style="color: #990000">[</span><span style="color: #FF0000">'{}'</span><span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">format</span></span><span style="color: #990000">(</span>y<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">],</span> width<span style="color: #990000">=</span><span style="color: #993399">8</span><span style="color: #990000">,</span>
                                                precision<span style="color: #990000">=</span><span style="color: #993399">8</span><span style="color: #990000">)])</span>
    data_file<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">close</span></span><span style="color: #990000">()</span></tt></pre></div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">The labels need to be integer values. Importing will fail if they are not.</td>
</tr></table>
</div>
<div class="paragraph"><p>After importing some modules we first fix the numpy seed to 426 in order
to get the same items reproducibly. Then, we first create 500 items
using the <span class="monospaced">ClassificationDatasets</span> class from the <strong>TWOCLUSTERS</strong> dataset with
a random perturbation of relative 0.01 magnitude. We shuffle the dataset as the
generators typically create first items of one label class and then items of
the  other label class. This is not needed here as our <em>batch_size</em> will equal
the dataset size but it is good practice generally.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The class <span class="monospaced">ClassificationDatasets</span> mimicks the dataset examples that can also
be found on the <a href="https://playground.tensorflow.org/">Tensorflow playground</a>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Afterwards, we write the dataset to a simple CSV file with columns "x1", "x2",
and "label".</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Caution</div>
</td>
<td class="content">The file <span class="monospaced">dataset-twoclusters.csv</span> is used in the following examples, so keep
it around.</td>
</tr></table>
</div>
<div class="paragraph"><p>This is the very simple dataset we want to learn, sample from and exlore in the
following.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.simple_evaluation">2.2.5. Evaluating loss and gradients</h4>
<div class="paragraph"><p>Having created the dataset, we are good to go and the remaining examples are
easy and straight-forward.</p></div>
<div class="paragraph"><p>The main idea of the <span class="monospaced">simulation</span> module is to be used as a simplified
interface to access the loss and the gradients of the neural network without
having to know about the internal of the neural network. In other words, we
want to treat it as an abstract high-dimensional function, depending
implicitly on the weights and explicitly on the dataset. Moreover, we have
another abstract high-dimensional function, the loss that depends explicitly
on the weights and implicitly on the dataset, whose derivative (the gradients
with respect to the parameters) is available as a numpy array, see also the
section <a href="#quickstart.simulation.notation">Notation</a>.</p></div>
<div class="paragraph"><p>See the following example which sets up a simple fully-connected hidden network
and evaluates loss and then the associated gradients.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

<span style="font-style: italic"><span style="color: #9A1900"># prepare parameters</span></span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">10</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span>
<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># assign parameters of NN</span></span>
nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">([</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()])</span>

<span style="font-style: italic"><span style="color: #9A1900"># simply evaluate loss</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">())</span>

<span style="font-style: italic"><span style="color: #9A1900"># also evaluate gradients (from same batch)</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">())</span>


</tt></pre></div></div>
<div class="paragraph"><p>All we need to do is set some parameters&#8201;&#8212;&#8201;here, <em>batch_data_files</em> sets the
dataset file to parse, batch size of 10 and we use a linear output activation
function&#8201;&#8212;&#8201;assign all network parameters to zero and then evaluate first the
loss and then the gradients of the network.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Under the hood it is a bit more complicated: loss and gradients are inherently
connected. If <em>batch_size</em> is chosen smaller than the dataset dimension,
naive evaluation of first loss and then gradients in two separate function calls
would cause them to be evaluated on different batches. Depending on the size of
the batch, the gradients will then not belong the to the respective loss
evaluation and vice versa.
Therefore, loss, accuracy, gradients, and hessians (if <em>do_hessians</em> is True) are
cached. Only when one of them is evaluated for the second time (e.g., inside the
loop body on the next iteration), then the next batch is used. This makes sure
that calling either <span class="monospaced">loss()</span> first and then <span class="monospaced">gradients()</span> or the other way
round yields the same values connected to the same dataset batch.
In essence, just don&#8217;t worry about it!</td>
</tr></table>
</div>
<div class="paragraph"><p>As you see in the above example, <span class="monospaced">tati</span> forms the general interface class that
contains the network along with the dataset and everything in its internal
state.</p></div>
<div class="paragraph"><p>This is basically all the access you need in order to use your own optimization,
sampling, or exploration methods in the context of neural networks in a
high-level, abstract way.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.optimizing">2.2.6. Optimizing the network</h4>
<div class="paragraph"><p>Let us then start with optimizing the network, i.e. learning the data.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    learning_rate<span style="color: #990000">=</span><span style="color: #993399">3e-2</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">1000</span><span style="color: #990000">,</span>
    optimizer<span style="color: #990000">=</span><span style="color: #FF0000">"GradientDescent"</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
training_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">fit</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Train results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>training_data<span style="color: #990000">.</span>run_info<span style="color: #990000">[-</span><span style="color: #993399">10</span><span style="color: #990000">::]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>training_data<span style="color: #990000">.</span>trajectory<span style="color: #990000">[-</span><span style="color: #993399">10</span><span style="color: #990000">:]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>training_data<span style="color: #990000">.</span>averages<span style="color: #990000">[-</span><span style="color: #993399">10</span><span style="color: #990000">:]))</span></tt></pre></div></div>
<div class="paragraph"><p>Again all options are set in the init call to the interface. These options
control how the optimization is performed, what kind of network is created,
how often values are stored, and so on.</p></div>
<div class="paragraph"><p>Let us quickly go through each of the parameters:</p></div>
<div class="ulist"><ul>
<li>
<p>
<em>batch_size</em>
</p>
<div class="paragraph"><p>sets the subset size of the data set looked at per training step, if
smaller than dimension, then we add stochasticity/noise to the training
but for the advantage of smaller runtime.</p></div>
</li>
<li>
<p>
<em>learning_rate</em>
</p>
<div class="paragraph"><p>defines the scaling of the gradients in each training step, i.e. the
learning rate. Values too large may miss the minimum, values too small
need longer to reach it.</p></div>
</li>
<li>
<p>
<em>max_steps</em>
</p>
<div class="paragraph"><p>gives the amount of training steps to be performed.</p></div>
</li>
<li>
<p>
optimizer
</p>
<div class="paragraph"><p>defines the method to use for training. Here, we use Gradient Descent
(in case <em>batch_size</em> is smaller than dimension, then we actually have
Stochastic Gradient Descent).</p></div>
</li>
<li>
<p>
<em>output_activation</em>
</p>
<div class="paragraph"><p>defines the activation function of all output nodes, here it is linear.
Other choices are: tanh, relu, relu6.</p></div>
</li>
<li>
<p>
<em>seed</em>
</p>
<div class="paragraph"><p>sets the seed of the random number generator. We will still have full
randomness but in a deterministic manner, i.e. calling the same
procedure again will bring up the exactly same values.</p></div>
</li>
</ul></div>
<div class="paragraph"><p>In our case, the default option values are such that the network we use
looks exactly as in the Figure <a href="#quickstart.perceptron">Network</a>, namely
a single-layer perceptron whose number of input and output nodes are completely
fixed by the dataset. See <a href="#reference.simulation.setup.setting_up_network">Setting up the network</a>
for more details on how to specify the network topology.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">In case you need to change these options elsewhere in your python code,
use <span class="monospaced">set_options()</span>.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content"><span class="monospaced">set_options()</span> may need to reinitialize certain parts of <span class="monospaced">tati</span> s
internal state depending on what options you choose to reset. Keep in mind
that modifying the network will reinitialize all its parameters and other
possible side-effects. See <span class="monospaced">simulation._affected_map</span> in
<span class="monospaced">src/TATi/simulation.py</span> for an up-to-date list of what options affects what
part of the state.</td>
</tr></table>
</div>
<div class="paragraph"><p>For these small networks the option <em>do_hessians</em> might be useful which will
compute the hessian matrix at the end of the trajectory and use the
largest eigenvalue to compute the optimal step width. This will add
nodes to the underlying computational graph for computing the components
of the hessian matrix. However, we will not do so here.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Caution</div>
</td>
<td class="content">
<div class="paragraph"><p>The creation of these hessian evaluation nodes (not speaking of their
evaluation) is a $O(N^2)$ process in the number of parameters of the
network N. Hence, this should only be done for small networks and on purpose.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>After the options have been provided, the network is initialized internally
and automatically, we then call <span class="monospaced">fit()</span> which performs the training and returns
a structure containing runtime info, trajectory, and averages as a pandas
<span class="monospaced">DataFrame</span>.</p></div>
<div class="paragraph"><p>In the following section on <a href="#quickstart.simulation.sampling">sampling</a> we will
explain what each of these three dataframes contains exactly.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">You want more output of what is actually going on in each training step?
Set <span class="monospaced">verbose=1</span> or even <span class="monospaced">verbose=2</span> in the options when constructing <span class="monospaced">tati()</span>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let us have a quick glance at the decrease of the loss function over the steps
by using <span class="monospaced">matplotlib</span>. In other words, let us look at how effective the training
has been.</p></div>
<div class="listingblock">
<a id="quickstart.simulation.optimizing.plot_optimize"></a>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> pandas as pd
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib
<span style="font-style: italic"><span style="color: #9A1900"># use agg as backend to allow command-line use as well</span></span>
matplotlib<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">use</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"agg"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib<span style="color: #990000">.</span>pyplot as plt

df_run <span style="color: #990000">=</span> pd<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">read_csv</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"run.csv"</span><span style="color: #990000">,</span> sep<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> header<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>
run<span style="color: #990000">=</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>df_run<span style="color: #990000">.</span>loc<span style="color: #990000">[:,\</span>
   <span style="color: #990000">[</span><span style="color: #FF0000">'step'</span><span style="color: #990000">,</span><span style="color: #FF0000">'loss'</span><span style="color: #990000">,</span><span style="color: #FF0000">'kinetic_energy'</span><span style="color: #990000">,</span> <span style="color: #FF0000">'total_energy'</span><span style="color: #990000">]])</span>

plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">scatter</span></span><span style="color: #990000">(</span>run<span style="color: #990000">[:,</span><span style="color: #993399">0</span><span style="color: #990000">],</span> run<span style="color: #990000">[:,</span><span style="color: #993399">1</span><span style="color: #990000">])</span>
plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">savefig</span></span><span style="color: #990000">(</span><span style="color: #FF0000">'loss-step.png'</span><span style="color: #990000">,</span>
            bbox_inches<span style="color: #990000">=</span><span style="color: #FF0000">'tight'</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900">#plt.show()</span></span></tt></pre></div></div>
<div class="paragraph"><p>The graph should look similar to the one obtained with <span class="monospaced">pgfplots</span> here (see
<a href="https://sourceforge.net/pgfplots">https://sourceforge.net/pgfplots</a>).</p></div>
<div class="imageblock">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/optimization-step_loss.png" alt="pictures/optimization-step_loss.png" width="500">
</div>
<div class="title">Figure 6. Loss history: Behavior of the loss over the optimization run</div>
</div>
<div class="paragraph"><p>As you see the loss has decreased quite quickly down to 1e-3. Go and have a
look at the other columns such as accuracy. Or try to visualize the change
in the parameters (weights and biases) in the trajectories dataframe. See
<a href="https://pandas.pydata.org/pandas-docs/stable/10min.html">10 Minutes to pandas</a>
if you are unfamiliar with the <span class="monospaced">pandas</span> module, yet.</p></div>
<div class="paragraph"><p>Obviously, we did not use a different dataset set for testing the effectiveness
of the training which should commonly be done. This way we cannot check whether
we have overfitted or not. However, our example is trivial by design and the
network too small to be prone to overfitting this dataset.</p></div>
<div class="paragraph"><p>Nonetheless, we show how to supply a different dataset and evaluate loss and
accuracy on it.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.sampling.supply_dataset">Provide your own dataset</h5>
<div class="paragraph"><p>You can directly supply your own dataset, e.g., from a numpy array residing in
memory. See the following example where we do not generate the data but parse
them from a CSV file instead of using the pandas module.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> pandas as pd

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># e.g. parse dataset from CSV file into pandas frame</span></span>
input_dimension <span style="color: #990000">=</span> <span style="color: #993399">2</span>
output_dimension <span style="color: #990000">=</span> <span style="color: #993399">1</span>
parsed_csv <span style="color: #990000">=</span> pd<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">read_csv</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"dataset-twoclusters-test.csv"</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
                         sep<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> header<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># extract feature and label columns as numpy arrays</span></span>
features <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(\</span>
    parsed_csv<span style="color: #990000">.</span>iloc<span style="color: #990000">[:,</span> <span style="color: #993399">0</span><span style="color: #990000">:</span>input_dimension<span style="color: #990000">])</span>
labels <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(\</span>
    parsed_csv<span style="color: #990000">.</span>iloc<span style="color: #990000">[:,</span> <span style="color: #990000">\</span>
        input_dimension<span style="color: #990000">:</span>input_dimension <span style="color: #990000">\</span>
                        <span style="color: #990000">+</span> output_dimension<span style="color: #990000">])</span>

<span style="font-style: italic"><span style="color: #9A1900"># supply dataset (this creates the input layer)</span></span>
nn<span style="color: #990000">.</span>dataset <span style="color: #990000">=</span> <span style="color: #990000">[</span>features<span style="color: #990000">,</span> labels<span style="color: #990000">]</span>

<span style="font-style: italic"><span style="color: #9A1900"># this has created the network, now set</span></span>
<span style="font-style: italic"><span style="color: #9A1900"># parameters obtained from optimization run</span></span>
nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">array</span></span><span style="color: #990000">([</span><span style="color: #993399">2.42835492e-01</span><span style="color: #990000">,</span> <span style="color: #993399">2.40057245e-01</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
    <span style="color: #993399">2.66429665e-03</span><span style="color: #990000">])</span>

<span style="font-style: italic"><span style="color: #9A1900"># evaluate loss and accuracy</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Loss: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">()))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Accuracy: "</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">score</span></span><span style="color: #990000">()))</span></tt></pre></div></div>
<div class="paragraph"><p>The major difference is that <span class="monospaced">batch_data_files</span> in <span class="monospaced">tati()</span> is now empty and
instead we simply later assign <span class="monospaced">dataset</span> a numpy array to use. Note that we
could also have supplied it directly with the filename <span class="monospaced">dataset-twoclusters.csv</span>,
i.e. <span class="monospaced">nn.dataset = "dataset-twoclusters.csv"</span>.
In this example we have parsed the same file as the in the previous section
into a numpy array using the pandas module. Natually, this is just one way of
creating a suitable numpy array.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Input and output dimensions are directly deduced from the the tuple sizes.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The nodes in the input layer can be modified using <em>input_columns</em>, e.g.,
<span class="monospaced">input_columns=["x1", "sin(x2)", "x1^2"]</span>.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.sampling">2.2.7. Sampling the network</h4>
<div class="paragraph"><p>Optimization only steps down to the nearest local minimum from some initial
random starting position. Only through sampling do we actually uncover the shape
of the loss manifold and thereby are able to deduce whether our network is
efficient at doing its job.</p></div>
<div class="paragraph"><p>Nonetheless, optimization is always the initial step to sampling as we are
still generally interested in all minimum regions.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="title">Statistical background</div>
<div class="paragraph"><p>In general, when sampling from a distribution (to compute empirical averages
for example), one wants to start <em>close to equilibrium</em>, i.e. from states which
are of high probability with respect to the target distribution (therefore
the minima of the loss). The initial optimisation procedure is therefore a
first guess to find such states, or at least to get close to them. In molecular
dynamics, it is common to run sampling during an "equilibration period" in
order to let the system relax to its equilibrium. During this equilibration
time, the generated samples are not used for computing averages, as they will
introduce large statistical error.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>However, let us first ignore this good practice for a moment and simply look
at sampling from a random initial place on the loss manifold. We will come
back to it later on.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">1000</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    sampler<span style="color: #990000">=</span><span style="color: #FF0000">"GeometricLangevinAlgorithm_2ndOrder"</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
    step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span>
<span style="color: #990000">)</span>
sampling_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sample</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Sample results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>run_info<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>averages<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span></tt></pre></div></div>
<div class="paragraph"><p>Here, the <em>sampler</em> setting takes the place of the <em>optimizer</em> before as
it states which sampling scheme to use. See <a href="#reference.samplers">[reference.samplers]</a> for a
complete list and their parameter names. Apart from that the example code
is very much the same as in the example involving <span class="monospaced">fit()</span>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In the context of sampling we use <em>step_width</em> in place of <em>learning_rate</em>.</td>
</tr></table>
</div>
<div class="paragraph"><p>Again, we produce a single data structure that contains three data frames: run
info, trajectory, and averages. Trajectories contains among others all
parameter degrees of freedom for each step (or <em>every_nth</em> step). Run info
contains loss, accuracy, norm of gradient, norm of noise and others, again for
each step. Finally, in averages we compute running averages over the trajectory
such as average (ensemble) loss, average kinetic energy, average virial, see
<a href="#reference.concepts">general concepts</a>.</p></div>
<div class="paragraph"><p>Take a peep at <span class="monospaced">sampling_data.run_info.columns</span> to see all columns in the
run info dataframe (and similarly for the others.)</p></div>
<div class="paragraph"><p>For the running averages it is advisable to skip some initial ateps
(<em>burn_in_steps</em>) to allow for some burn in time, i.e. for kinetic energies to
adjust from initially zero momenta.</p></div>
<div class="paragraph"><p>Some columns in averages and in run info depend on whether the sampler
provides the specific quantity, e.g. <a href="#reference.samplers.sgld">SGLD</a> does
not have momentum, hence there will be no average kinetic energy.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.sampling.priors">Using a prior</h5>
<div class="paragraph"><p>You may add a prior to the sampling. At the current state two kinds of
priors are available: wall-repelling and tethering.</p></div>
<div class="paragraph"><p>The options <em>prior_upper_boundary</em> and <em>prior_lower_boundary</em> give the admitted
interval per parameter. Within a relative distance of 0.01 (with respect to
length of domain and only in that small region next to the specified boundary)
an additional force acts upon the particles to drives them back into the desired
domain. Its magnitude increases with distance to the covered inside the boundary
region. The distance is taken to the power of <em>prior_power</em>. The force
is scaled by <em>prior_factor</em>.</p></div>
<div class="paragraph"><p>In detail, the prior consists of an extra force added to the time integration
within each sampler. We compute its magnitude as</p></div>
<div class="mathblock">
<div class="content">
\[\Theta(\frac{||x - \pi||}{\tau}-1.) \cdot a ||x - \pi||^n\]
</div></div>
<div class="paragraph"><p>where <strong>x</strong> is the position of the particle, <strong>a</strong> is the <em>prior_factor</em>,
$\pi$ is the position of the boundary (<em>prior_upper_boundary</em>
$\pi_{ub}$ or <em>prior_lower_boundary</em> $\pi_{lb}$), and <strong>n</strong>
is the <span class="monospaced">prior_power</span>. Finally, the force is only in effect within a distance of
$\tau = 0.01 \cdot || \pi_{ub} - \pi_{lb} ||$ to either boundary by
virtue of the Heaviside function $\Theta()$.
Note that the direction of the force is such that it always points back into
the desired domain.</p></div>
<div class="paragraph"><p>If upper and lower boundary coincide, then we have the case of
tethering, where all parameters are pulled inward to the same point.</p></div>
<div class="paragraph"><p>At the moment applying prior on just a subset of particles is not supported.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The prior force is acting directly on the variables. It does not modify
momentum. Moreover, it is a force! In other words, it depends on step
width. If the step width is too large and if the repelling force
increases too steeply close to the walls with respect to the normal
dynamics of the system, it may blow up. On the other hand, if it is too weak,
then particles may even escape.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.sampling.optimize_then_sample">First optimize, then sample</h5>
<div class="paragraph"><p>As we have already alluded to before, optimizing before sampling is the
<strong>recommended</strong> procedure. In the following example, we concatenate the two.
To this end, we might need to modify some of the options in between. Let us have
a look, however with a slight twist.</p></div>
<div class="paragraph"><p>The dataset shown in Figure <a href="#quickstart.dataset">Dataset</a> can be even
learned by a simpler network: only one of the input nodes is actually
needed because of the symmetry.</p></div>
<div class="paragraph"><p>Hence, we look at such a network by using <em>input_columns</em> to only use input
column "x1" although the dataset contains both "x1" and "x2".</p></div>
<div class="paragraph"><p>Moreover, we will add a hidden layer with a single node and thus obtain a
network as depicted in Figure <a href="#quickstart.network">Permutation symmetry</a>.
We add this hidden node to make the loss manifold a little bit more
interesting.</p></div>
<div class="paragraph"><p>Additionally, we fix the biases to <strong>0.</strong> for both the hidden layer bias and the
output bias. Effectively, we have two degrees of freedom left. This is not
strictly necessary but allows to plot all degrees of freedom at once.</p></div>
<div class="paragraph"><p>Finally, we add a <a href="#quickstart.simulation.sampling.priors">prior</a>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    every_nth<span style="color: #990000">=</span><span style="color: #993399">100</span><span style="color: #990000">,</span>
    fix_parameters<span style="color: #990000">=</span><span style="color: #FF0000">"layer1/biases/Variable:0=0.;output/biases/Variable:0=0."</span><span style="color: #990000">,</span>
    hidden_dimension<span style="color: #990000">=[</span><span style="color: #993399">1</span><span style="color: #990000">],</span>
    input_columns<span style="color: #990000">=[</span><span style="color: #FF0000">"x1"</span><span style="color: #990000">],</span>
    learning_rate<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">100</span><span style="color: #990000">,</span>
    optimizer<span style="color: #990000">=</span><span style="color: #FF0000">"GradientDescent"</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    sampler <span style="color: #990000">=</span> <span style="color: #FF0000">"BAOAB"</span><span style="color: #990000">,</span>
    prior_factor<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    prior_lower_boundary<span style="color: #990000">=-</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    prior_power<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    prior_upper_boundary<span style="color: #990000">=</span><span style="color: #993399">2</span><span style="color: #990000">.,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">428</span><span style="color: #990000">,</span>
    step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span>
    trajectory_file<span style="color: #990000">=</span><span style="color: #FF0000">"trajectory.csv"</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>
training_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">fit</span></span><span style="color: #990000">()</span>

nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">set_options</span></span><span style="color: #990000">(</span>
    friction_constant <span style="color: #990000">=</span> <span style="color: #993399">10</span><span style="color: #990000">.,</span>
    inverse_temperature <span style="color: #990000">=</span> <span style="color: #990000">.</span><span style="color: #993399">2</span><span style="color: #990000">,</span>
    max_steps <span style="color: #990000">=</span> <span style="color: #993399">5000</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>

sampling_data <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sample</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Sample results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>run_info<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>sampling_data<span style="color: #990000">.</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span></tt></pre></div></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Setting <em>every_nth</em> large enough is essential when playing around with
small networks and datsets as otherwise time spent writing files and adding
values to arrays will dominate the actual neural network computations by far.</td>
</tr></table>
</div>
<div class="paragraph"><p>As you see, some more options have popped up in the <span class="monospaced">__init__()</span> of the
simulation interface: <em>hidden_dimension</em> which is a list of the number of
hidden nodes per layer, <em>input_columns</em> which contains a list of strings,
each giving the name of an input dimension (indexing starts at 1), and all
sorts of <em>prior_&#8230;</em> that define a wall-repelling prior, again see
<a id="quickstart.simulation.priors"></a> for details. This will keep parameter values
within the interval of [-2,2]. Last but not least, <em>trajectory_file</em> writes
all parameters per <em>every_nth</em> step to this file.</p></div>
<div class="paragraph"><p>Moreover, we needed to change the number of steps, set a sampling step width
and add the sampler (which might depend on additional parameters, see <a href="#reference.samplers">[reference.samplers]</a>
).
At the very end we again obtain the data structure containing the <span class="monospaced">pandas</span>
DataFrame containing runtime information, trajectory, and averages as its
member variables.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">This time we need the trajectory file for the upcoming analysis. Hence,
we write it to a file using the <em>trajectory_file</em> option. Keep the file around
as it is needed in the following.</td>
</tr></table>
</div>
<div class="paragraph"><p>Let us take a look at the two degrees of freedom of the network, namely the two
weights, where we plot one against the other similarly to the
<a href="#quickstart.simulation.optimizing.plot_optimize">Sampled weights</a> before.</p></div>
<div class="imageblock" id="quickstart.simulation.analysis.optimize_sample.weights" style="text-align:center;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/weights.png" alt="pictures/weights.png" width="400">
</div>
<div class="title">Figure 7. Sampled weights: Plot of first against second weight.</div>
</div>
<div class="paragraph"><p>First of all, take note that the prior (given <em>prior_force</em> is strong enough
with respect to the chosen <em>inverse_temperature</em>) indeed retains both parameters
within the interval [-2,2] as requested.</p></div>
<div class="paragraph"><p>Compare this to the Figure <a href="#quickstart.landscape.loss">Loss manifold</a>. You
will notice that this trajectory (due to the large enough temperature) has also
jumped over the ridge around the origin.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">To bound the runtime of this example, we have set the parameters such that
we obtain a good example of a barrier-jumping trajectory. The original values
from the introduction are obtained when you reduce the <em>inverse_temperature</em>
to <strong>4.</strong> and increase <em>max_steps</em> to <strong>20000</strong> (or even more) if you do not mind
waiting a minute or two for the sampling to execute.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.analysis">2.2.8. Analysing trajectories</h4>
<div class="paragraph"><p>Analysis involves parsing in run and trajectory files that you would write
through optimization and sampling runs. Naturally, you could also perform this
on the <span class="monospaced">pandas</span> dataframes directly. However, for completeness we will read
from files in the examples of this section.</p></div>
<div class="paragraph"><p>To this end, specify <span class="monospaced">FLAGS.run_file</span> and <span class="monospaced">FLAGS.trajectory_file</span> with
some valid file names.</p></div>
<div class="sect4">
<h5 id="quickstart.simulation.analysis.averages">Averages</h5>
<div class="paragraph"><p>Subsequently, these may be easily parsed as follows, see also
<span class="monospaced">tools/TATiAnalyser.in</span> in the repository.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> pandas as pd
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib
<span style="font-style: italic"><span style="color: #9A1900"># use agg as backend to allow command-line use as well</span></span>
matplotlib<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">use</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"agg"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #000080">import</span></span> matplotlib<span style="color: #990000">.</span>pyplot as plt

df_trajectory <span style="color: #990000">=</span> pd<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">read_csv</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"trajectory.csv"</span><span style="color: #990000">,</span> sep<span style="color: #990000">=</span><span style="color: #FF0000">','</span><span style="color: #990000">,</span> <span style="color: #990000">\</span>
    header<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>
traj<span style="color: #990000">=</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>df_trajectory<span style="color: #990000">)</span>

conv<span style="color: #990000">=</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">(</span>traj<span style="color: #990000">.</span>shape<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># then we plot the running averages of the parameters</span></span>
<span style="font-style: italic"><span style="color: #9A1900"># inside weights</span></span>
<span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="color: #993399">1</span><span style="color: #990000">,</span>traj<span style="color: #990000">.</span>shape<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">]):</span>
    <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> d <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span>traj<span style="color: #990000">.</span>shape<span style="color: #990000">[</span><span style="color: #993399">1</span><span style="color: #990000">]):</span>

        conv<span style="color: #990000">[</span>i<span style="color: #990000">,</span>d<span style="color: #990000">]=</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">mean</span></span><span style="color: #990000">(</span>traj<span style="color: #990000">[:</span>i<span style="color: #990000">,</span>d<span style="color: #990000">])</span>

<span style="color: #990000">[</span>plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">scatter</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #000000">len</span></span><span style="color: #990000">(</span>traj<span style="color: #990000">)),</span> conv<span style="color: #990000">[:,</span>i<span style="color: #990000">])</span> <span style="color: #990000">\</span>
    <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span>traj<span style="color: #990000">.</span>shape<span style="color: #990000">[</span><span style="color: #993399">1</span><span style="color: #990000">])]</span>
plt<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">savefig</span></span><span style="color: #990000">(</span><span style="color: #FF0000">'step-parameters.png'</span><span style="color: #990000">,</span> bbox_inches<span style="color: #990000">=</span><span style="color: #FF0000">'tight'</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900">#plt.show()</span></span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>conv<span style="color: #990000">[-</span><span style="color: #993399">1</span><span style="color: #990000">,:])</span></tt></pre></div></div>
<div class="paragraph"><p>This would give a plot of the running average for each parameter in the
trajectory file. In a similar, the run file can be loaded and its
average quantities such as loss or kinetic energy be analysed and
plotted.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.simulation.analysis.diffusion_map">Diffusion Map</h5>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The former text was not agreed upon by the whole team and is therefore withdrawn at the moment.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="quickstart.simulation.exploration">2.2.9. Exploring the loss manifold</h4>
<div class="paragraph"><p>Exploration of the loss manifold is a bit more involved and hence uses a
different part of the Python interface. We do not use the simulation interface
anymore but the general Python interface as we require greater access to its
internals.</p></div>
<div class="paragraph"><p>In general, the procedure has the following stages:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
<a id="step1"></a> We sample a few starting trajectory.
</p>
</li>
<li>
<p>
<a id="step2"></a> For the current set of trajectory points we perform a diffusion map
analysis. Using the first eigenvector as the dominant diffusion mode, we
pick the first corner points at its maximal component.
</p>
</li>
<li>
<p>
<a id="step3"></a> If more corner points are needed, then we look at the diffusion
distance with respect to already picked corner points over all
eigenvectors of the diffusion map and pick the next point always such
that it maximizes the diffusion distance to the present ones.
</p>
</li>
<li>
<p>
<a id="step4"></a> Finally, we sample further trajectories, one starting at each of the
picked corner points.
</p>
</li>
<li>
<p>
<a id="step5"></a> This is repeated (go to <a href="#step2">second step</a>) for as many
exploration steps as we want to do.
</p>
</li>
</ol></div>
<div class="paragraph"><p>Note that each single trajectory is sampled in a special way:</p></div>
<div class="ulist"><ul>
<li>
<p>
First, three legs of sampling are performed
</p>
</li>
<li>
<p>
Then, we analyse the resulting diffusion map.
</p>
</li>
<li>
<p>
If the eigenvalues have not yet converged with respect to some
relative threshold, we continue for one more leg and analyse again after
that
</p>
</li>
<li>
<p>
If they have converged, we stop.
</p>
</li>
<li>
<p>
Finally, we look at the norm of the gradients along the trajectory. If
it is below a certain threshold, then within this section of the
trajectory (with gradient norms beneath the threshold) we pick the
smallest gradient value as the trajectory step being a possible minimum
candidate.
</p>
</li>
<li>
<p>
For all minimum candidates (if any) we run additional optimization
trajectories, e.g. using GradientDescent, to find a local minima.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Have a look at the following example.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">This example code is very much involved. Do not worry if you do not
understand what is going on right away. This may become a lot simpler in future
versions of TATi.</td>
</tr></table>
</div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>models<span style="color: #990000">.</span>model <span style="font-weight: bold"><span style="color: #000080">import</span></span> model
<span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>exploration<span style="color: #990000">.</span>explorer <span style="font-weight: bold"><span style="color: #000080">import</span></span> Explorer

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

FLAGS <span style="color: #990000">=</span> model<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">setup_parameters</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
    batch_size<span style="color: #990000">=</span><span style="color: #993399">500</span><span style="color: #990000">,</span>
    diffusion_map_method<span style="color: #990000">=</span><span style="color: #FF0000">"vanilla"</span><span style="color: #990000">,</span>
    learning_rate<span style="color: #990000">=</span><span style="color: #993399">3e-2</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">10</span><span style="color: #990000">,</span>
    number_of_eigenvalues<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">,</span>
    optimizer<span style="color: #990000">=</span><span style="color: #FF0000">"GradientDescent"</span><span style="color: #990000">,</span>
    output_activation<span style="color: #990000">=</span><span style="color: #FF0000">"linear"</span><span style="color: #990000">,</span>
    sampler<span style="color: #990000">=</span><span style="color: #FF0000">"BAOAB"</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
    step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span>
    use_reweighting<span style="color: #990000">=</span>False
<span style="color: #990000">)</span>
nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">model</span></span><span style="color: #990000">(</span>FLAGS<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># init both sample and train right away</span></span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_network</span></span><span style="color: #990000">(</span>None<span style="color: #990000">,</span> setup<span style="color: #990000">=</span><span style="color: #FF0000">"sample"</span><span style="color: #990000">)</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_network</span></span><span style="color: #990000">(</span>None<span style="color: #990000">,</span> setup<span style="color: #990000">=</span><span style="color: #FF0000">"train"</span><span style="color: #990000">)</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_input_pipeline</span></span><span style="color: #990000">()</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">reset_dataset</span></span><span style="color: #990000">()</span>

explorer <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">Explorer</span></span><span style="color: #990000">(</span>parameters<span style="color: #990000">=</span>FLAGS<span style="color: #990000">,</span> max_legs<span style="color: #990000">=</span><span style="color: #993399">5</span><span style="color: #990000">,</span> number_pruning<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">)</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Creating starting trajectory."</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># a. add three legs to queue</span></span>
explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">spawn_starting_trajectory</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># b. continue until queue has run dry</span></span>
explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run_all_jobs</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">,</span> FLAGS<span style="color: #990000">)</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Starting multiple explorations from starting trajectory."</span><span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># 2. with the initial trajectory done and analyzed,</span></span>
<span style="font-style: italic"><span style="color: #9A1900">#    find maximally separate points and sample from these</span></span>
max_exploration_steps <span style="color: #990000">=</span> <span style="color: #993399">2</span>
exploration_step <span style="color: #990000">=</span> <span style="color: #993399">1</span>
<span style="font-weight: bold"><span style="color: #0000FF">while</span></span> exploration_step <span style="color: #990000">&lt;</span> max_exploration_steps<span style="color: #990000">:</span>
    <span style="font-style: italic"><span style="color: #9A1900"># a. combine all trajectories</span></span>
    steps<span style="color: #990000">,</span> parameters<span style="color: #990000">,</span> losses <span style="color: #990000">=</span> <span style="color: #990000">\</span>
        explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">combine_sampled_trajectories</span></span><span style="color: #990000">()</span>
    <span style="font-style: italic"><span style="color: #9A1900"># b. perform diffusion map analysis for eigenvectors</span></span>
    idx_corner <span style="color: #990000">=</span> <span style="color: #990000">\</span>
        explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_corner_points</span></span><span style="color: #990000">(</span>parameters<span style="color: #990000">,</span> losses<span style="color: #990000">,</span> <span style="color: #990000">\</span>
                                   FLAGS<span style="color: #990000">,</span> <span style="color: #990000">\</span>
                                   number_of_corner_points<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">)</span>
    <span style="font-style: italic"><span style="color: #9A1900"># d. spawn new trajectories from these points</span></span>
    explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">spawn_corner_trajectories</span></span><span style="color: #990000">(</span>steps<span style="color: #990000">,</span> parameters<span style="color: #990000">,</span> losses<span style="color: #990000">,</span>
                                       idx_corner<span style="color: #990000">,</span> nn<span style="color: #990000">)</span>
    <span style="font-style: italic"><span style="color: #9A1900"># d. run all trajectories till terminated</span></span>
    explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run_all_jobs</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">,</span> FLAGS<span style="color: #990000">)</span>

    exploration_step <span style="color: #990000">+=</span> <span style="color: #993399">1</span>


run_info<span style="color: #990000">,</span> trajectory <span style="color: #990000">=</span> explorer<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_run_info_and_trajectory</span></span><span style="color: #990000">()</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Exploration results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>run_info<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">10</span><span style="color: #990000">]))</span></tt></pre></div></div>
<div class="paragraph"><p>This performs exactly the procedure described before using very, very
short trajectories (<em>max_steps</em>), only a few legs (<em>max_legs</em>) and only
a very limited number of exploration steps (<em>exploration_steps</em>). This is
simple for the purpose of illustration. Naturally, larger values for all these
parameters are required in order to explore complex manifolds and eventually
find the global minimum.</p></div>
<div class="paragraph"><p>Note the calls to <span class="monospaced">run_all_jobs()</span>: What is happening behind the scenes is that
the class <span class="monospaced">Explorer</span> contains a queue. Sampling a single leg of a trajectory
is encoded as a single job, similarly performing a diffusion map analysis and so
on. All these jobs are placed in the queue. <span class="monospaced">run_all_jobs()</span> will launch the
jobs one after the other, or even in parallel if <em>number_processes</em> is larger
than <strong>1</strong>.</p></div>
</div>
<div class="sect3">
<h4 id="_conclusion">2.2.10. Conclusion</h4>
<div class="paragraph"><p>This has been the quickstart introduction to the <span class="monospaced">simulation</span> interface.</p></div>
<div class="paragraph"><p>If you want to take this further, we recommend reading how to implement a
<a href="#reference.implementing_sampler">GLA2 sampler</a> using this module.</p></div>
<div class="paragraph"><p>If you still want to take it further, then you need to look at the
 <a href="programmersguide.html">programmer&#8217;s guide</a> 
that should accompany your installation.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.cmdline">2.3. Using command-line interface</h3>
<div class="paragraph"><p>All the tests use the command-line interface and for performing rigorous
scientific experiments, we recommend using this interface as well. Here,
it is to do parameter studies and have extensive runs using different
seeds.</p></div>
<div class="sect3">
<h4 id="quickstart.cmdline.writing_dataset">2.3.1. Creating the dataset</h4>
<div class="paragraph"><p>As data is read from file, this file needs to be created beforehand.</p></div>
<div class="paragraph"><p>For a certain set of simple classification problems, namely those that
can be found in the tensorflow playground, we have added a
TATiDatasetWriter` that spills out the dataset in CSV format.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiDatasetWriter <span style="color: #990000">\</span>
        --data_type <span style="color: #993399">2</span> <span style="color: #990000">\</span>
        --dimension <span style="color: #993399">500</span> <span style="color: #990000">\</span>
        --noise <span style="color: #993399">0.1</span> <span style="color: #990000">\</span>
        --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
        --train_test_ratio <span style="color: #993399">0</span> <span style="color: #990000">\</span>
        --test_data_file testset-twoclusters<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will write 500 datums of the dataset type 2 ("two clusters") to a
file &#8220;testset-twoclusters.csv&#8221; using all of the points as we have set
the test/train ratio to 0. Note that we also perturb the points by 0.1
relative noise.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.parsing_dataset">2.3.2. Parsing the dataset</h4>
<div class="paragraph"><p>Similarly, for testing the dataset can be parsed using the same
tensorflow machinery as is done for sampling and optimizing, using</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiDatasetParser <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span></tt></pre></div></div>
<div class="paragraph"><p>where the <em>seed</em> is used for shuffling the dataset.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.optimizing">2.3.3. Optimizing the network</h4>
<div class="paragraph"><p>As weights (and biases) are usually uniformly random initialized and the
potential may therefore start with large values, we first have to
optimize the network, using (Stochastic) Gradient Descent (GD).</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiOptimizer <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">50</span> <span style="color: #990000">\</span>
    --loss mean_squared <span style="color: #990000">\</span>
        --learning_rate <span style="color: #993399">1e-2</span> <span style="color: #990000">\</span>
    --max_steps <span style="color: #993399">1000</span> <span style="color: #990000">\</span>
    --optimizer GradientDescent <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --save_model `pwd`/model<span style="color: #990000">.</span>ckpt<span style="color: #990000">.</span>meta <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
    -v</tt></pre></div></div>
<div class="paragraph"><p>This call will parse the dataset from the file
"dataset-twoclusters.csv". It will then perform a (Stochastic) Gradient
Descent optimization in batches of 50 (10% of the dataset) of the
parameters of the network using a step width/learning rate of 0.01 and
do this for 1000 steps after which it stops and writes the resulting
neural network in a TensorFlow-specific format to a set of files, one of
which is called <span class="monospaced">model.ckpt.meta</span> (and the other filenames are derived
from this).</p></div>
<div class="paragraph"><p>We have also created a file <span class="monospaced">run.csv</span> which contains among others the
loss at each (&#8220;every_nth&#8221;, respectively) step of the optimization run.
Plotting the loss over the step column from the run file will result in
a figure similar to in
<a href="#quickstart.simulation.optimizing.plot">Loss history</a>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Since Tensorflow 1.4 an absolute path is required for the storing the
model. In the example we use the current directory returned by the unix
command <span class="monospaced">pwd</span>.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>If you need to compute the optimal step width, which is possible for
smaller networks from the largest eigenvalue of the hessian matrix, then
use the option &#8220;do_hessians 1&#8221; to activate it.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The creation of the nodes is costly, $O(N^2)$ in the number of
parameters of the network N. Hence, may not work for anything but small
networks and should be done on purpose.</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>In case you have read the quickstart tutorial on the Python interface
before, then the names of the command-line option will probably remind
you of the variables in the FLAGS structure.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.sampling">2.3.4. Sampling trajectories on the loss manifold</h4>
<div class="paragraph"><p>We continue from this optimized or equilibrated state with sampling. It
is called equilibrated as the network&#8217;s parameter should now be close to
a (local) minimum of the potential function and hence in equilibrium.
This means that small changes to the parameters will result in gradients
that force it back into the minimum.</p></div>
<div class="paragraph"><p>Let us call the sampler.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiSampler <span style="color: #990000">\</span>
    --averages_file averages<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">50</span> <span style="color: #990000">\</span>
    --friction_constant <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --inverse_temperature <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --loss mean_squared <span style="color: #990000">\</span>
    --max_steps <span style="color: #993399">1000</span> <span style="color: #990000">\</span>
    --sampler GeometricLangevinAlgorithm_2ndOrder <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
    --step_width <span style="color: #993399">1e-2</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will cause the sampler to parse the same dataset as before.
Moreover, the sampler will load the neural network from the model, i.e.
using the optimized parameters right from the start. Afterwards it will
use the GLA in 2nd order discetization using again <em>step_width</em> of 0.01
and running for 1000 steps in total. The GLA is a descretized variant of
Langevin Dynamics whose accuracy scales with the inverse square of the
<em>step_width</em> (hence, 2nd order).</p></div>
<div class="paragraph"><p>The seed is needed as we sample using Langevin Dynamics where a noise
term is present. The term basically ascertains a specific temperature
which is proportional to the average momentum of each particle.</p></div>
<div class="paragraph"><p>After it has finished, it will create three files; a run file
<span class="monospaced">run.csv</span> containing run time information such as the step, the
potential, kinetic and total energy at each step, a trajectory file
<span class="monospaced">trajectory.csv</span> with each parameter of the neural network at each step,
and an averages file <span class="monospaced">averages.csv</span> containing averages accumulated
along the trajectory such as average kinetic energy, average virial (
connected to the kinetic energy through the virial theorem, valid if a
prior keeps parameters bound to finite values), and the average
(ensemble) loss. Moreover, for the HMC sampler the average rejection
rate is stored there. The first two files we need in the next stage.</p></div>
</div>
<div class="sect3">
<h4 id="quickstart.cmdline.analysing">2.3.5. Analysing trajectories</h4>
<div class="paragraph"><p>Eventually, we now perform the diffusion map analysis on the obtained
trajectories. The trajectory file written in the last step is simply a
matrix of dimension (number of parameters) times (number of trajectory
steps). The eigenvector to the largest (but one) eigenvalue will give
the dominant direction in which the trajectory is moving.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The largest eigenvalue is usually unity and its eigenvector is constant.
Therefore, it is omitted. That&#8217;s why indexing for the diffusion maps
eigenvectors starts at 1 (omitted the constant eigenvector 0).</p></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>The analysis can perform three different tasks:</p></div>
<div class="ulist"><ul>
<li>
<p>
Calculating averages.
</p>
</li>
<li>
<p>
Calculating the diffusion map&#8217;s largest eigenvalues and eigenvectors.
</p>
</li>
<li>
<p>
Calculating landmarks and level sets to obtain an approximation to the
free energy.
</p>
</li>
</ul></div>
<div class="sect4">
<h5 id="quickstart.cmdline.analysing.averages">Averages</h5>
<div class="paragraph"><p>Averages are calculated by specifying two options as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiAnalyser <span style="color: #990000">\</span>
    --average_run_file average_run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --average_trajectory_file average_trajectory<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --drop_burnin <span style="color: #993399">100</span> <span style="color: #990000">\</span>
    --every_nth <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --steps <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will load both the run file <span class="monospaced">run.csv</span> and the trajectory file
`trajectory.csv`and average over them using only every 10th data point
(<em>every_nth</em>) and also dropping the first steps below 100
(<em>drop_burnin</em>). It will produce then ten averages (<em>steps</em>) for each of
energies in the run file and each of the parameters in the trajectories
file (along with the variance) from the first non-dropped step till one
of the ten end steps. These end steps are obtained by equidistantly
splitting up the whole step interval.</p></div>
<div class="paragraph"><p>Eventually, we have two output file. The averages over the run
information such as total, kinetic, and potential energy in
<span class="monospaced">average_run.csv</span>. Also, we have the averages over the degrees of
freedom in <span class="monospaced">average_trajectories.csv</span>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>Averages depend crucially on the number of steps we average over. I.e.
the more points we throw away, the less accurate it becomes. In other
words, if large accuracy is required, the averages file (if it contains
the value of interest) is a better place to look for.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="quickstart.cmdline.analysing.diffusion_map">Diffusion map</h5>
<div class="paragraph"><p>The eigenvalues and eigenvectors can be written as well to two output
files.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiAnalyser <span style="color: #990000">\</span>
    --diffusion_map_file diffusion_map_values<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --diffusion_map_method vanilla <span style="color: #990000">\</span>
    --diffusion_matrix_file diffusion_map_vectors<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --drop_burnin <span style="color: #993399">100</span> <span style="color: #990000">\</span>
    --every_nth <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --inverse_temperatur <span style="color: #993399">1e4</span> <span style="color: #990000">\</span>
    --number_of_eigenvalues <span style="color: #993399">4</span> <span style="color: #990000">\</span>
    --steps <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>The files ending in <span class="monospaced">..values.csv</span> contains the eigenvalues in two
columns, the first is the eigenvalue index, the second is the
eigenvalue.</p></div>
<div class="paragraph"><p>The other file ending in <span class="monospaced">..vectors.csv</span> is simply a matrix of the
eigenvector components in one direction and the trajectory steps in the
other. Additionally, it contains the parameters at the steps and also
the loss and the kernel matrix entry.</p></div>
<div class="paragraph"><p>Note that again the all values up till step 100 are dropped and only
every 10th trajectory point is considered afterwards.</p></div>
<div class="paragraph"><p>There are two methods available. Here, we have used the simpler (and
less accurate) (plain old) vanilla method. The other is called TMDMap.</p></div>
<div class="paragraph"><p>If you have installed the <em>pydiffmap</em> python package, this mal also be
specified as diffusion map method. It has the benefit of an interal
optimal parameter choice. Hence, it should behave more robustly than the
other two methods. TMDMap is different only in reweighting tre samples
according to the specific temperature.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.cmdline.analysing.free_energy">Free energy</h5>
<div class="paragraph"><p>Last but not least, the free energy is calculated.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiAnalyser <span style="color: #990000">\</span>
    --diffusion_map_method TMDMap <span style="color: #990000">\</span>
    --drop_burnin <span style="color: #993399">100</span> <span style="color: #990000">\</span>
    --every_nth <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --inverse_temperature <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --landmarks <span style="color: #993399">5</span> <span style="color: #990000">\</span>
    --landmark_file landmarks-ev_1<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --number_of_eigenvalues <span style="color: #993399">2</span> <span style="color: #990000">\</span>
    --steps <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>This will extract landmark points from the trajectory. Basically, the
loss manifold is discretized using these landmarks where all
configurations close to a landmark step are combined onto a so-called
level-set, i.e. all these configurations have a similar loss function
value. By knowing the number of configurations in each level set and
knowing the level sets loss value, an approximation of the free energy
is computed.</p></div>
<div class="paragraph"><p>This is computed for every step of the trajectory and it is insightful
to look at the free energy over the course of the trajectory represented
by the first eigenvalue. If in this graph clear minima with maxima in
between can be seen, then there are enthalpic barriers between two local
minima. If on the other hand there are flat areas, then we found
entropic barriers.</p></div>
<div class="paragraph"><p>Both these types of barriers obstruct trajectories and keep the
optimization trapped in so-called meta-stable states. Each type of
barrier requires a different type of remedy to overcome.</p></div>
</div>
<div class="sect4">
<h5 id="quickstart.cmdline.exploration">Exploring the loss manifold</h5>
<div class="paragraph"><p>Eventually, we are not interested in obtaining trajectories on the loss
manifold. Instead we would like to find the global minima. Or at least
have a good idea about whether the minimas we have found so far are
reasonable.</p></div>
<div class="paragraph"><p>To this end, a command-line tool called <span class="monospaced">TATiExplorer</span> is provided. The
idea is to make use of the diffusion map with its diffusion distance to
assess what part of the loss manifold has been explored already.
Moreover, we use multiple trajectories that are spawned from a specific
number of places that are maximally separate with respect to their
diffusion distance. This will ensure that we cover the most ground
possible.</p></div>
<div class="paragraph"><p>In the end, the eigenvectors obtained through a run using the
<span class="monospaced">TATiExplorer</span> will return the dominant diffusion directions and
therefore those pointing in the direction along the minima, i.e. where
the sampling usually gets stuck and remains for a while, hence diffusion
is slow.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiExplorer <span style="color: #990000">\</span>
        --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">50</span> <span style="color: #990000">\</span>
    --diffusion_map_method vanilla <span style="color: #990000">\</span>
    --friction_constant <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --inverse_temperature <span style="color: #993399">10</span> <span style="color: #990000">\</span>
        --learning_rate <span style="color: #993399">3e-2</span> <span style="color: #990000">\</span>
    --loss mean_squared <span style="color: #990000">\</span>
    --max_exploration_steps <span style="color: #993399">2</span> <span style="color: #990000">\</span>
    --max_legs <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --max_steps <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --number_of_eigenvalues <span style="color: #993399">1</span> <span style="color: #990000">\</span>
    --number_of_parallel_trajectories <span style="color: #993399">1</span> <span style="color: #990000">\</span>
    --number_pruning <span style="color: #993399">0</span> <span style="color: #990000">\</span>
    --optimizer GradientDescent <span style="color: #990000">\</span>
    --sampler GeometricLangevinAlgorithm_2ndOrder <span style="color: #990000">\</span>
    --run_file run<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --seed <span style="color: #993399">426</span> <span style="color: #990000">\</span>
    --step_width <span style="color: #993399">1e-2</span> <span style="color: #990000">\</span>
    --trajectory_file trajectory<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --use_reweighting <span style="color: #993399">0</span></tt></pre></div></div>
<div class="paragraph"><p>In the example we call the explorer utility in much the same way as we
have called the sampler. There are some additional options that give the
number of eigenvalues to calculate and which diffusion map method to
use. Note that <span class="monospaced">max_steps</span> now gives the number of steps of a single
leg. Further down you find what a lag actually is.</p></div>
<div class="paragraph"><p>Furthermore, there are two options unique to the explorer. This is
<span class="monospaced">max_legs</span> which gives the maximum number of legs to look at. Each leg
goes over max_steps. After that a diffusion map analysis is performed
that checks whether the eigenvalues have converged already. If yes, the
trajectory is ended, if not we continue with a new leg (of max_steps
steps). If no convergence should occur, max_legs gives the maximum
number of legs after which the trajectory is terminated regardlessly.</p></div>
<div class="paragraph"><p>Finally, we run multiple trajectories in parallel from starting points
that are maximally apart from each other in the sense of the diffusion
distances. This is controlled by <span class="monospaced">number_of_parallel_trajectories</span>.</p></div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.parallelization">2.4. A note on parallelization</h3>
<div class="paragraph"><p>Internally, Tensorflow uses a computational graph to represent all
operations. Nodes in the graph represent computations and their results
and edges represent dependencies between these values, i.e. some may act
as input to operations resulting in certain output.</p></div>
<div class="paragraph"><p>Because of this internal representation Tensorflow has two kind of
parallelisms:</p></div>
<div class="ulist"><ul>
<li>
<p>
inter ops
</p>
</li>
<li>
<p>
intra ops
</p>
</li>
</ul></div>
<div class="paragraph"><p>Each is connected to its its own thread pool. Both the command-line and
the Python interface let you pick the number of threads per pool. If 0
is stated (default), then the number of threads is picked automatically.</p></div>
<div class="paragraph"><p>In general, &#8220;inter_ops_threads''refers to multiple cores performing
matrix multiplication or reduction operations together.
``intra_ops_threads&#8221; seems to be connected to executing multiple nodes
in parallel that are independent of each other but this is guessing at
the moment.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Warning</div>
</td>
<td class="content">
<div class="paragraph"><p>When setting <span class="monospaced">inter_ops_threads</span> <em>unequal</em> to 1, then subsequent runs
may produce different results, i.e. results are no longer strictly
reproducible. According to Tensorflow this is because reduction
operations such as <span class="monospaced">reduce_sum</span> run non-deterministically on multiple
cores for sake of speed.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect2">
<h3 id="quickstart.conclusion">2.5. Conclusion</h3>
<div class="paragraph"><p>This has been the very quick introduction into samping done on neural
network&#8217;s loss function manifolds. You have to take it from here.</p></div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="reference">3. The reference</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="reference.concepts">3.1. General concepts</h3>
<div class="paragraph"><p>Before we dive into the internals of this program suite, let us first
introduce some general underlying concepts assuming that the reader is
only roughly familiar with them. This is not meant as a replacement for
the study of more in-depth material but should rather be seen as a
reminder of the terms and notation that will appear later on.</p></div>
<div class="ulist"><ul>
<li>
<p>
Dataset
</p>
<div class="paragraph"><p>The dataset contains a fixed number of datums of input tuples and output
tuples. They are typically referred to as <em>features</em> and <em>labels</em> in
the machine learning community. Basically, they are samples taken from
the unknown function which we wish to approximate using the neural
network. If the output tuples are binary in each component, the
approximation problem is called a <em>classification</em> problem. Otherwise,
it is a <em>regression</em> problem.</p></div>
</li>
<li>
<p>
Neural network
</p>
<div class="paragraph"><p>The neural network is a black-box representing a certain set of general
functions that are efficient in solving classification problems (among
others). They are parametrized explicitly using weights and biases and
implicitly through the topology of the network (connections of nodes
residing in layers) and the activation functions used. Moreover, the
loss function determines the best set of parameters for a given task.</p></div>
</li>
<li>
<p>
Loss
</p>
<div class="paragraph"><p>The default is <em>mean_squared</em>.</p></div>
</li>
</ul></div>
<div class="paragraph"><p>The loss function determines for a given (labeled) dataset what set of
neural network&#8217;s parameters are best. Note that there are losses that do
not require labels though. Different losses result in different set of
parameters. It is a high-dimensional manifold that we want to learn and
capture using the neural network. It implicitly depends on the given
dataset and explicitly on the parameters of the neural network, namely
weights and biases. Dual to the loss function is the network&#8217;s output
that explicitly depends on the dataset&#8217;s current datum (fed into the
network) and implicitly on the parameters.</p></div>
<div class="paragraph"><p>+
Most important to understand about the loss is that it is a <em>non-convex</em>
function and therefore in general does not just have a single minimum.
This makes the task of finding a good set of parameters that (globally)
minimize the loss difficult as one would have to find each and every
minima in this high-dimensional manifold and check whether it is
actually the global one.</p></div>
<div class="ulist"><ul>
<li>
<p>
Momenta and kinetic energy
</p>
<div class="paragraph"><p>Momenta is a concept taken over from physics where the parameters are
considered as particles each in a one-dimensional space where the loss
is a potential function whose ( negative) gradient acts as a force onto
the particle driving them down-hill (towards the local minimum). This
force is integrated in a classical Newton&#8217;s mechanic style, i.e.
Newton&#8217;s equation of motion is discretized with small time steps
(similar to the learning rate in Gradient Descent). This gives first
rise to/velocity and second to momenta, i.e. second order ordinary
differential equation (ODE) split up into a system of two
one-dimensional ODEs. There are numerous stable time integrators, i.e.
velocity Verlet/leapfrog, that are employed to propagate both particle
position (i.e. the parameter value) and its momentum through time. Note
that momentum and velocity are actually equivalent as usually the mass
is set to unity.</p></div>
<div class="paragraph"><p>The kinetic energy is computed as sum over kinetic energies of each parameter.</p></div>
</li>
<li>
<p>
Virials
</p>
<div class="paragraph"><p>Virials are defined as one half of the sum over the scalar product of
gradients with parameters, see <a href="https://en.wikipedia.org/wiki/Virial_theorem">https://en.wikipedia.org/wiki/Virial_theorem</a>.</p></div>
</li>
<li>
<p>
Optimizers
</p>
<div class="paragraph"><p>Optimizers are used to drive the parameters to the local minimum from a
given (random) starting position. <a href="#GD">[GD]</a> is best known,
but there are more elaborate Optimizers that use the concept of momentum
as well. This helps in overcoming flat parts of the manifold where the
gradient is effectively zero but momentum still drives the particles
towards the minimum.</p></div>
</li>
<li>
<p>
Samplers
</p>
<div class="paragraph"><p>The goal of samplers is different than the goal of optimizers. Samplers
such as <a href="#GLA">[GLA]</a> aim at discovering a great deal of the manifold, not constraint
to the local minimum. Usually, they are started from the local minimum and
drive the particles further and further out until new minima are found
between which potential barriers had to be overcome.</p></div>
</li>
</ul></div>
<div class="sect3">
<h4 id="reference.concepts.neural_networks">3.1.1. Neural Networks</h4>
<div class="paragraph"><p>A neural network (NN) is a tool used in the context of machine learning.
Formally, it is a graph with nodes and edges, where nodes represent
(simple) functions. The edges represent scalar values by which the
output of one node is scaled as input to another node. The scalar value
is called <em>weight</em> and each node also has a constant value, the <em>bias</em>,
that does not depend on the input of other nodes. Nodes are organized in
layers and nodes are (mostly) only connected between adjacent layer.
Special are the very first layer with input nodes that simply accept
input from the user and the very last layer whose output is eventually
all that matters.</p></div>
<div class="paragraph"><p>Typically, a NN might be used for the task of classification: Data is
fed into the network&#8217;s input layer and its output layer has nodes equal
to the number of classes to be distinguished. This can for example be
used for image classification.</p></div>
<div class="paragraph"><p>The essential task at hand is to determine a good set of parameters,
i.e. values for the weights and biases, such that the task is performed
best with respect to some measure.</p></div>
</div>
<div class="sect3">
<h4 id="reference.concepts.loss">3.1.2. The loss function</h4>
<div class="paragraph"><p>At the moment, there are two little utility programs that help in
evaluating the loss function given a certain dataset, namely the
<span class="monospaced">TATiLossFunctionSampler</span>. Let us give an example call right away.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
        --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
        --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --parse_parameters_file trajectory<span style="color: #990000">.</span>csv</tt></pre></div></div>
<div class="paragraph"><p>It takes as input the dataset file <span class="monospaced">dataset-twoclusters.csv</span> and
either a parameter file <span class="monospaced">trajectory.csv</span>. This will cause the program
the re-evaluate the loss function at the trajectory points which should
hopefully give the same values as already stored in the trajectory file
itself.</p></div>
<div class="paragraph"><p>However, this may be used with a different dataset file, e.g. the
testing or validation dataset, in order to evaluate the generalization
error in terms of the overall accuracy or the loss at the points along
the given trajectory.</p></div>
<div class="paragraph"><p>Interesting is also the second case, where instead of giving a
parameters file, we sample the parameter space equidistantly as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
    --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
    --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
    --interval_weights -<span style="color: #993399">5</span> <span style="color: #993399">5</span> <span style="color: #990000">\</span>
    --interval_biases -<span style="color: #993399">1</span> <span style="color: #993399">1</span> <span style="color: #990000">\</span>
    --samples_weights <span style="color: #993399">10</span> <span style="color: #990000">\</span>
    --samples_biases <span style="color: #993399">4</span></tt></pre></div></div>
<div class="paragraph"><p>Here, sample for each weight in the interval [-5,5] at 11 points (10<br>
endpoint), and similarly for the weights in the interval [-1,1] at 5
points.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>For anything but trivial networks the computational cost quickly becomes
prohibitively large. However, you may use <span class="monospaced">fix_parameter</span> to lower the
computational cost by choosing a certain subsets of weights and biases to
sample.</p></div>
</td>
</tr></table>
</div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
  --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
  --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --fix_parameters <span style="color: #FF0000">"output/weights/Variable:0=2.,2."</span> <span style="color: #990000">\</span>
  --interval_weights -<span style="color: #993399">5</span> <span style="color: #993399">5</span> <span style="color: #990000">\</span>
  --interval_biases -<span style="color: #993399">1</span> <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --samples_weights <span style="color: #993399">10</span> <span style="color: #990000">\</span>
  --samples_biases <span style="color: #993399">4</span></tt></pre></div></div>
<div class="paragraph"><p>Moreover, using <span class="monospaced">exclude_parameters</span> can be used to exclude parameters
from the variation, i.e. this subset is kept at fixed values read from
the file given by <span class="monospaced">parse_parameters_file</span> where the row designated by
the value in <span class="monospaced">parse_steps</span> is taken.</p></div>
<div class="paragraph"><p>This can be used to assess the shape of the loss manifold around a found
minimum.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiLossFunctionSampler <span style="color: #990000">\</span>
  --batch_data_files dataset-twoclusters<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --batch_size <span style="color: #993399">20</span> <span style="color: #990000">\</span>
  --csv_file TATiLossFunctionSampler-output-SGLD<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --exclude_parameters <span style="color: #FF0000">"w1"</span> <span style="color: #990000">\</span>
  --interval_center_step <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --interval_weights -<span style="color: #993399">5</span> <span style="color: #993399">5</span> <span style="color: #990000">\</span>
  --interval_biases -<span style="color: #993399">1</span> <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --parse_parameters_file centers<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
  --parse_steps <span style="color: #993399">1</span> <span style="color: #990000">\</span>
  --samples_weights <span style="color: #993399">10</span> <span style="color: #990000">\</span>
  --samples_biases <span style="color: #993399">4</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we have excluded the second weight, named <strong>w1</strong>, from the sampling.
Note that all weight and all bias degrees of freedom are simply
enumerated one after the other when going from the input layer till the
output layer.</p></div>
<div class="paragraph"><p>Furthermore, we have specified a file containing center points for all
excluded parameters. This file is of CSV style having a column <strong>step</strong> to
identify which row is to be used and moreover a column for every
(excluded) parameter that is fixed at a value unequal to 0. Note that
the minima file written by <span class="monospaced">TATiExplorer</span> can be used as this centers
file. Moreover, also the trajectory files have the same structure.</p></div>
</div>
<div class="sect3">
<h4 id="reference.concepts.network">3.1.3. The learned function</h4>
<div class="paragraph"><p>The second little utility programs does not evaluate the loss function
itself but the unknown function learned by the neural network depending
on the loss function, called the <span class="monospaced">TATiInputSpaceSampler</span>. In other
words, it gives the classification result for data point sampled from an
equidistant grid. Let us give an example call right away.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>TATiInputSpaceSampler <span style="color: #990000">\</span>
        --batch_data_files grid<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --csv_file TATiInputSpaceSampler-output<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --input_dimension <span style="color: #993399">2</span> <span style="color: #990000">\</span>
        --interval_input -<span style="color: #993399">4</span> <span style="color: #993399">4</span> <span style="color: #990000">\</span>
        --parse_steps <span style="color: #993399">1</span> <span style="color: #990000">\</span>
        --parse_parameters_file trajectory<span style="color: #990000">.</span>csv <span style="color: #990000">\</span>
        --samples_input <span style="color: #993399">10</span> <span style="color: #990000">\</span>
        --seed <span style="color: #993399">426</span></tt></pre></div></div>
<div class="paragraph"><p>Here, <span class="monospaced">batch_data_files</span> is an input file but it does not need to be
present. (Sorry about that abuse of the parameter as usually
<span class="monospaced">batch_data_files</span> is read-only. Here, it is overwritten!). Namely, it
is generated by the utility in that it equidistantly samples the input
space, using the interval [-4,4] for each input dimension and 10+1
samples (points on -4 and 4 included). The parameters file
<span class="monospaced">trajectory.csv</span> now contains the values of the parameters (weights
and biases) to use on which the learned function depends or by, in other
words, by which it is parametrized. As the trajectory contains a whole
flock of these, the <span class="monospaced">parse_steps</span> parameter tells it which steps to
use for evaluating each point on the equidistant input space grid,
simply referring to rows in said file.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>For anything but trivial input spaces the computational cost quickly
becomes prohibitively large. But again <span class="monospaced">fix_parameters</span> is heeded and can be
used to fix certain parameters. This is even necessary if parsing a trajectory
that was created using some parameters fixed as they then will <em>not</em>
appear in the set of parameters written to file. This will raise an
error as the file will contain too few values.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reference.examples">3.2. Examples</h3>
<div class="paragraph"><p>We show how to set up some basic models, which can be a useful tool
to study and test methods.</p></div>
<div class="sect3">
<h4 id="reference.examples.harmonic_oscillator">3.2.1. Harmonic oscillator</h4>
<div class="paragraph"><p>It can be sometimes useful to use TATi to simulate a simple harmonic oscillator.
This model can be obtained by training the neural network with one parameter
on one point $X=1$ in dimension one with a label
equal to zero, i.e. $Y=0$, and using the mean square loss function and
linear activation function.
More precisely, the cost function becomes in this setting:</p></div>
<div class="paragraph"><p>$L(\omega | X,Y) = | \omega X + b - Y|^2$,</p></div>
<div class="paragraph"><p>where we fix the bias $b=0$.</p></div>
<div class="ulist"><ul>
<li>
<p>
Dataset:
</p>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>common <span style="font-weight: bold"><span style="color: #000080">import</span></span> data_numpy_to_csv
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

X <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">([[</span><span style="color: #993399">1</span><span style="color: #990000">]])</span>
Y <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">([[</span><span style="color: #993399">0</span><span style="color: #990000">]])</span>

<span style="font-style: italic"><span style="color: #9A1900"># prepare and save the trivial data set for later</span></span>
datasetName <span style="color: #990000">=</span> <span style="color: #FF0000">'dataset_ho.csv'</span>
<span style="font-weight: bold"><span style="color: #000000">data_numpy_to_csv</span></span><span style="color: #990000">(</span>X<span style="color: #990000">,</span> Y<span style="color: #990000">,</span> datasetName<span style="color: #990000">)</span>
numberOfPoints <span style="color: #990000">=</span> <span style="color: #993399">1</span></tt></pre></div></div>
</li>
<li>
<p>
Setup and train neural network:
</p>
<div class="listingblock">
<div class="content monospaced">
<pre>import TATi.simulation as tati

import numpy as np
import pandas as pd

nn = tati(
    batch_data_files=["dataset_ho.csv"],
    batch_size=1,
    fix_parameters="output/biases/Variable:0=0.",
    friction_constant=10.0,
    input_dimension=1,
    inverse_temperature=1.0,
    loss="mean_squared",
    max_steps=1000,
    output_activation = "linear",
    output_dimension=1,
    run_file="run_ho.csv",
    sampler="BAOAB",
    seed=426,
    step_width=0.5,
    trajectory_file="trajectory_ho.csv",
    verbose=1
)

data = nn.sample()

df_trajectories = pd.DataFrame(data.trajectory)
# sampled trajectory
weight0 = np.asarray(df_trajectories['weight0'])</pre>
</div></div>
</li>
<li>
<p>
Sampled trajectory:
</p>
<div class="paragraph"><p>The output trajectory in <span class="monospaced">trajectory_ho.csv</span> or <span class="monospaced">weight0</span> is distributed w.r.t.
a Gaussian, i.e. the density of X:=weight0 is $exp(-X^2)$, see Figure
<a href="#reference_simulation_harmonic_oscillator.harmonic_oscillator">Gaussian distribution</a>.</p></div>
</li>
</ul></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">The figure was obtained with setting <span class="monospaced">max_steps</span> to 10000.</td>
</tr></table>
</div>
<div class="imageblock" id="reference.examples.harmonic_oscillator.density">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/density_harmonic_oscillator.png" alt="density harmonic oscillator" width="400">
</div>
<div class="title">Figure 8. Gaussian distribution: Histogram of the trajectories obtained by simulating 1D harmonic oscillator with BAOAB sampler.</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reference.samplers">3.3. Samplers</h3>
<div class="paragraph"><p>Samplers are discretizations of a given dynamical system, set by an ODE or
SDE. In our case, it is mostly Langevin dynamics. Given that the  sampler
is ergodic we can replace integrals over the whole domain by integrals along
sufficiently long trajectories.</p></div>
<div class="paragraph"><p>Certain asymptotical properties are inherently connected to the respective
dynamics, for example, the average kinetic energy. It can evaluated as average
by integrating along the trajecories. However, as it is not possible to
integrate the continuous dynamics directly, we rely on the respective
discretizen, the sampler, which naturally introduces an error.</p></div>
<div class="paragraph"><p>This discretization error depends on the chosen <em>finite step width</em> by which
the trajectories are produced. It shows as a finite error to the
asymptotical value regardless of the length of the trajectory. Choosing a
smaller step width will produce an error.</p></div>
<div class="paragraph"><p>This can be observed for the average kinetic energy by looking at a small
example network and dataset and producing sufficiently long trajectories.
If one plots the difference against the known asymptotical value (namely
$\frac{N^\text{dof}}{2} k_B T$ with temperature <strong>T</strong>, and degrees of
freedom $N^\text{dof}$) over different step widths in double
logarithmic fashion, one obtains straight lines whose slope depends on the
discretization order.</p></div>
<div class="paragraph"><p>Different samplers have different discretization orders and the order also
depends on the observed quantity.</p></div>
<div class="paragraph"><p>This makes picking the right sampler a critical choice: From a statistical point
of view the most accurate sampler is best, i.e. the one having the highest
discretization order. However, as all of them have roughly the same
computational cost, it is generally recommended to pick BAOAB which has second
order convergence and even fourth order in the high-friction limit, see
<a href="#Leimkuhler2012">[Leimkuhler2012]</a> for details.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">At the beginning of each the following subsections we give the name of
the respective sampler in order to activate it using the <strong>sampler</strong> keyword,
see <a href="#quickstart.simulation.sampling">[quickstart.simulation.sampling]</a> and <a href="#quickstart.cmdline.sampling">[quickstart.cmdline.sampling]</a>.</td>
</tr></table>
</div>
<div class="sect3">
<h4 id="reference.samplers.sgld">3.3.1. Stochastic Gradient Langevin Dynamics</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">StochasticGradientLangevinDynamics</span></p></div>
<div class="paragraph"><p>The Stochastic Gradient Langevin Dynamics (SGLD) was proposed by
<a href="#Welling2011">[Welling2011]</a> based on the <a href="#SGD">[SGD]</a>[Stochastic Gradient Descent], which is a
variant of the <a href="#GD">[GD]</a>[Gradient Descent] using only a subset of the dataset
for computing gradients. The central idea behind SGLD was to add an
additional noise term whose magnitude then controls the noise induced by
the approximate gradients.</p></div>
<div class="paragraph"><p>Implements a Stochastic Gradient Langevin Dynamics Sampler
  in the form of a TensorFlow Optimizer, overriding tensorflow.python.training.Optimizer.</p></div>
<div class="paragraph"><p>The step update is:
$\theta^{n+1} = \theta^{n} - \beta \nabla  U(\theta) \Delta t + \sqrt{\Delta t} G^n$,
where
$\beta$ is the inverse temperature coefficient, $\Delta t$ is the (discretization)
step width and $\theta$ is the parameter vector and $U(\theta)$ the energy or loss function.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 1. Table of parameters for SGLD</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p><strong>SGLD</strong> is very much like <strong>SGD</strong> and <strong>GD</strong> in terms that the <span class="monospaced">step_width</span> needs
to be small enough with respect to the gradient sizes of your problem.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.ccadl">3.3.2. Covariance Controlled Adaptive Langevin</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">CovarianceControlledAdaptiveLangevin</span></p></div>
<div class="paragraph"><p>This is an extension of Stochastic Gradient Descent proposed by
<a href="#Shang2015">[Shang2015]</a>. The key idea is to dissipate the extra heat caused by the
approximate gradients through a suitable thermostat. However, the
discretisation used here is not based on the (first-order)
Euler-Maruyama as <a href="#SGLD">[SGLD]</a> but on <a href="#GLA">[GLA]</a> 2nd order.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 2. Table of parameters for CCAdL</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">friction_constant</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">friction constant $\gamma$ that controls
how much momentum is replaced by random noise each step</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the noise,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">sigma</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">controlling the thermostat</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">sigmaA</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">controlling the thermostat&#8217;s adaptivity</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p><span class="monospaced">sigma</span> and <span class="monospaced">sigmaA</span> are two additional parameters that control the action
of the thermostat. Moreover, we require the same parameters as for <a href="#GLA">[GLA]</a> 2nd
order.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.gla">3.3.3. Geometric Langevin Algorithms</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">Geometric Langevin Algorithms_1stOrder</span>, <span class="monospaced">Geometric Langevin Algorithms_2ndOrder</span></p></div>
<div class="paragraph"><p>GLA results from a first-order splitting between the Hamiltonian and the
Ornstein-Uhlenbeck parts, see <a href="#Leimkuhler2015">[Leimkuhler2015]</a>[section 2.2.3, Leimkuhler 2015] and also
<a href="#Leimkuhler2012">[Leimkuhler2012]</a>. If the Hamiltonian part is discretized with a scheme of second order as in <strong>GLA2</strong>,
it provides second order accuracy at basically no extra
cost.</p></div>
<div class="paragraph"><p>The update step of the parameters for second order GLA is BABO:</p></div>
<div class="paragraph"><p>$B: p^{n+1/2} = p^n -\nabla U(q^n)\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$A: q^{n+1} =q^n+M^{-1}p^{n+1/2}\Delta t$</p></div>
<div class="paragraph"><p>$B: \tilde{p}^{n+1} = p^{n+1/2} -\nabla U(q^{n+1})\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$O: p^{n+1} = \alpha_{\Delta t}\tilde{p}^{n+1}+ \sqrt{\frac{1-\alpha_{\Delta t}^2}{\beta}M}G^n$
where $\alpha_{\Delta t}=exp(-\gamma \Delta t)$ and $G\sim N (0, 1)$.</p></div>
<div class="paragraph"><p>The first order GLA is BAO.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 3. Table of parameters for GLA1 and GLA2</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">friction_constant</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">friction constant $\gamma$ that controls
how much momentum is replaced by random noise each step</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the noise,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>All GLA samplers have two more parameters: <span class="monospaced">inverse_temperature</span> (usually denoted as $\beta$) and <span class="monospaced">friction_constant</span> (usually denoted as $\gamma$). Inverse
temperature controls the average momentum of each parameter while the
friction constant decides over how much of the momentum is replaced by
random noise, i.e. the random walker character of the trajectory.</p></div>
<div class="paragraph"><p>Good values for beta depend on the loss manifold and its barriers and
need to be find by try&amp;error at the moment.</p></div>
<div class="paragraph"><p>As a rough guide, $\gamma=10$ is a good start for the friction
constant. Moreover, when choosing such a friction constant, with
$\beta=1000$ sampling will remain in the starting minimum basin,
while $\beta=1$ exists basins very soon. Note that large noise can
be hidden by a too small friction constant, i.e. they both depend on each
other.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.samplers.baoab">3.3.4. BAOAB</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">BAOAB</span></p></div>
<div class="paragraph"><p>BAOAB derives from the basic building blocks A (position update), B
(momentum update), and O (noise update) into which the Langevin system
is split up. Each step is solved in a separate step. Hence, we perform a
B step, then an A step, &#8230; and so on. This scheme has second-order
accuracy and superb overall accuracy with respect to positions. See
<a href="#Leimkuhler2012">[Leimkuhler2012]</a> for more details.</p></div>
<div class="paragraph"><p>The update step of the parameters is:</p></div>
<div class="paragraph"><p>$B: p^{n+1/2} = p^n -\nabla U(q^n)\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$A: q^{n+1} =q^n+M^{-1}p^{n+1/2}\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$O: p^{n+1} = \alpha_{\Delta t}\tilde{p}^{n+1}+ \sqrt{\frac{1-\alpha_{\Delta t}^2}{\beta}M}G^n$</p></div>
<div class="paragraph"><p>$A: q^{n+1} =q^n+M^{-1}p^{n+1/2}\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>$B: p^{n+1/2} = p^n -\nabla U(q^n)\frac{\Delta t}{2}$</p></div>
<div class="paragraph"><p>where $\alpha_{\Delta t}=exp(-\gamma \Delta t)$ and $G\sim N (0, 1)$.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 4. Table of parameters for BAOAB</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">friction_constant</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">friction constant $\gamma$ that controls
how much momentum is replaced by random noise each step</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the noise,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="reference.samplers.hmc">3.3.5. Hamiltonian Monte Carlo</h4>
<div class="paragraph"><p><strong>sampler</strong>: <span class="monospaced">HamiltonianMonteCarlo</span></p></div>
<div class="paragraph"><p>HMC is based on Hamiltonian dynamics instead of Langevin Dynamics. Noise
only enters when, after the evaluation of an acceptance criterion, the
momenta are redrawn randomly. It has first been proposed by <a href="#Duane1987">[Duane1987]</a>.
The Metropolisation ensures that the sampled distribution is unbiased.
One virtue of HMC is that when using longer Verlet time integration
legs (<span class="monospaced">hamiltonian_dynamics_time</span> larger than 1) that the walker will
progress much further than in the case of Langevin Dynamics. However,
this comes at the price of extra computational work in terms of rejected
legs. See also <a href="#Neal2011">[Neal2011]</a> for a very readable introduction.</p></div>
<table class="tableblock frame-all grid-all"
style="
width:80%;
">
<caption class="title">Table 5. Table of parameters for HMC</caption>
<col style="width:60%;">
<col style="width:40%;">
<thead>
<tr>
<th class="tableblock halign-left valign-top" >Option name </th>
<th class="tableblock halign-center valign-top" >Description</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">hamiltonian_dynamics_time</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">Time used for integrating Hamiltonian
dynamics. This is relative to <span class="monospaced">step_width</span>. For example, if <span class="monospaced">step_width</span> is
<strong>0.05</strong> and this chose as <strong>0.05</strong> as well, it will evaluate every other step,
i.e. perform only a single time integration step before evaluating the
acceptance criterion. If you want to evaluate every <strong>n</strong> steps, then use
$n \cdot \Delta t$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">inverse_temperature</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">inverse temperature factor scaling the initially
randomly drawn momenta,
$\beta$</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top" ><p class="tableblock"><span class="monospaced">step_width</span></p></td>
<td class="tableblock halign-center valign-top" ><p class="tableblock">time integration step width $\delta t$</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">The exact amount of Hamiltonian dynamics time integration steps
is slightly varied (uniformly randomly scaled in [0.9,1.1]) such that not all
walkers evaluate at the same step and are correlated thereby.</td>
</tr></table>
</div>
<div class="paragraph"><p>NOTE:</p></div>
</div>
<div class="sect3">
<h4 id="reference.samplers.walkerensemble">3.3.6. Ensemble of Walkers</h4>
<div class="paragraph"><p>Ensemble of Walkers uses a collection of walkers that exchange gradient
and parameter information in each step in order to calculate a
preconditioning matrix. This preconditioning allows to explore elongated
minimum basins faster than independent walkers would do alone, see
<a href="#Matthews2018">[Matthews2018]</a>.</p></div>
<div class="paragraph"><p>This is activated by setting the <span class="monospaced">number_walkers</span> to a value larger than
1. Note that <span class="monospaced">covariance_blending</span> controls the magnitude of the
covariance matrix approximation and <span class="monospaced">collapse_after_steps</span> controls
after how many steps the walkers are restarted at the parameter
configuration of the first walker to ensure that the harmonic
approximation still holds.</p></div>
<div class="paragraph"><p>This works for all of the aforementioned samplers as simply the gradient
of each walker is rescaled.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="reference.implementing_sampler">3.4. Simulation module: Implementing a sampler</h3>
<div class="paragraph"><p>We would like to demonstrate how to implement the <a href="#GLA">[GLA]</a> sampler of 2nd order
using the simulation module.</p></div>
<div class="paragraph"><p>Let us look at the four integration steps.</p></div>
<div class="openblock" id="reference.implementing_sampler.gla2">
<div class="title">Geometric Langevin Algorithm 2nd order</div>
<div class="content">
<div class="olist arabic"><ol class="arabic">
<li>
<p>
$p_{n+\tfrac 1 2} = p_n - \tfrac {\lambda}{2} \nabla_x L(x_n)$
</p>
</li>
<li>
<p>
$x_{n+1} = x_n + \lambda p_{n+\tfrac 1 2}$
</p>
</li>
<li>
<p>
$\widehat{p}_{n+1} = p_{n+\tfrac 1 2} - \tfrac {\lambda}{2} \nabla_x L(x_{n+1})$
</p>
</li>
<li>
<p>
$p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n$
</p>
</li>
</ol></div>
</div></div>
<div class="paragraph"><p>If you are familar with the <strong>A</strong> (position integration), <strong>B</strong>
(momentum integration), <strong>O</strong> (noise integration) notation, see <a href="#Leimkuhler2012">[Leimkuhler2012]</a>
then you will notice that we have the steps: <strong>BABO</strong>.</p></div>
<div class="sect3">
<h4 id="reference.implementing_sampler.naive_update">3.4.1. Simple update implementation</h4>
<div class="paragraph"><p>Therefore, let us write a python function that works on several <span class="monospaced">numpy</span> arrays
producing a single GLA2 update step by performing the four integration steps
above. To this end, we make use <span class="monospaced">tati.gradients()</span> for computing the gradients
$\nabla_x L(x_{n+1})$.</p></div>
<div class="listingblock">
<div class="title">A first GLA2 update implementation</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">gla2_update_step</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">,</span> momenta<span style="color: #990000">,</span> step_width<span style="color: #990000">,</span> beta<span style="color: #990000">,</span> gamma<span style="color: #990000">):</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  <span style="font-style: italic"><span style="color: #9A1900"># 1. p_{n+\tfrac 1 2} = p_n - \tfrac {\lambda}{2} \nabla_x L(x_n)</span></span>
  momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">()</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 2. x_{n+1} = x_n + \lambda p_{n+\tfrac 1 2}</span></span>
  nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> nn<span style="color: #990000">.</span>parameters <span style="color: #990000">+</span> step_width <span style="color: #990000">*</span> momenta  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 3. \widehat{p}_{n+1} = p_{n+\tfrac 1 2} - \tfrac {\lambda}{2} \nabla_x L(x_{n+1})</span></span>
  momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">()</span>  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 4. p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n</span></span>
  alpha <span style="color: #990000">=</span> math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">exp</span></span><span style="color: #990000">(-</span>gamma<span style="color: #990000">*</span>step_width<span style="color: #990000">)</span>
  momenta <span style="color: #990000">=</span> alpha <span style="color: #990000">*</span> momenta <span style="color: #990000">+</span> <span style="color: #990000">\</span>
            math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sqrt</span></span><span style="color: #990000">((</span><span style="color: #993399">1</span><span style="color: #990000">.-</span>math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">pow</span></span><span style="color: #990000">(</span>alpha<span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">.))/</span>beta<span style="color: #990000">)</span> <span style="color: #990000">*</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">standard_normal</span></span><span style="color: #990000">(</span>momenta<span style="color: #990000">.</span>shape<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span>

  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> momenta</tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
In the function header we need access to the <span class="monospaced">tati</span> reference, to the
<span class="monospaced">numpy</span> array containing the <span class="monospaced">momenta</span>. Moreover, we need a few parameters,
namely the step width <span class="monospaced">step_width</span>, the inverse temperature factor <span class="monospaced">beta</span> and
the friction constant <span class="monospaced">gamma</span>.
</p>
</li>
<li>
<p>
First, we perform the <strong>B</strong> step integrating the momenta.
</p>
</li>
<li>
<p>
Next comes the <strong>A</strong> step, integrating positions with the updated momenta.
</p>
</li>
<li>
<p>
Then, we integrate momenta again, <strong>B</strong>.
</p>
</li>
<li>
<p>
Last, we perform the noise integration <strong>O</strong>. First, we compute the value of
$\alpha_n$ and the the momenta are partially reset by the noise.
</p>
</li>
</ol></div>
<div class="paragraph"><p>As the source of noise we have simply used `numpy`s standard normal
distribution.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">It is advisable to fix the seed using <span class="monospaced">numpy.random.seed(426)</span> (or any
other value) to allow for reproducible runs.</td>
</tr></table>
</div>
<div class="paragraph"><p>We could have used <span class="monospaced">nn.momenta</span> for storing momenta. However, this
needs some extra computations for assigning the momenta inside the tensorflow
computational graph. As they are not needed in the graph anyway, we can store
them outside directly.</p></div>
<div class="paragraph"><p>Note that we have been a bit wasteful in the above implementation but very
close to the formulas in <a href="#reference.implementing_sampler">[reference.implementing_sampler]</a>.</p></div>
</div>
<div class="sect3">
<h4 id="reference.implementing_sampler.saving_old_gradients">3.4.2. Saving a gradient evaluation</h4>
<div class="paragraph"><p>We evaluate the gradient twice but actually only one evaluation would have been
needed: The update gradients in step 3. are the same as the gradients in step 1.
on the next iteration.</p></div>
<div class="paragraph"><p>Hence, let us refine the function with respect to this.</p></div>
<div class="listingblock">
<div class="title">GLa2 update implementation with just one gradient evaluation</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">gla2_update_step</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">,</span> momenta<span style="color: #990000">,</span> old_gradients<span style="color: #990000">,</span> step_width<span style="color: #990000">,</span> beta<span style="color: #990000">,</span> gamma<span style="color: #990000">):</span>
    <span style="font-style: italic"><span style="color: #9A1900"># 1. p_{n+\tfrac 1 2} = p_n - \tfrac {\lambda}{2} \nabla_x L(x_n)</span></span>
    momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> old_gradients

    <span style="font-style: italic"><span style="color: #9A1900"># 2. x_{n+1} = x_n + \lambda p_{n+\tfrac 1 2}</span></span>
    nn<span style="color: #990000">.</span>parameters <span style="color: #990000">=</span> nn<span style="color: #990000">.</span>parameters <span style="color: #990000">+</span> step_width <span style="color: #990000">*</span> momenta

    <span style="font-style: italic"><span style="color: #9A1900"># \nabla_x L(x_{n+1})</span></span>
    gradients <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">gradients</span></span><span style="color: #990000">()</span>

    <span style="font-style: italic"><span style="color: #9A1900"># 3. \widehat{p}_{n+1} = p_{n+\tfrac 1 2} - \tfrac {\lambda}{2} \nabla_x L(x_{n+1})</span></span>
    momenta <span style="color: #990000">-=</span> <span style="color: #990000">.</span><span style="color: #993399">5</span><span style="color: #990000">*</span>step_width <span style="color: #990000">*</span> gradients

    <span style="font-style: italic"><span style="color: #9A1900"># 4. p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n</span></span>
    alpha <span style="color: #990000">=</span> math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">exp</span></span><span style="color: #990000">(-</span>gamma<span style="color: #990000">*</span>step_width<span style="color: #990000">)</span>
    momenta <span style="color: #990000">=</span> alpha <span style="color: #990000">*</span> momenta <span style="color: #990000">+</span> <span style="color: #990000">\</span>
              math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sqrt</span></span><span style="color: #990000">((</span><span style="color: #993399">1</span><span style="color: #990000">.-</span>math<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">pow</span></span><span style="color: #990000">(</span>alpha<span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">.))/</span>beta<span style="color: #990000">)</span> <span style="color: #990000">*</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">standard_normal</span></span><span style="color: #990000">(</span>momenta<span style="color: #990000">.</span>shape<span style="color: #990000">)</span>

    <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> gradients<span style="color: #990000">,</span> momenta</tt></pre></div></div>
<div class="paragraph"><p>Now, we use <span class="monospaced">old_gradients</span> in step 1. and return the updated gradients such
that it can be given as old gradients in the next call.</p></div>
</div>
<div class="sect3">
<h4 id="reference.implementing_sampler.loop_body">3.4.3. The loop</h4>
<div class="paragraph"><p>Now, we ad the loop body.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> math
<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
<span style="font-weight: bold"><span style="color: #000080">import</span></span> TATi<span style="color: #990000">.</span>simulation as tati

np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">426</span><span style="color: #990000">)</span>

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters.csv"</span><span style="color: #990000">],</span>
<span style="color: #990000">)</span>

momenta <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">((</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
old_gradients <span style="color: #990000">=</span> np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">((</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">num_parameters</span></span><span style="color: #990000">()))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>

<span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="color: #993399">100</span><span style="color: #990000">):</span>  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Current step #"</span><span style="color: #990000">+</span><span style="font-weight: bold"><span style="color: #000000">str</span></span><span style="color: #990000">(</span>i<span style="color: #990000">))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
    old_gradients<span style="color: #990000">,</span> momenta <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">gla2_update_step</span></span><span style="color: #990000">(</span>
        nn<span style="color: #990000">,</span> momenta<span style="color: #990000">,</span> old_gradients<span style="color: #990000">,</span> step_width<span style="color: #990000">=</span><span style="color: #993399">1e-2</span><span style="color: #990000">,</span> beta<span style="color: #990000">=</span><span style="color: #993399">1e3</span><span style="color: #990000">,</span> gamma<span style="color: #990000">=</span><span style="color: #993399">10</span><span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">loss</span></span><span style="color: #990000">())</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span></tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
We instantiate a <span class="monospaced">tati</span> instance as usual, giving it the dataset and using
its default single-layer perceptron topology.
</p>
</li>
<li>
<p>
We create two numpy arrays to contain the momenta and the old gradients.
</p>
</li>
<li>
<p>
We iterate for 100 steps, printing the current step.
</p>
</li>
<li>
<p>
We use the <span class="monospaced">gla2_update_step()</span> function written to perform a single update
step. We store the returned gradients and momenta.
</p>
</li>
<li>
<p>
Finally, we print the loss per step.
</p>
</li>
</ol></div>
</div>
</div>
<div class="sect2">
<h3 id="reference.reproducibility">3.5. A Note on Reproducibility</h3>
<div class="paragraph"><p>In many of the examples in the quickstart tutorials we have set a <em>seed</em> value
to enforce reproducible runs.</p></div>
<div class="paragraph"><p>We have gone through great lengths to make sure that runs using the same set
of options yield the same output on every evocation.</p></div>
<div class="paragraph"><p>Tensorflow is not fully reproducible per se. Its internal random number seeds
change when the computational graph changes. Its reduction operations are
non-deterministic. The latter can be overcome by setting <em>inter_ops_threads</em> to
<em>1</em>, which take away some of the parallelization for the sake of
reproducibility. The former is taken care of by TATi itself. We make sure to
set the random number seeds deterministically to ensure that values are
unchanged even if the graph is slighlty changed.</p></div>
<div class="paragraph"><p>If you find that this should not be the case, please file an issue, see
<a href="#introduction.feedback">[introduction.feedback]</a>.</p></div>
</div>
<div class="sect2">
<h3 id="reference.performance">3.6. A Note on Performance</h3>
<div class="paragraph"><p>Performance is everything in the world of neural network training. Codes and
machines are measured by how fast they perform in images/second when training
AlexNet or other networks on the ImageNet dataset, see <a href="https://www.tensorflow.org/performance/benchmarks">Tensorflow Benchmarks</a>.</p></div>
<div class="paragraph"><p>We worked hard to ensure that whatever Tensorflow offers in performance is also
seen when using TATi. In order to guide the user in what to expect and what to
do when these expectations are not met, we invite to go through this section.</p></div>
<div class="paragraph"><p>In general, performance hinges <strong>critically</strong> on the input pipeline. In other
words, it depends very much on how fast a specific machine setup can feed the
dataset into the input layer of the neural network.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">In our examples both datasets and networks are very small. This causes
the sequential parts of tensorflow to overwhelm any kind of parallel execution.</td>
</tr></table>
</div>
<div class="paragraph"><p>Typically, these datasets are stored as a set of files residing on disk. Note
that reading from disk is very slow compared to reading from memory. Hence, the
first step is to read the dataset from disk and this will completely dominate
the computational load at the beginning.</p></div>
<div class="paragraph"><p>If the dataset is small enough to completely fit in memory, TATi will uses
Tensorflow&#8217;s <em>caching</em> to speed up the operations. This will become noticeable
after the first epoch, i.e. when all batches of the dataset have been processed
exactly once. Caching delivers at least a tenfold increase in learning speed,
depending on your hard drive setup.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="title">In memory pipeline</div>
<div class="paragraph"><p>If your dataset fits in memory, it is advised to use the <span class="monospaced">InMemoryPipeline</span>
by setting the appropriate options in <span class="monospaced">tati</span> instantiation, see
<a href="#quickstart.simulation">[quickstart.simulation]</a>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">tati</span></span><span style="color: #990000">(</span>
        <span style="font-style: italic"><span style="color: #9A1900"># ...</span></span>
        in_memory_pipeline <span style="color: #990000">=</span> True<span style="color: #990000">,</span>
        <span style="font-style: italic"><span style="color: #9A1900"># ...</span></span>
<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>When using the command-line interface, add the respective option, see <a href="#quickstart.cmdline">[quickstart.cmdline]</a>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">...</span>
  --in_memory_pipeline <span style="color: #993399">1</span> <span style="color: #990000">\</span>
<span style="color: #990000">...</span></tt></pre></div></div>
</td>
</tr></table>
</div>
<div class="paragraph"><p>Furthermore, TATi uses Tensorflow&#8217;s prefetching to interleave feeding and
training operations. This will take effect roughly after the second epoch.
Prefetching will show an increase by another factor of 2.</p></div>
<div class="paragraph"><p>A typical runtime profile is given in Figure <a href="#references.performance.runtime_comparison_cpu">[references.performance.runtime_comparison_cpu]</a>
where we show the time spent for every 10 steps over the whole history. This is
done by simply plotting the <em>time_per_nth_step</em> column from the run file against
the <em>step</em> column.
There, we have used the <a href="#BAOAB">[BAOAB]</a> sampler. Initially, there is a large peak
caused by the necessary parsing of the dataset from disk. This is followed by a
period where the caching is effective and runtime per nth step has dropped
dramatically. From this time on, Tensorflow will be able to make use of parallel
threads for training. Then, we see another drop when prefetching kicks in.</p></div>
<div class="imageblock" id="references.performance.runtime_comparison_cpu">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/time_per_nth_step_hidden_dimension-dimension_16000-batch_size_1000-2018-06-28.png" alt="runtime comparison" width="600">
</div>
<div class="title">Figure 9. Runtime comparison, CPU: Core i7, network with a single hidden layer and various numbers of nodes on a random MNIST dataset</div>
</div>
<div class="paragraph"><p>Note that Tensorflow has been designed to use GPGPU cards such as offered by
NVIDIA (and also Google&#8217;s own domain-specific chip called Tensor Proccessing
Unit). If such a GPGPU card is employed, the actual linear algebra operations
necessary for the gradient calculation and weight and bias updates during
training will become negligible except for very large networks (1e6 dof and
beyond).</p></div>
<div class="paragraph"><p>In <a href="#references.performance.runtime_comparison_gpu">[references.performance.runtime_comparison_gpu]</a> we give the same runtime
profile as before. In contrast to before, the simulation is now done on a
system with 2 NVIDIA V100 cards. Comparing this to figure <a href="#references.performance.runtime_comparison_cpu">[references.performance.runtime_comparison_cpu]</a>
we notice that now all curves associated to different number of nodes in the
hidden layer (<strong>hidden_dimension</strong>) basically lie on top of each other. In the
runtime profile on CPUs alone there is a clear trend for networks with more
degrees of freedom to significantly require more time per training step. We
conclude that with these networks (784 input nodes, 10 output nodes,
<strong>hidden_dimension</strong> hidden nodes, i.e. ~1e6 dof) the V100s do not see full load,
yet.</p></div>
<div class="imageblock" id="references.performance.runtime_comparison_gpu">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/time_per_nth_step_hidden_dimension-hash_912b074-dimension_5000-batch_size_100-semilogy-2018-06-27.png" alt="runtime comparison" width="600">
</div>
<div class="title">Figure 10. Runtime comparison, GPU: 2x V100 cards, network with a single hidden layer and various numbers of nodes on a random MNIST dataset</div>
</div>
</div>
<div class="sect2">
<h3 id="reference.miscellaneous">3.7. Miscellaneous</h3>
<div class="sect3">
<h4 id="reference.miscellaneous.parameter_freeze">3.7.1. Freezing parameters</h4>
<div class="paragraph"><p>Sometimes it might be desirable to freeze parameters during training or
sampling. This can be done as follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> TATi<span style="color: #990000">.</span>models<span style="color: #990000">.</span>model <span style="font-weight: bold"><span style="color: #000080">import</span></span> model

<span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np

FLAGS <span style="color: #990000">=</span> model<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">setup_parameters</span></span><span style="color: #990000">(</span>
    batch_data_files<span style="color: #990000">=[</span><span style="color: #FF0000">"dataset-twoclusters-small.csv"</span><span style="color: #990000">],</span>
    fix_parameters<span style="color: #990000">=</span><span style="color: #FF0000">"output/biases/Variable:0=2."</span><span style="color: #990000">,</span>
    max_steps<span style="color: #990000">=</span><span style="color: #993399">5</span><span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span><span style="color: #993399">426</span><span style="color: #990000">,</span>
<span style="color: #990000">)</span>

nn <span style="color: #990000">=</span> <span style="font-weight: bold"><span style="color: #000000">model</span></span><span style="color: #990000">(</span>FLAGS<span style="color: #990000">)</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_input_pipeline</span></span><span style="color: #990000">()</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">init_network</span></span><span style="color: #990000">(</span>None<span style="color: #990000">,</span> setup<span style="color: #990000">=</span><span style="color: #FF0000">"train"</span><span style="color: #990000">)</span>
nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">reset_dataset</span></span><span style="color: #990000">()</span>
run_info<span style="color: #990000">,</span> trajectory<span style="color: #990000">,</span> _ <span style="color: #990000">=</span> nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">train</span></span><span style="color: #990000">(</span>return_run_info<span style="color: #990000">=</span>True<span style="color: #990000">,</span> <span style="color: #990000">\</span>
  return_trajectories<span style="color: #990000">=</span>True<span style="color: #990000">)</span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"Train results"</span><span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>np<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">asarray</span></span><span style="color: #990000">(</span>trajectory<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span><span style="color: #993399">5</span><span style="color: #990000">]))</span></tt></pre></div></div>
<div class="paragraph"><p>Note that you need to initialize the network without adding training or
sampling methods, i.e. <span class="monospaced">setup</span> is None. Then, we fix the parameter
where we give its name in full tensorflow parlance. Afterwards, we may
add sample or training nodes and start training/sampling.</p></div>
<div class="exampleblock">
<div class="content">
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">Single values cannot be frozen but only entire weight matrices or bias
vectors per layer at the moment.</td>
</tr></table>
</div>
</div></div>
</div>
<div class="sect3">
<h4 id="reference.miscellaneous.progress_bar">3.7.2. Displaying a progress bar</h4>
<div class="paragraph"><p>For longer simulation runs it is desirable to obtain an estimate after a
few steps of the time required for the entire run.</p></div>
<div class="paragraph"><p>This is possible using the <span class="monospaced">progress</span> option. Specified to 1 or True it
will produce a progress bar showing the total number of steps, the
iterations per second, the elapsed time since start and the estimated
time till finish.</p></div>
<div class="paragraph"><p>This features requires the <a href="https://github.com/tqdm/tqdm">tqdm</a> package.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>On the debug verbosity level per output step also an estimate of the
remaining run time is given.</p></div>
</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="reference.miscellaneous.summaries">3.7.3. Tensorflow summaries</h4>
<div class="paragraph"><p>Tensorflow delivers a powerful instrument for inspecting the inner
workings of its computational graph: TensorBoard.</p></div>
<div class="paragraph"><p>This tool allows also to inspect values such as the activation
histogram, the loss and accuracy and many other parameters and values
internal to TATi.</p></div>
<div class="paragraph"><p>Supplying a path <span class="monospaced">/foo/bar</span> present in the file system using the
<span class="monospaced">summaries_path</span> variable, summaries are automatically written to the
path and can be inspected with the following call to tensorboard.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>tensorboard --logdir /foo/bar</tt></pre></div></div>
<div class="paragraph"><p>The tensorboard essentially comprises a web server for rendering the
nodes of the graph and figures of the inspected values inside a web page.
On execution it provides a URL that needs to be entered in any
web browser to access the web page.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph"><p>The accumulation and writing of the summaries has quite an impact on
TATi&#8217;s overall performance and is therefore switched off by default.</p></div>
</td>
</tr></table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_acknowledgements">Acknowledgements</h2>
<div class="sectionbody">
<div class="paragraph"><p>Thanks to all users of the code!</p></div>
</div>
</div>
<div class="sect1">
<h2 id="bibliography">Literature</h2>
<div class="sectionbody">
<div class="ulist"><ul>
<li>
<p>
<a id="Coifman2006"></a> Coifman, R. R., &amp; Lafon, S. (2006).
Diffusion maps.
Applied and Computational Harmonic Analysis, 21(1), 5–30.
<a href="https://doi.org/10.1016/j.acha.2006.04.006">https://doi.org/10.1016/j.acha.2006.04.006</a>
</p>
</li>
<li>
<p>
<a id="Duane1987"></a> Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987).
Hybrid Monte Carlo.
Physics Letters B, 195(2), 216–222.
<a href="http://doi.org/10.1016/0370-2693(87)91197-X">http://doi.org/10.1016/0370-2693(87)91197-X</a>
</p>
</li>
<li>
<p>
<a id="Leimkuhler2012"></a> Leimkuhler, B., &amp; Matthews, C. (2012).
Rational Construction of Stochastic Numerical Methods for Molecular Sampling.
Applied Mathematics Research EXpress, 2013(1), 34–56.
<a href="http://doi.org/10.1093/amrx/abs010">http://doi.org/10.1093/amrx/abs010</a>
</p>
</li>
<li>
<p>
<a id="Leimkuhler2015"></a> Leimkuhler, B., Matthews, C., &amp; Stoltz, G. (2015).
The computation of averages from equilibrium and nonequilibrium Langevin molecular dynamics.
IMA Journal of Numerical Analysis, 1–55.
<a href="http://doi.org/10.1093/imanum/dru056">http://doi.org/10.1093/imanum/dru056</a>
</p>
</li>
<li>
<p>
<a id="Matthews2018"></a> Matthews, C., Weare, J., &amp; Leimkuhler, B. (2018).
Ensemble preconditioning for Markov chain Monte Carlo simulation.
Statistics and Computing, 28(2), 277–290.
<a href="http://doi.org/10.1007/s11222-017-9730-1">http://doi.org/10.1007/s11222-017-9730-1</a>
</p>
</li>
<li>
<p>
<a id="Neal2011"></a> Neal, R. M. (2011).
MCMC Using Hamiltonian Dynamics.
In Handbook of Markov Chain Monte Carlo (pp. 113–162).
</p>
</li>
<li>
<p>
<a id="Shang2015"></a> Shang, X., Zhu, Z., Leimkuhler, B., &amp; Storkey, A. J. (2015).
Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling.
In C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett (Ed.),
Advances in Neural Information Processing Systems 28 (pp. 37–45).
Curran Associates, Inc.
<a href="http://doi.org/10.1515/jip-2012-0071">http://doi.org/10.1515/jip-2012-0071</a>
</p>
</li>
<li>
<p>
<a id="Trstanova2016"></a> Trstanova, Z. (2016).
Mathematical and Algorithmic Analysis of Modified Langevin Dynamics.
</p>
</li>
<li>
<p>
<a id="Welling2011"></a> Welling, M., &amp; Teh, Y.-W. (2011).
Bayesian Learning via Stochastic Gradient Langevin Dynamics.
Proceedings of the 28th International Conference on Machine Learning, 681–688.
<a href="http://doi.org/10.1515/jip-2012-0071">http://doi.org/10.1515/jip-2012-0071</a>
</p>
</li>
</ul></div>
</div>
</div>
<div class="sect1">
<h2 id="glossary">Glossary</h2>
<div class="sectionbody">
<div class="ulist"><ul>
<li>
<p>
<a id="BAOAB"></a> <strong>BAOAB</strong>
</p>
<div class="paragraph"><p>BAOAB is the short-form for the order of the exact solution steps in the
splitting of the Langevin Dynamics SDE: B means momentum update, A is the
position update, and O is the random noise update. It has 2nd order convergence
properties, showing even 4th order super-convergence in the context of
high friction, see <a href="#Leimkuhler2012">[Leimkuhler2012]</a>.</p></div>
</li>
<li>
<p>
<a id="CCAdL"></a> <strong>Covariance Controlled Adaptive Langevin</strong> (CCAdL)
</p>
<div class="paragraph"><p>This is an extension of <a href="#SGD">[SGD]</a> that uses a thermostat to dissipate the
extra noise through approximate gradients from the system.</p></div>
</li>
<li>
<p>
<a id="GD"></a> <strong>Gradient Descent</strong> (GD)
</p>
<div class="paragraph"><p>An iterative, first-order optimization that use the negative gradient
times a step width to converge towards the minimum.</p></div>
</li>
<li>
<p>
<a id="GLA"></a> <strong>Geometric Langevin Algorithm</strong> (GLA)
</p>
<div class="paragraph"><p>This family of samplers results from a first-order splitting between the
Hamiltonian and the Ornstein-Uhlenbeck parts. It provides up to
second-order accuracy. In the package we have implemented both the 1st
and 2nd order variant. GLA2nd is among the most accurate samplers,
especially when it comes to accuracy of momenta. It is surpassed by
BAOAB, particularly for positions.</p></div>
</li>
<li>
<p>
<a id="HMC"></a> <strong>Hamiltonian Monte Carlo</strong> (HMC)
</p>
<div class="paragraph"><p>Instead of Langevin Dynamics this sampler relies on Hamiltonian
Dynamics. After a specific number of trajectory steps an acceptance
criterion is evaluated. Afterwards momenta are drawn randomly. Hence,
here noise comes into play at distinct intervals while for the other
samplers noise enters gradually in every step.</p></div>
</li>
<li>
<p>
<a id="SGD"></a> <strong>Stochastic Gradient Descent</strong> (SGD)
</p>
<div class="paragraph"><p>A variant of <a href="#GD">[GD]</a> where not the whole dataset is used for the gradient
computation but only a smaller part. This lightens the computational
complexity and adds some noise to the iteration as gradients are only
approximate. However, given redundancy in the dataset this noise is
often welcome and helps in overcoming barriers in the non-convex
minimization problem.</p></div>
<div class="paragraph"><p>See also <a href="#GD">[GD]</a>.</p></div>
</li>
<li>
<p>
<a id="SGLD"></a> <strong>Stochastic Gradient Langevin Dynamics</strong> (SGLD)
</p>
<div class="paragraph"><p>A variant of SGD where the approximate gradients are not only source of
noise but an additional noise term is added whose magnitude controls the
noise from the gradients.</p></div>
<div class="paragraph"><p>See also <a href="#SGD">[SGD]</a>.</p></div>
</li>
</ul></div>
</div>
</div>
</div>
<div id="footnotes"><hr></div>
<div id="footer">
<div id="footer-text">
Version v0.9.1-0-g994aa50<br>
Last updated
 2018-07-27 21:38:51 BST
</div>
</div>
</body>
</html>
