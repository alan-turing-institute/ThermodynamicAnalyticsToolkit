<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="AsciiDoc 8.6.10">
<meta name="description" content="TATi is a software suite based on tensorflow that brings enhanced sampling methods based on Langevin Dynamics and Hamiltonian Dynamics to neural network training. :Date:        2018-08-17">
<meta name="keywords" content="neural networks, loss, loss manifold, sampling, exploration">
<title>Thermodynamic Analytics Toolkit (TATi) - Programmer&#8217;s Guide</title>
<style type="text/css">
/* Shared CSS for AsciiDoc xhtml11 and html5 backends */

/* Default font. */
body {
  font-family: Georgia,serif;
}

/* Title font. */
h1, h2, h3, h4, h5, h6,
div.title, caption.title,
thead, p.table.header,
#toctitle,
#author, #revnumber, #revdate, #revremark,
#footer {
  font-family: Arial,Helvetica,sans-serif;
}

body {
  margin: 1em 5% 1em 5%;
}

a {
  color: blue;
  text-decoration: underline;
}
a:visited {
  color: fuchsia;
}

em {
  font-style: italic;
  color: navy;
}

strong {
  font-weight: bold;
  color: #083194;
}

h1, h2, h3, h4, h5, h6 {
  color: #527bbd;
  margin-top: 1.2em;
  margin-bottom: 0.5em;
  line-height: 1.3;
}

h1, h2, h3 {
  border-bottom: 2px solid silver;
}
h2 {
  padding-top: 0.5em;
}
h3 {
  float: left;
}
h3 + * {
  clear: left;
}
h5 {
  font-size: 1.0em;
}

div.sectionbody {
  margin-left: 0;
}

hr {
  border: 1px solid silver;
}

p {
  margin-top: 0.5em;
  margin-bottom: 0.5em;
}

ul, ol, li > p {
  margin-top: 0;
}
ul > li     { color: #aaa; }
ul > li > * { color: black; }

.monospaced, code, pre {
  font-family: "Courier New", Courier, monospace;
  font-size: inherit;
  color: navy;
  padding: 0;
  margin: 0;
}
pre {
  white-space: pre-wrap;
}

#author {
  color: #527bbd;
  font-weight: bold;
  font-size: 1.1em;
}
#email {
}
#revnumber, #revdate, #revremark {
}

#footer {
  font-size: small;
  border-top: 2px solid silver;
  padding-top: 0.5em;
  margin-top: 4.0em;
}
#footer-text {
  float: left;
  padding-bottom: 0.5em;
}
#footer-badges {
  float: right;
  padding-bottom: 0.5em;
}

#preamble {
  margin-top: 1.5em;
  margin-bottom: 1.5em;
}
div.imageblock, div.exampleblock, div.verseblock,
div.quoteblock, div.literalblock, div.listingblock, div.sidebarblock,
div.admonitionblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.admonitionblock {
  margin-top: 2.0em;
  margin-bottom: 2.0em;
  margin-right: 10%;
  color: #606060;
}

div.content { /* Block element content. */
  padding: 0;
}

/* Block element titles. */
div.title, caption.title {
  color: #527bbd;
  font-weight: bold;
  text-align: left;
  margin-top: 1.0em;
  margin-bottom: 0.5em;
}
div.title + * {
  margin-top: 0;
}

td div.title:first-child {
  margin-top: 0.0em;
}
div.content div.title:first-child {
  margin-top: 0.0em;
}
div.content + div.title {
  margin-top: 0.0em;
}

div.sidebarblock > div.content {
  background: #ffffee;
  border: 1px solid #dddddd;
  border-left: 4px solid #f0f0f0;
  padding: 0.5em;
}

div.listingblock > div.content {
  border: 1px solid #dddddd;
  border-left: 5px solid #f0f0f0;
  background: #f8f8f8;
  padding: 0.5em;
}

div.quoteblock, div.verseblock {
  padding-left: 1.0em;
  margin-left: 1.0em;
  margin-right: 10%;
  border-left: 5px solid #f0f0f0;
  color: #888;
}

div.quoteblock > div.attribution {
  padding-top: 0.5em;
  text-align: right;
}

div.verseblock > pre.content {
  font-family: inherit;
  font-size: inherit;
}
div.verseblock > div.attribution {
  padding-top: 0.75em;
  text-align: left;
}
/* DEPRECATED: Pre version 8.2.7 verse style literal block. */
div.verseblock + div.attribution {
  text-align: left;
}

div.admonitionblock .icon {
  vertical-align: top;
  font-size: 1.1em;
  font-weight: bold;
  text-decoration: underline;
  color: #527bbd;
  padding-right: 0.5em;
}
div.admonitionblock td.content {
  padding-left: 0.5em;
  border-left: 3px solid #dddddd;
}

div.exampleblock > div.content {
  border-left: 3px solid #dddddd;
  padding-left: 0.5em;
}

div.imageblock div.content { padding-left: 0; }
span.image img { border-style: none; vertical-align: text-bottom; }
a.image:visited { color: white; }

dl {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
dt {
  margin-top: 0.5em;
  margin-bottom: 0;
  font-style: normal;
  color: navy;
}
dd > *:first-child {
  margin-top: 0.1em;
}

ul, ol {
    list-style-position: outside;
}
ol.arabic {
  list-style-type: decimal;
}
ol.loweralpha {
  list-style-type: lower-alpha;
}
ol.upperalpha {
  list-style-type: upper-alpha;
}
ol.lowerroman {
  list-style-type: lower-roman;
}
ol.upperroman {
  list-style-type: upper-roman;
}

div.compact ul, div.compact ol,
div.compact p, div.compact p,
div.compact div, div.compact div {
  margin-top: 0.1em;
  margin-bottom: 0.1em;
}

tfoot {
  font-weight: bold;
}
td > div.verse {
  white-space: pre;
}

div.hdlist {
  margin-top: 0.8em;
  margin-bottom: 0.8em;
}
div.hdlist tr {
  padding-bottom: 15px;
}
dt.hdlist1.strong, td.hdlist1.strong {
  font-weight: bold;
}
td.hdlist1 {
  vertical-align: top;
  font-style: normal;
  padding-right: 0.8em;
  color: navy;
}
td.hdlist2 {
  vertical-align: top;
}
div.hdlist.compact tr {
  margin: 0;
  padding-bottom: 0;
}

.comment {
  background: yellow;
}

.footnote, .footnoteref {
  font-size: 0.8em;
}

span.footnote, span.footnoteref {
  vertical-align: super;
}

#footnotes {
  margin: 20px 0 20px 0;
  padding: 7px 0 0 0;
}

#footnotes div.footnote {
  margin: 0 0 5px 0;
}

#footnotes hr {
  border: none;
  border-top: 1px solid silver;
  height: 1px;
  text-align: left;
  margin-left: 0;
  width: 20%;
  min-width: 100px;
}

div.colist td {
  padding-right: 0.5em;
  padding-bottom: 0.3em;
  vertical-align: top;
}
div.colist td img {
  margin-top: 0.3em;
}

@media print {
  #footer-badges { display: none; }
}

#toc {
  margin-bottom: 2.5em;
}

#toctitle {
  color: #527bbd;
  font-size: 1.1em;
  font-weight: bold;
  margin-top: 1.0em;
  margin-bottom: 0.1em;
}

div.toclevel0, div.toclevel1, div.toclevel2, div.toclevel3, div.toclevel4 {
  margin-top: 0;
  margin-bottom: 0;
}
div.toclevel2 {
  margin-left: 2em;
  font-size: 0.9em;
}
div.toclevel3 {
  margin-left: 4em;
  font-size: 0.9em;
}
div.toclevel4 {
  margin-left: 6em;
  font-size: 0.9em;
}

span.aqua { color: aqua; }
span.black { color: black; }
span.blue { color: blue; }
span.fuchsia { color: fuchsia; }
span.gray { color: gray; }
span.green { color: green; }
span.lime { color: lime; }
span.maroon { color: maroon; }
span.navy { color: navy; }
span.olive { color: olive; }
span.purple { color: purple; }
span.red { color: red; }
span.silver { color: silver; }
span.teal { color: teal; }
span.white { color: white; }
span.yellow { color: yellow; }

span.aqua-background { background: aqua; }
span.black-background { background: black; }
span.blue-background { background: blue; }
span.fuchsia-background { background: fuchsia; }
span.gray-background { background: gray; }
span.green-background { background: green; }
span.lime-background { background: lime; }
span.maroon-background { background: maroon; }
span.navy-background { background: navy; }
span.olive-background { background: olive; }
span.purple-background { background: purple; }
span.red-background { background: red; }
span.silver-background { background: silver; }
span.teal-background { background: teal; }
span.white-background { background: white; }
span.yellow-background { background: yellow; }

span.big { font-size: 2em; }
span.small { font-size: 0.6em; }

span.underline { text-decoration: underline; }
span.overline { text-decoration: overline; }
span.line-through { text-decoration: line-through; }

div.unbreakable { page-break-inside: avoid; }


/*
 * xhtml11 specific
 *
 * */

div.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
div.tableblock > table {
  border: 3px solid #527bbd;
}
thead, p.table.header {
  font-weight: bold;
  color: #527bbd;
}
p.table {
  margin-top: 0;
}
/* Because the table frame attribute is overriden by CSS in most browsers. */
div.tableblock > table[frame="void"] {
  border-style: none;
}
div.tableblock > table[frame="hsides"] {
  border-left-style: none;
  border-right-style: none;
}
div.tableblock > table[frame="vsides"] {
  border-top-style: none;
  border-bottom-style: none;
}


/*
 * html5 specific
 *
 * */

table.tableblock {
  margin-top: 1.0em;
  margin-bottom: 1.5em;
}
thead, p.tableblock.header {
  font-weight: bold;
  color: #527bbd;
}
p.tableblock {
  margin-top: 0;
}
table.tableblock {
  border-width: 3px;
  border-spacing: 0px;
  border-style: solid;
  border-color: #527bbd;
  border-collapse: collapse;
}
th.tableblock, td.tableblock {
  border-width: 1px;
  padding: 4px;
  border-style: solid;
  border-color: #527bbd;
}

table.tableblock.frame-topbot {
  border-left-style: hidden;
  border-right-style: hidden;
}
table.tableblock.frame-sides {
  border-top-style: hidden;
  border-bottom-style: hidden;
}
table.tableblock.frame-none {
  border-style: hidden;
}

th.tableblock.halign-left, td.tableblock.halign-left {
  text-align: left;
}
th.tableblock.halign-center, td.tableblock.halign-center {
  text-align: center;
}
th.tableblock.halign-right, td.tableblock.halign-right {
  text-align: right;
}

th.tableblock.valign-top, td.tableblock.valign-top {
  vertical-align: top;
}
th.tableblock.valign-middle, td.tableblock.valign-middle {
  vertical-align: middle;
}
th.tableblock.valign-bottom, td.tableblock.valign-bottom {
  vertical-align: bottom;
}


/*
 * manpage specific
 *
 * */

body.manpage h1 {
  padding-top: 0.5em;
  padding-bottom: 0.5em;
  border-top: 2px solid silver;
  border-bottom: 2px solid silver;
}
body.manpage h2 {
  border-style: none;
}
body.manpage div.sectionbody {
  margin-left: 3em;
}

@media print {
  body.manpage div#toc { display: none; }
}


@media screen {
  body {
    max-width: 50em; /* approximately 80 characters wide */
    margin-left: 16em;
  }

  #toc {
    position: fixed;
    top: 0;
    left: 0;
    bottom: 0;
    width: 13em;
    padding: 0.5em;
    padding-bottom: 1.5em;
    margin: 0;
    overflow: auto;
    border-right: 3px solid #f8f8f8;
    background-color: white;
  }

  #toc .toclevel1 {
    margin-top: 0.5em;
  }

  #toc .toclevel2 {
    margin-top: 0.25em;
    display: list-item;
    color: #aaaaaa;
  }

  #toctitle {
    margin-top: 0.5em;
  }
}
</style>
<script type="text/javascript">
/*<![CDATA[*/
var asciidoc = {  // Namespace.

/////////////////////////////////////////////////////////////////////
// Table Of Contents generator
/////////////////////////////////////////////////////////////////////

/* Author: Mihai Bazon, September 2002
 * http://students.infoiasi.ro/~mishoo
 *
 * Table Of Content generator
 * Version: 0.4
 *
 * Feel free to use this script under the terms of the GNU General Public
 * License, as long as you do not remove or alter this notice.
 */

 /* modified by Troy D. Hanson, September 2006. License: GPL */
 /* modified by Stuart Rackham, 2006, 2009. License: GPL */

// toclevels = 1..4.
toc: function (toclevels) {

  function getText(el) {
    var text = "";
    for (var i = el.firstChild; i != null; i = i.nextSibling) {
      if (i.nodeType == 3 /* Node.TEXT_NODE */) // IE doesn't speak constants.
        text += i.data;
      else if (i.firstChild != null)
        text += getText(i);
    }
    return text;
  }

  function TocEntry(el, text, toclevel) {
    this.element = el;
    this.text = text;
    this.toclevel = toclevel;
  }

  function tocEntries(el, toclevels) {
    var result = new Array;
    var re = new RegExp('[hH]([1-'+(toclevels+1)+'])');
    // Function that scans the DOM tree for header elements (the DOM2
    // nodeIterator API would be a better technique but not supported by all
    // browsers).
    var iterate = function (el) {
      for (var i = el.firstChild; i != null; i = i.nextSibling) {
        if (i.nodeType == 1 /* Node.ELEMENT_NODE */) {
          var mo = re.exec(i.tagName);
          if (mo && (i.getAttribute("class") || i.getAttribute("className")) != "float") {
            result[result.length] = new TocEntry(i, getText(i), mo[1]-1);
          }
          iterate(i);
        }
      }
    }
    iterate(el);
    return result;
  }

  var toc = document.getElementById("toc");
  if (!toc) {
    return;
  }

  // Delete existing TOC entries in case we're reloading the TOC.
  var tocEntriesToRemove = [];
  var i;
  for (i = 0; i < toc.childNodes.length; i++) {
    var entry = toc.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div'
     && entry.getAttribute("class")
     && entry.getAttribute("class").match(/^toclevel/))
      tocEntriesToRemove.push(entry);
  }
  for (i = 0; i < tocEntriesToRemove.length; i++) {
    toc.removeChild(tocEntriesToRemove[i]);
  }

  // Rebuild TOC entries.
  var entries = tocEntries(document.getElementById("content"), toclevels);
  for (var i = 0; i < entries.length; ++i) {
    var entry = entries[i];
    if (entry.element.id == "")
      entry.element.id = "_toc_" + i;
    var a = document.createElement("a");
    a.href = "#" + entry.element.id;
    a.appendChild(document.createTextNode(entry.text));
    var div = document.createElement("div");
    div.appendChild(a);
    div.className = "toclevel" + entry.toclevel;
    toc.appendChild(div);
  }
  if (entries.length == 0)
    toc.parentNode.removeChild(toc);
},


/////////////////////////////////////////////////////////////////////
// Footnotes generator
/////////////////////////////////////////////////////////////////////

/* Based on footnote generation code from:
 * http://www.brandspankingnew.net/archive/2005/07/format_footnote.html
 */

footnotes: function () {
  // Delete existing footnote entries in case we're reloading the footnodes.
  var i;
  var noteholder = document.getElementById("footnotes");
  if (!noteholder) {
    return;
  }
  var entriesToRemove = [];
  for (i = 0; i < noteholder.childNodes.length; i++) {
    var entry = noteholder.childNodes[i];
    if (entry.nodeName.toLowerCase() == 'div' && entry.getAttribute("class") == "footnote")
      entriesToRemove.push(entry);
  }
  for (i = 0; i < entriesToRemove.length; i++) {
    noteholder.removeChild(entriesToRemove[i]);
  }

  // Rebuild footnote entries.
  var cont = document.getElementById("content");
  var spans = cont.getElementsByTagName("span");
  var refs = {};
  var n = 0;
  for (i=0; i<spans.length; i++) {
    if (spans[i].className == "footnote") {
      n++;
      var note = spans[i].getAttribute("data-note");
      if (!note) {
        // Use [\s\S] in place of . so multi-line matches work.
        // Because JavaScript has no s (dotall) regex flag.
        note = spans[i].innerHTML.match(/\s*\[([\s\S]*)]\s*/)[1];
        spans[i].innerHTML =
          "[<a id='_footnoteref_" + n + "' href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
        spans[i].setAttribute("data-note", note);
      }
      noteholder.innerHTML +=
        "<div class='footnote' id='_footnote_" + n + "'>" +
        "<a href='#_footnoteref_" + n + "' title='Return to text'>" +
        n + "</a>. " + note + "</div>";
      var id =spans[i].getAttribute("id");
      if (id != null) refs["#"+id] = n;
    }
  }
  if (n == 0)
    noteholder.parentNode.removeChild(noteholder);
  else {
    // Process footnoterefs.
    for (i=0; i<spans.length; i++) {
      if (spans[i].className == "footnoteref") {
        var href = spans[i].getElementsByTagName("a")[0].getAttribute("href");
        href = href.match(/#.*/)[0];  // Because IE return full URL.
        n = refs[href];
        spans[i].innerHTML =
          "[<a href='#_footnote_" + n +
          "' title='View footnote' class='footnote'>" + n + "</a>]";
      }
    }
  }
},

install: function(toclevels) {
  var timerId;

  function reinstall() {
    asciidoc.footnotes();
    if (toclevels) {
      asciidoc.toc(toclevels);
    }
  }

  function reinstallAndRemoveTimer() {
    clearInterval(timerId);
    reinstall();
  }

  timerId = setInterval(reinstall, 500);
  if (document.addEventListener)
    document.addEventListener("DOMContentLoaded", reinstallAndRemoveTimer, false);
  else
    window.onload = reinstallAndRemoveTimer;
}

}
asciidoc.install(2);
/*]]>*/
</script>
<script type="text/javascript">
/*<![CDATA[*/
/*
LaTeXMathML.js
==============

This file, in this form, is due to Douglas Woodall, June 2006.
It contains JavaScript functions to convert (most simple) LaTeX
math notation to Presentation MathML.  It was obtained by
downloading the file ASCIIMathML.js from
	http://www1.chapman.edu/~jipsen/mathml/asciimathdownload/
and modifying it so that it carries out ONLY those conversions
that would be carried out in LaTeX.  A description of the original
file, with examples, can be found at
	www1.chapman.edu/~jipsen/mathml/asciimath.html
	ASCIIMathML: Math on the web for everyone

Here is the header notice from the original file:

ASCIIMathML.js
==============
This file contains JavaScript functions to convert ASCII math notation
to Presentation MathML. The conversion is done while the (X)HTML page
loads, and should work with Firefox/Mozilla/Netscape 7+ and Internet
Explorer 6+MathPlayer (http://www.dessci.com/en/products/mathplayer/).
Just add the next line to your (X)HTML page with this file in the same folder:
This is a convenient and inexpensive solution for authoring MathML.

Version 1.4.7 Dec 15, 2005, (c) Peter Jipsen http://www.chapman.edu/~jipsen
Latest version at http://www.chapman.edu/~jipsen/mathml/ASCIIMathML.js
For changes see http://www.chapman.edu/~jipsen/mathml/asciimathchanges.txt
If you use it on a webpage, please send the URL to jipsen@chapman.edu

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or (at
your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
General Public License (at http://www.gnu.org/copyleft/gpl.html)
for more details.

LaTeXMathML.js (ctd)
==============

The instructions for use are the same as for the original
ASCIIMathML.js, except that of course the line you add to your
file should be
Or use absolute path names if the file is not in the same folder
as your (X)HTML page.
*/

var checkForMathML = true;   // check if browser can display MathML
var notifyIfNoMathML = true; // display note if no MathML capability
var alertIfNoMathML = false;  // show alert box if no MathML capability
// was "red":
var mathcolor = "";	     // change it to "" (to inherit) or any other color
// was "serif":
var mathfontfamily = "";      // change to "" to inherit (works in IE)
                              // or another family (e.g. "arial")
var showasciiformulaonhover = true; // helps students learn ASCIIMath
/*
// Commented out by DRW -- not now used -- see DELIMITERS (twice) near the end
var displaystyle = false;     // puts limits above and below large operators
var decimalsign = ".";        // change to "," if you like, beware of `(1,2)`!
var AMdelimiter1 = "`", AMescape1 = "\\\\`"; // can use other characters
var AMdelimiter2 = "$", AMescape2 = "\\\\\\$", AMdelimiter2regexp = "\\$";
var doubleblankmathdelimiter = false; // if true,  x+1  is equal to `x+1`
                                      // for IE this works only in <!--   -->
//var separatetokens;// has been removed (email me if this is a problem)
*/
var isIE = document.createElementNS==null;

if (document.getElementById==null)
  alert("This webpage requires a recent browser such as\
\nMozilla/Netscape 7+ or Internet Explorer 6+MathPlayer")

// all further global variables start with "AM"

function AMcreateElementXHTML(t) {
  if (isIE) return document.createElement(t);
  else return document.createElementNS("http://www.w3.org/1999/xhtml",t);
}

function AMnoMathMLNote() {
  var nd = AMcreateElementXHTML("h3");
  nd.setAttribute("align","center")
  nd.appendChild(AMcreateElementXHTML("p"));
  nd.appendChild(document.createTextNode("To view the "));
  var an = AMcreateElementXHTML("a");
  an.appendChild(document.createTextNode("LaTeXMathML"));
  an.setAttribute("href","http://www.maths.nott.ac.uk/personal/drw/lm.html");
  nd.appendChild(an);
  nd.appendChild(document.createTextNode(" notation use Internet Explorer 6+"));
  an = AMcreateElementXHTML("a");
  an.appendChild(document.createTextNode("MathPlayer"));
  an.setAttribute("href","http://www.dessci.com/en/products/mathplayer/download.htm");
  nd.appendChild(an);
  nd.appendChild(document.createTextNode(" or Netscape/Mozilla/Firefox"));
  nd.appendChild(AMcreateElementXHTML("p"));
  return nd;
}

function AMisMathMLavailable() {
  if (navigator.appName.slice(0,8)=="Netscape")
    if (navigator.appVersion.slice(0,1)>="5") return null;
    else return AMnoMathMLNote();
  else if (navigator.appName.slice(0,9)=="Microsoft")
    try {
        var ActiveX = new ActiveXObject("MathPlayer.Factory.1");
        return null;
    } catch (e) {
        return AMnoMathMLNote();
    }
  else return AMnoMathMLNote();
}

// character lists for Mozilla/Netscape fonts
var AMcal = [0xEF35,0x212C,0xEF36,0xEF37,0x2130,0x2131,0xEF38,0x210B,0x2110,0xEF39,0xEF3A,0x2112,0x2133,0xEF3B,0xEF3C,0xEF3D,0xEF3E,0x211B,0xEF3F,0xEF40,0xEF41,0xEF42,0xEF43,0xEF44,0xEF45,0xEF46];
var AMfrk = [0xEF5D,0xEF5E,0x212D,0xEF5F,0xEF60,0xEF61,0xEF62,0x210C,0x2111,0xEF63,0xEF64,0xEF65,0xEF66,0xEF67,0xEF68,0xEF69,0xEF6A,0x211C,0xEF6B,0xEF6C,0xEF6D,0xEF6E,0xEF6F,0xEF70,0xEF71,0x2128];
var AMbbb = [0xEF8C,0xEF8D,0x2102,0xEF8E,0xEF8F,0xEF90,0xEF91,0x210D,0xEF92,0xEF93,0xEF94,0xEF95,0xEF96,0x2115,0xEF97,0x2119,0x211A,0x211D,0xEF98,0xEF99,0xEF9A,0xEF9B,0xEF9C,0xEF9D,0xEF9E,0x2124];

var CONST = 0, UNARY = 1, BINARY = 2, INFIX = 3, LEFTBRACKET = 4,
    RIGHTBRACKET = 5, SPACE = 6, UNDEROVER = 7, DEFINITION = 8,
    TEXT = 9, BIG = 10, LONG = 11, STRETCHY = 12, MATRIX = 13; // token types

var AMsqrt = {input:"\\sqrt",	tag:"msqrt", output:"sqrt",	ttype:UNARY},
  AMroot = {input:"\\root",	tag:"mroot", output:"root",	ttype:BINARY},
  AMfrac = {input:"\\frac",	tag:"mfrac", output:"/",	ttype:BINARY},
  AMover = {input:"\\stackrel", tag:"mover", output:"stackrel", ttype:BINARY},
  AMatop = {input:"\\atop",	tag:"mfrac", output:"",		ttype:INFIX},
  AMchoose = {input:"\\choose", tag:"mfrac", output:"",		ttype:INFIX},
  AMsub  = {input:"_",		tag:"msub",  output:"_",	ttype:INFIX},
  AMsup  = {input:"^",		tag:"msup",  output:"^",	ttype:INFIX},
  AMtext = {input:"\\mathrm",	tag:"mtext", output:"text",	ttype:TEXT},
  AMmbox = {input:"\\mbox",	tag:"mtext", output:"mbox",	ttype:TEXT};

// Commented out by DRW to prevent 1/2 turning into a 2-line fraction
// AMdiv   = {input:"/",	 tag:"mfrac", output:"/",    ttype:INFIX},
// Commented out by DRW so that " prints literally in equations
// AMquote = {input:"\"",	 tag:"mtext", output:"mbox", ttype:TEXT};

var AMsymbols = [
//Greek letters
{input:"\\alpha",	tag:"mi", output:"\u03B1", ttype:CONST},
{input:"\\beta",	tag:"mi", output:"\u03B2", ttype:CONST},
{input:"\\gamma",	tag:"mi", output:"\u03B3", ttype:CONST},
{input:"\\delta",	tag:"mi", output:"\u03B4", ttype:CONST},
{input:"\\epsilon",	tag:"mi", output:"\u03B5", ttype:CONST},
{input:"\\varepsilon",  tag:"mi", output:"\u025B", ttype:CONST},
{input:"\\zeta",	tag:"mi", output:"\u03B6", ttype:CONST},
{input:"\\eta",		tag:"mi", output:"\u03B7", ttype:CONST},
{input:"\\theta",	tag:"mi", output:"\u03B8", ttype:CONST},
{input:"\\vartheta",	tag:"mi", output:"\u03D1", ttype:CONST},
{input:"\\iota",	tag:"mi", output:"\u03B9", ttype:CONST},
{input:"\\kappa",	tag:"mi", output:"\u03BA", ttype:CONST},
{input:"\\lambda",	tag:"mi", output:"\u03BB", ttype:CONST},
{input:"\\mu",		tag:"mi", output:"\u03BC", ttype:CONST},
{input:"\\nu",		tag:"mi", output:"\u03BD", ttype:CONST},
{input:"\\xi",		tag:"mi", output:"\u03BE", ttype:CONST},
{input:"\\pi",		tag:"mi", output:"\u03C0", ttype:CONST},
{input:"\\varpi",	tag:"mi", output:"\u03D6", ttype:CONST},
{input:"\\rho",		tag:"mi", output:"\u03C1", ttype:CONST},
{input:"\\varrho",	tag:"mi", output:"\u03F1", ttype:CONST},
{input:"\\varsigma",	tag:"mi", output:"\u03C2", ttype:CONST},
{input:"\\sigma",	tag:"mi", output:"\u03C3", ttype:CONST},
{input:"\\tau",		tag:"mi", output:"\u03C4", ttype:CONST},
{input:"\\upsilon",	tag:"mi", output:"\u03C5", ttype:CONST},
{input:"\\phi",		tag:"mi", output:"\u03C6", ttype:CONST},
{input:"\\varphi",	tag:"mi", output:"\u03D5", ttype:CONST},
{input:"\\chi",		tag:"mi", output:"\u03C7", ttype:CONST},
{input:"\\psi",		tag:"mi", output:"\u03C8", ttype:CONST},
{input:"\\omega",	tag:"mi", output:"\u03C9", ttype:CONST},
{input:"\\Gamma",	tag:"mo", output:"\u0393", ttype:CONST},
{input:"\\Delta",	tag:"mo", output:"\u0394", ttype:CONST},
{input:"\\Theta",	tag:"mo", output:"\u0398", ttype:CONST},
{input:"\\Lambda",	tag:"mo", output:"\u039B", ttype:CONST},
{input:"\\Xi",		tag:"mo", output:"\u039E", ttype:CONST},
{input:"\\Pi",		tag:"mo", output:"\u03A0", ttype:CONST},
{input:"\\Sigma",	tag:"mo", output:"\u03A3", ttype:CONST},
{input:"\\Upsilon",	tag:"mo", output:"\u03A5", ttype:CONST},
{input:"\\Phi",		tag:"mo", output:"\u03A6", ttype:CONST},
{input:"\\Psi",		tag:"mo", output:"\u03A8", ttype:CONST},
{input:"\\Omega",	tag:"mo", output:"\u03A9", ttype:CONST},

//fractions
{input:"\\frac12",	tag:"mo", output:"\u00BD", ttype:CONST},
{input:"\\frac14",	tag:"mo", output:"\u00BC", ttype:CONST},
{input:"\\frac34",	tag:"mo", output:"\u00BE", ttype:CONST},
{input:"\\frac13",	tag:"mo", output:"\u2153", ttype:CONST},
{input:"\\frac23",	tag:"mo", output:"\u2154", ttype:CONST},
{input:"\\frac15",	tag:"mo", output:"\u2155", ttype:CONST},
{input:"\\frac25",	tag:"mo", output:"\u2156", ttype:CONST},
{input:"\\frac35",	tag:"mo", output:"\u2157", ttype:CONST},
{input:"\\frac45",	tag:"mo", output:"\u2158", ttype:CONST},
{input:"\\frac16",	tag:"mo", output:"\u2159", ttype:CONST},
{input:"\\frac56",	tag:"mo", output:"\u215A", ttype:CONST},
{input:"\\frac18",	tag:"mo", output:"\u215B", ttype:CONST},
{input:"\\frac38",	tag:"mo", output:"\u215C", ttype:CONST},
{input:"\\frac58",	tag:"mo", output:"\u215D", ttype:CONST},
{input:"\\frac78",	tag:"mo", output:"\u215E", ttype:CONST},

//binary operation symbols
{input:"\\pm",		tag:"mo", output:"\u00B1", ttype:CONST},
{input:"\\mp",		tag:"mo", output:"\u2213", ttype:CONST},
{input:"\\triangleleft",tag:"mo", output:"\u22B2", ttype:CONST},
{input:"\\triangleright",tag:"mo",output:"\u22B3", ttype:CONST},
{input:"\\cdot",	tag:"mo", output:"\u22C5", ttype:CONST},
{input:"\\star",	tag:"mo", output:"\u22C6", ttype:CONST},
{input:"\\ast",		tag:"mo", output:"\u002A", ttype:CONST},
{input:"\\times",	tag:"mo", output:"\u00D7", ttype:CONST},
{input:"\\div",		tag:"mo", output:"\u00F7", ttype:CONST},
{input:"\\circ",	tag:"mo", output:"\u2218", ttype:CONST},
//{input:"\\bullet",	  tag:"mo", output:"\u2219", ttype:CONST},
{input:"\\bullet",	tag:"mo", output:"\u2022", ttype:CONST},
{input:"\\oplus",	tag:"mo", output:"\u2295", ttype:CONST},
{input:"\\ominus",	tag:"mo", output:"\u2296", ttype:CONST},
{input:"\\otimes",	tag:"mo", output:"\u2297", ttype:CONST},
{input:"\\bigcirc",	tag:"mo", output:"\u25CB", ttype:CONST},
{input:"\\oslash",	tag:"mo", output:"\u2298", ttype:CONST},
{input:"\\odot",	tag:"mo", output:"\u2299", ttype:CONST},
{input:"\\land",	tag:"mo", output:"\u2227", ttype:CONST},
{input:"\\wedge",	tag:"mo", output:"\u2227", ttype:CONST},
{input:"\\lor",		tag:"mo", output:"\u2228", ttype:CONST},
{input:"\\vee",		tag:"mo", output:"\u2228", ttype:CONST},
{input:"\\cap",		tag:"mo", output:"\u2229", ttype:CONST},
{input:"\\cup",		tag:"mo", output:"\u222A", ttype:CONST},
{input:"\\sqcap",	tag:"mo", output:"\u2293", ttype:CONST},
{input:"\\sqcup",	tag:"mo", output:"\u2294", ttype:CONST},
{input:"\\uplus",	tag:"mo", output:"\u228E", ttype:CONST},
{input:"\\amalg",	tag:"mo", output:"\u2210", ttype:CONST},
{input:"\\bigtriangleup",tag:"mo",output:"\u25B3", ttype:CONST},
{input:"\\bigtriangledown",tag:"mo",output:"\u25BD", ttype:CONST},
{input:"\\dag",		tag:"mo", output:"\u2020", ttype:CONST},
{input:"\\dagger",	tag:"mo", output:"\u2020", ttype:CONST},
{input:"\\ddag",	tag:"mo", output:"\u2021", ttype:CONST},
{input:"\\ddagger",	tag:"mo", output:"\u2021", ttype:CONST},
{input:"\\lhd",		tag:"mo", output:"\u22B2", ttype:CONST},
{input:"\\rhd",		tag:"mo", output:"\u22B3", ttype:CONST},
{input:"\\unlhd",	tag:"mo", output:"\u22B4", ttype:CONST},
{input:"\\unrhd",	tag:"mo", output:"\u22B5", ttype:CONST},


//BIG Operators
{input:"\\sum",		tag:"mo", output:"\u2211", ttype:UNDEROVER},
{input:"\\prod",	tag:"mo", output:"\u220F", ttype:UNDEROVER},
{input:"\\bigcap",	tag:"mo", output:"\u22C2", ttype:UNDEROVER},
{input:"\\bigcup",	tag:"mo", output:"\u22C3", ttype:UNDEROVER},
{input:"\\bigwedge",	tag:"mo", output:"\u22C0", ttype:UNDEROVER},
{input:"\\bigvee",	tag:"mo", output:"\u22C1", ttype:UNDEROVER},
{input:"\\bigsqcap",	tag:"mo", output:"\u2A05", ttype:UNDEROVER},
{input:"\\bigsqcup",	tag:"mo", output:"\u2A06", ttype:UNDEROVER},
{input:"\\coprod",	tag:"mo", output:"\u2210", ttype:UNDEROVER},
{input:"\\bigoplus",	tag:"mo", output:"\u2A01", ttype:UNDEROVER},
{input:"\\bigotimes",	tag:"mo", output:"\u2A02", ttype:UNDEROVER},
{input:"\\bigodot",	tag:"mo", output:"\u2A00", ttype:UNDEROVER},
{input:"\\biguplus",	tag:"mo", output:"\u2A04", ttype:UNDEROVER},
{input:"\\int",		tag:"mo", output:"\u222B", ttype:CONST},
{input:"\\oint",	tag:"mo", output:"\u222E", ttype:CONST},

//binary relation symbols
{input:":=",		tag:"mo", output:":=",	   ttype:CONST},
{input:"\\lt",		tag:"mo", output:"<",	   ttype:CONST},
{input:"\\gt",		tag:"mo", output:">",	   ttype:CONST},
{input:"\\ne",		tag:"mo", output:"\u2260", ttype:CONST},
{input:"\\neq",		tag:"mo", output:"\u2260", ttype:CONST},
{input:"\\le",		tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\leq",		tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\leqslant",	tag:"mo", output:"\u2264", ttype:CONST},
{input:"\\ge",		tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\geq",		tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\geqslant",	tag:"mo", output:"\u2265", ttype:CONST},
{input:"\\equiv",	tag:"mo", output:"\u2261", ttype:CONST},
{input:"\\ll",		tag:"mo", output:"\u226A", ttype:CONST},
{input:"\\gg",		tag:"mo", output:"\u226B", ttype:CONST},
{input:"\\doteq",	tag:"mo", output:"\u2250", ttype:CONST},
{input:"\\prec",	tag:"mo", output:"\u227A", ttype:CONST},
{input:"\\succ",	tag:"mo", output:"\u227B", ttype:CONST},
{input:"\\preceq",	tag:"mo", output:"\u227C", ttype:CONST},
{input:"\\succeq",	tag:"mo", output:"\u227D", ttype:CONST},
{input:"\\subset",	tag:"mo", output:"\u2282", ttype:CONST},
{input:"\\supset",	tag:"mo", output:"\u2283", ttype:CONST},
{input:"\\subseteq",	tag:"mo", output:"\u2286", ttype:CONST},
{input:"\\supseteq",	tag:"mo", output:"\u2287", ttype:CONST},
{input:"\\sqsubset",	tag:"mo", output:"\u228F", ttype:CONST},
{input:"\\sqsupset",	tag:"mo", output:"\u2290", ttype:CONST},
{input:"\\sqsubseteq",  tag:"mo", output:"\u2291", ttype:CONST},
{input:"\\sqsupseteq",  tag:"mo", output:"\u2292", ttype:CONST},
{input:"\\sim",		tag:"mo", output:"\u223C", ttype:CONST},
{input:"\\simeq",	tag:"mo", output:"\u2243", ttype:CONST},
{input:"\\approx",	tag:"mo", output:"\u2248", ttype:CONST},
{input:"\\cong",	tag:"mo", output:"\u2245", ttype:CONST},
{input:"\\Join",	tag:"mo", output:"\u22C8", ttype:CONST},
{input:"\\bowtie",	tag:"mo", output:"\u22C8", ttype:CONST},
{input:"\\in",		tag:"mo", output:"\u2208", ttype:CONST},
{input:"\\ni",		tag:"mo", output:"\u220B", ttype:CONST},
{input:"\\owns",	tag:"mo", output:"\u220B", ttype:CONST},
{input:"\\propto",	tag:"mo", output:"\u221D", ttype:CONST},
{input:"\\vdash",	tag:"mo", output:"\u22A2", ttype:CONST},
{input:"\\dashv",	tag:"mo", output:"\u22A3", ttype:CONST},
{input:"\\models",	tag:"mo", output:"\u22A8", ttype:CONST},
{input:"\\perp",	tag:"mo", output:"\u22A5", ttype:CONST},
{input:"\\smile",	tag:"mo", output:"\u2323", ttype:CONST},
{input:"\\frown",	tag:"mo", output:"\u2322", ttype:CONST},
{input:"\\asymp",	tag:"mo", output:"\u224D", ttype:CONST},
{input:"\\notin",	tag:"mo", output:"\u2209", ttype:CONST},

//matrices
{input:"\\begin{eqnarray}",	output:"X",	ttype:MATRIX, invisible:true},
{input:"\\begin{array}",	output:"X",	ttype:MATRIX, invisible:true},
{input:"\\\\",			output:"}&{",	ttype:DEFINITION},
{input:"\\end{eqnarray}",	output:"}}",	ttype:DEFINITION},
{input:"\\end{array}",		output:"}}",	ttype:DEFINITION},

//grouping and literal brackets -- ieval is for IE
{input:"\\big",	   tag:"mo", output:"X", atval:"1.2", ieval:"2.2", ttype:BIG},
{input:"\\Big",	   tag:"mo", output:"X", atval:"1.6", ieval:"2.6", ttype:BIG},
{input:"\\bigg",   tag:"mo", output:"X", atval:"2.2", ieval:"3.2", ttype:BIG},
{input:"\\Bigg",   tag:"mo", output:"X", atval:"2.9", ieval:"3.9", ttype:BIG},
{input:"\\left",   tag:"mo", output:"X", ttype:LEFTBRACKET},
{input:"\\right",  tag:"mo", output:"X", ttype:RIGHTBRACKET},
{input:"{",	   output:"{", ttype:LEFTBRACKET,  invisible:true},
{input:"}",	   output:"}", ttype:RIGHTBRACKET, invisible:true},

{input:"(",	   tag:"mo", output:"(",      atval:"1", ttype:STRETCHY},
{input:"[",	   tag:"mo", output:"[",      atval:"1", ttype:STRETCHY},
{input:"\\lbrack", tag:"mo", output:"[",      atval:"1", ttype:STRETCHY},
{input:"\\{",	   tag:"mo", output:"{",      atval:"1", ttype:STRETCHY},
{input:"\\lbrace", tag:"mo", output:"{",      atval:"1", ttype:STRETCHY},
{input:"\\langle", tag:"mo", output:"\u2329", atval:"1", ttype:STRETCHY},
{input:"\\lfloor", tag:"mo", output:"\u230A", atval:"1", ttype:STRETCHY},
{input:"\\lceil",  tag:"mo", output:"\u2308", atval:"1", ttype:STRETCHY},

// rtag:"mi" causes space to be inserted before a following sin, cos, etc.
// (see function AMparseExpr() )
{input:")",	  tag:"mo",output:")",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"]",	  tag:"mo",output:"]",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rbrack",tag:"mo",output:"]",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\}",	  tag:"mo",output:"}",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rbrace",tag:"mo",output:"}",	    rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rangle",tag:"mo",output:"\u232A", rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rfloor",tag:"mo",output:"\u230B", rtag:"mi",atval:"1",ttype:STRETCHY},
{input:"\\rceil", tag:"mo",output:"\u2309", rtag:"mi",atval:"1",ttype:STRETCHY},

// "|", "\\|", "\\vert" and "\\Vert" modified later: lspace = rspace = 0em
{input:"|",		tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\|",		tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"\\vert",	tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\Vert",	tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"\\mid",		tag:"mo", output:"\u2223", atval:"1", ttype:STRETCHY},
{input:"\\parallel",	tag:"mo", output:"\u2225", atval:"1", ttype:STRETCHY},
{input:"/",		tag:"mo", output:"/",	atval:"1.01", ttype:STRETCHY},
{input:"\\backslash",	tag:"mo", output:"\u2216", atval:"1", ttype:STRETCHY},
{input:"\\setminus",	tag:"mo", output:"\\",	   ttype:CONST},

//miscellaneous symbols
{input:"\\!",	  tag:"mspace", atname:"width", atval:"-0.167em", ttype:SPACE},
{input:"\\,",	  tag:"mspace", atname:"width", atval:"0.167em", ttype:SPACE},
{input:"\\>",	  tag:"mspace", atname:"width", atval:"0.222em", ttype:SPACE},
{input:"\\:",	  tag:"mspace", atname:"width", atval:"0.222em", ttype:SPACE},
{input:"\\;",	  tag:"mspace", atname:"width", atval:"0.278em", ttype:SPACE},
{input:"~",	  tag:"mspace", atname:"width", atval:"0.333em", ttype:SPACE},
{input:"\\quad",  tag:"mspace", atname:"width", atval:"1em", ttype:SPACE},
{input:"\\qquad", tag:"mspace", atname:"width", atval:"2em", ttype:SPACE},
//{input:"{}",		  tag:"mo", output:"\u200B", ttype:CONST}, // zero-width
{input:"\\prime",	tag:"mo", output:"\u2032", ttype:CONST},
{input:"'",		tag:"mo", output:"\u02B9", ttype:CONST},
{input:"''",		tag:"mo", output:"\u02BA", ttype:CONST},
{input:"'''",		tag:"mo", output:"\u2034", ttype:CONST},
{input:"''''",		tag:"mo", output:"\u2057", ttype:CONST},
{input:"\\ldots",	tag:"mo", output:"\u2026", ttype:CONST},
{input:"\\cdots",	tag:"mo", output:"\u22EF", ttype:CONST},
{input:"\\vdots",	tag:"mo", output:"\u22EE", ttype:CONST},
{input:"\\ddots",	tag:"mo", output:"\u22F1", ttype:CONST},
{input:"\\forall",	tag:"mo", output:"\u2200", ttype:CONST},
{input:"\\exists",	tag:"mo", output:"\u2203", ttype:CONST},
{input:"\\Re",		tag:"mo", output:"\u211C", ttype:CONST},
{input:"\\Im",		tag:"mo", output:"\u2111", ttype:CONST},
{input:"\\aleph",	tag:"mo", output:"\u2135", ttype:CONST},
{input:"\\hbar",	tag:"mo", output:"\u210F", ttype:CONST},
{input:"\\ell",		tag:"mo", output:"\u2113", ttype:CONST},
{input:"\\wp",		tag:"mo", output:"\u2118", ttype:CONST},
{input:"\\emptyset",	tag:"mo", output:"\u2205", ttype:CONST},
{input:"\\infty",	tag:"mo", output:"\u221E", ttype:CONST},
{input:"\\surd",	tag:"mo", output:"\\sqrt{}", ttype:DEFINITION},
{input:"\\partial",	tag:"mo", output:"\u2202", ttype:CONST},
{input:"\\nabla",	tag:"mo", output:"\u2207", ttype:CONST},
{input:"\\triangle",	tag:"mo", output:"\u25B3", ttype:CONST},
{input:"\\therefore",	tag:"mo", output:"\u2234", ttype:CONST},
{input:"\\angle",	tag:"mo", output:"\u2220", ttype:CONST},
//{input:"\\\\ ",	  tag:"mo", output:"\u00A0", ttype:CONST},
{input:"\\diamond",	tag:"mo", output:"\u22C4", ttype:CONST},
//{input:"\\Diamond",	  tag:"mo", output:"\u25CA", ttype:CONST},
{input:"\\Diamond",	tag:"mo", output:"\u25C7", ttype:CONST},
{input:"\\neg",		tag:"mo", output:"\u00AC", ttype:CONST},
{input:"\\lnot",	tag:"mo", output:"\u00AC", ttype:CONST},
{input:"\\bot",		tag:"mo", output:"\u22A5", ttype:CONST},
{input:"\\top",		tag:"mo", output:"\u22A4", ttype:CONST},
{input:"\\square",	tag:"mo", output:"\u25AB", ttype:CONST},
{input:"\\Box",		tag:"mo", output:"\u25A1", ttype:CONST},
{input:"\\wr",		tag:"mo", output:"\u2240", ttype:CONST},

//standard functions
//Note UNDEROVER *must* have tag:"mo" to work properly
{input:"\\arccos", tag:"mi", output:"arccos", ttype:UNARY, func:true},
{input:"\\arcsin", tag:"mi", output:"arcsin", ttype:UNARY, func:true},
{input:"\\arctan", tag:"mi", output:"arctan", ttype:UNARY, func:true},
{input:"\\arg",	   tag:"mi", output:"arg",    ttype:UNARY, func:true},
{input:"\\cos",	   tag:"mi", output:"cos",    ttype:UNARY, func:true},
{input:"\\cosh",   tag:"mi", output:"cosh",   ttype:UNARY, func:true},
{input:"\\cot",	   tag:"mi", output:"cot",    ttype:UNARY, func:true},
{input:"\\coth",   tag:"mi", output:"coth",   ttype:UNARY, func:true},
{input:"\\csc",	   tag:"mi", output:"csc",    ttype:UNARY, func:true},
{input:"\\deg",	   tag:"mi", output:"deg",    ttype:UNARY, func:true},
{input:"\\det",	   tag:"mi", output:"det",    ttype:UNARY, func:true},
{input:"\\dim",	   tag:"mi", output:"dim",    ttype:UNARY, func:true}, //CONST?
{input:"\\exp",	   tag:"mi", output:"exp",    ttype:UNARY, func:true},
{input:"\\gcd",	   tag:"mi", output:"gcd",    ttype:UNARY, func:true}, //CONST?
{input:"\\hom",	   tag:"mi", output:"hom",    ttype:UNARY, func:true},
{input:"\\inf",	      tag:"mo", output:"inf",	 ttype:UNDEROVER},
{input:"\\ker",	   tag:"mi", output:"ker",    ttype:UNARY, func:true},
{input:"\\lg",	   tag:"mi", output:"lg",     ttype:UNARY, func:true},
{input:"\\lim",	      tag:"mo", output:"lim",	 ttype:UNDEROVER},
{input:"\\liminf",    tag:"mo", output:"liminf", ttype:UNDEROVER},
{input:"\\limsup",    tag:"mo", output:"limsup", ttype:UNDEROVER},
{input:"\\ln",	   tag:"mi", output:"ln",     ttype:UNARY, func:true},
{input:"\\log",	   tag:"mi", output:"log",    ttype:UNARY, func:true},
{input:"\\max",	      tag:"mo", output:"max",	 ttype:UNDEROVER},
{input:"\\min",	      tag:"mo", output:"min",	 ttype:UNDEROVER},
{input:"\\Pr",	   tag:"mi", output:"Pr",     ttype:UNARY, func:true},
{input:"\\sec",	   tag:"mi", output:"sec",    ttype:UNARY, func:true},
{input:"\\sin",	   tag:"mi", output:"sin",    ttype:UNARY, func:true},
{input:"\\sinh",   tag:"mi", output:"sinh",   ttype:UNARY, func:true},
{input:"\\sup",	      tag:"mo", output:"sup",	 ttype:UNDEROVER},
{input:"\\tan",	   tag:"mi", output:"tan",    ttype:UNARY, func:true},
{input:"\\tanh",   tag:"mi", output:"tanh",   ttype:UNARY, func:true},

//arrows
{input:"\\gets",		tag:"mo", output:"\u2190", ttype:CONST},
{input:"\\leftarrow",		tag:"mo", output:"\u2190", ttype:CONST},
{input:"\\to",			tag:"mo", output:"\u2192", ttype:CONST},
{input:"\\rightarrow",		tag:"mo", output:"\u2192", ttype:CONST},
{input:"\\leftrightarrow",	tag:"mo", output:"\u2194", ttype:CONST},
{input:"\\uparrow",		tag:"mo", output:"\u2191", ttype:CONST},
{input:"\\downarrow",		tag:"mo", output:"\u2193", ttype:CONST},
{input:"\\updownarrow",		tag:"mo", output:"\u2195", ttype:CONST},
{input:"\\Leftarrow",		tag:"mo", output:"\u21D0", ttype:CONST},
{input:"\\Rightarrow",		tag:"mo", output:"\u21D2", ttype:CONST},
{input:"\\Leftrightarrow",	tag:"mo", output:"\u21D4", ttype:CONST},
{input:"\\iff", tag:"mo", output:"~\\Longleftrightarrow~", ttype:DEFINITION},
{input:"\\Uparrow",		tag:"mo", output:"\u21D1", ttype:CONST},
{input:"\\Downarrow",		tag:"mo", output:"\u21D3", ttype:CONST},
{input:"\\Updownarrow",		tag:"mo", output:"\u21D5", ttype:CONST},
{input:"\\mapsto",		tag:"mo", output:"\u21A6", ttype:CONST},
{input:"\\longleftarrow",	tag:"mo", output:"\u2190", ttype:LONG},
{input:"\\longrightarrow",	tag:"mo", output:"\u2192", ttype:LONG},
{input:"\\longleftrightarrow",	tag:"mo", output:"\u2194", ttype:LONG},
{input:"\\Longleftarrow",	tag:"mo", output:"\u21D0", ttype:LONG},
{input:"\\Longrightarrow",	tag:"mo", output:"\u21D2", ttype:LONG},
{input:"\\Longleftrightarrow",  tag:"mo", output:"\u21D4", ttype:LONG},
{input:"\\longmapsto",		tag:"mo", output:"\u21A6", ttype:CONST},
							// disaster if LONG

//commands with argument
AMsqrt, AMroot, AMfrac, AMover, AMsub, AMsup, AMtext, AMmbox, AMatop, AMchoose,
//AMdiv, AMquote,

//diacritical marks
{input:"\\acute",	tag:"mover",  output:"\u00B4", ttype:UNARY, acc:true},
//{input:"\\acute",	  tag:"mover",  output:"\u0317", ttype:UNARY, acc:true},
//{input:"\\acute",	  tag:"mover",  output:"\u0301", ttype:UNARY, acc:true},
//{input:"\\grave",	  tag:"mover",  output:"\u0300", ttype:UNARY, acc:true},
//{input:"\\grave",	  tag:"mover",  output:"\u0316", ttype:UNARY, acc:true},
{input:"\\grave",	tag:"mover",  output:"\u0060", ttype:UNARY, acc:true},
{input:"\\breve",	tag:"mover",  output:"\u02D8", ttype:UNARY, acc:true},
{input:"\\check",	tag:"mover",  output:"\u02C7", ttype:UNARY, acc:true},
{input:"\\dot",		tag:"mover",  output:".",      ttype:UNARY, acc:true},
{input:"\\ddot",	tag:"mover",  output:"..",     ttype:UNARY, acc:true},
//{input:"\\ddot",	  tag:"mover",  output:"\u00A8", ttype:UNARY, acc:true},
{input:"\\mathring",	tag:"mover",  output:"\u00B0", ttype:UNARY, acc:true},
{input:"\\vec",		tag:"mover",  output:"\u20D7", ttype:UNARY, acc:true},
{input:"\\overrightarrow",tag:"mover",output:"\u20D7", ttype:UNARY, acc:true},
{input:"\\overleftarrow",tag:"mover", output:"\u20D6", ttype:UNARY, acc:true},
{input:"\\hat",		tag:"mover",  output:"\u005E", ttype:UNARY, acc:true},
{input:"\\widehat",	tag:"mover",  output:"\u0302", ttype:UNARY, acc:true},
{input:"\\tilde",	tag:"mover",  output:"~",      ttype:UNARY, acc:true},
//{input:"\\tilde",	  tag:"mover",  output:"\u0303", ttype:UNARY, acc:true},
{input:"\\widetilde",	tag:"mover",  output:"\u02DC", ttype:UNARY, acc:true},
{input:"\\bar",		tag:"mover",  output:"\u203E", ttype:UNARY, acc:true},
{input:"\\overbrace",	tag:"mover",  output:"\u23B4", ttype:UNARY, acc:true},
{input:"\\overline",	tag:"mover",  output:"\u00AF", ttype:UNARY, acc:true},
{input:"\\underbrace",  tag:"munder", output:"\u23B5", ttype:UNARY, acc:true},
{input:"\\underline",	tag:"munder", output:"\u00AF", ttype:UNARY, acc:true},
//{input:"underline",	tag:"munder", output:"\u0332", ttype:UNARY, acc:true},

//typestyles and fonts
{input:"\\displaystyle",tag:"mstyle",atname:"displaystyle",atval:"true", ttype:UNARY},
{input:"\\textstyle",tag:"mstyle",atname:"displaystyle",atval:"false", ttype:UNARY},
{input:"\\scriptstyle",tag:"mstyle",atname:"scriptlevel",atval:"1", ttype:UNARY},
{input:"\\scriptscriptstyle",tag:"mstyle",atname:"scriptlevel",atval:"2", ttype:UNARY},
{input:"\\textrm", tag:"mstyle", output:"\\mathrm", ttype: DEFINITION},
{input:"\\mathbf", tag:"mstyle", atname:"mathvariant", atval:"bold", ttype:UNARY},
{input:"\\textbf", tag:"mstyle", atname:"mathvariant", atval:"bold", ttype:UNARY},
{input:"\\mathit", tag:"mstyle", atname:"mathvariant", atval:"italic", ttype:UNARY},
{input:"\\textit", tag:"mstyle", atname:"mathvariant", atval:"italic", ttype:UNARY},
{input:"\\mathtt", tag:"mstyle", atname:"mathvariant", atval:"monospace", ttype:UNARY},
{input:"\\texttt", tag:"mstyle", atname:"mathvariant", atval:"monospace", ttype:UNARY},
{input:"\\mathsf", tag:"mstyle", atname:"mathvariant", atval:"sans-serif", ttype:UNARY},
{input:"\\mathbb", tag:"mstyle", atname:"mathvariant", atval:"double-struck", ttype:UNARY, codes:AMbbb},
{input:"\\mathcal",tag:"mstyle", atname:"mathvariant", atval:"script", ttype:UNARY, codes:AMcal},
{input:"\\mathfrak",tag:"mstyle",atname:"mathvariant", atval:"fraktur",ttype:UNARY, codes:AMfrk}
];

function compareNames(s1,s2) {
  if (s1.input > s2.input) return 1
  else return -1;
}

var AMnames = []; //list of input symbols

function AMinitSymbols() {
  AMsymbols.sort(compareNames);
  for (i=0; i<AMsymbols.length; i++) AMnames[i] = AMsymbols[i].input;
}

var AMmathml = "http://www.w3.org/1998/Math/MathML";

function AMcreateElementMathML(t) {
  if (isIE) return document.createElement("m:"+t);
  else return document.createElementNS(AMmathml,t);
}

function AMcreateMmlNode(t,frag) {
//  var node = AMcreateElementMathML(name);
  if (isIE) var node = document.createElement("m:"+t);
  else var node = document.createElementNS(AMmathml,t);
  node.appendChild(frag);
  return node;
}

function newcommand(oldstr,newstr) {
  AMsymbols = AMsymbols.concat([{input:oldstr, tag:"mo", output:newstr,
                                 ttype:DEFINITION}]);
}

function AMremoveCharsAndBlanks(str,n) {
//remove n characters and any following blanks
  var st;
  st = str.slice(n);
  for (var i=0; i<st.length && st.charCodeAt(i)<=32; i=i+1);
  return st.slice(i);
}

function AMposition(arr, str, n) {
// return position >=n where str appears or would be inserted
// assumes arr is sorted
  if (n==0) {
    var h,m;
    n = -1;
    h = arr.length;
    while (n+1<h) {
      m = (n+h) >> 1;
      if (arr[m]<str) n = m; else h = m;
    }
    return h;
  } else
    for (var i=n; i<arr.length && arr[i]<str; i++);
  return i; // i=arr.length || arr[i]>=str
}

function AMgetSymbol(str) {
//return maximal initial substring of str that appears in names
//return null if there is none
  var k = 0; //new pos
  var j = 0; //old pos
  var mk; //match pos
  var st;
  var tagst;
  var match = "";
  var more = true;
  for (var i=1; i<=str.length && more; i++) {
    st = str.slice(0,i); //initial substring of length i
    j = k;
    k = AMposition(AMnames, st, j);
    if (k<AMnames.length && str.slice(0,AMnames[k].length)==AMnames[k]){
      match = AMnames[k];
      mk = k;
      i = match.length;
    }
    more = k<AMnames.length && str.slice(0,AMnames[k].length)>=AMnames[k];
  }
  AMpreviousSymbol=AMcurrentSymbol;
  if (match!=""){
    AMcurrentSymbol=AMsymbols[mk].ttype;
    return AMsymbols[mk];
  }
  AMcurrentSymbol=CONST;
  k = 1;
  st = str.slice(0,1); //take 1 character
  if ("0"<=st && st<="9") tagst = "mn";
  else tagst = (("A">st || st>"Z") && ("a">st || st>"z")?"mo":"mi");
/*
// Commented out by DRW (not fully understood, but probably to do with
// use of "/" as an INFIX version of "\\frac", which we don't want):
//}
//if (st=="-" && AMpreviousSymbol==INFIX) {
//  AMcurrentSymbol = INFIX;  //trick "/" into recognizing "-" on second parse
//  return {input:st, tag:tagst, output:st, ttype:UNARY, func:true};
//}
*/
  return {input:st, tag:tagst, output:st, ttype:CONST};
}


/*Parsing ASCII math expressions with the following grammar
v ::= [A-Za-z] | greek letters | numbers | other constant symbols
u ::= sqrt | text | bb | other unary symbols for font commands
b ::= frac | root | stackrel	binary symbols
l ::= { | \left			left brackets
r ::= } | \right		right brackets
S ::= v | lEr | uS | bSS	Simple expression
I ::= S_S | S^S | S_S^S | S	Intermediate expression
E ::= IE | I/I			Expression
Each terminal symbol is translated into a corresponding mathml node.*/

var AMpreviousSymbol,AMcurrentSymbol;

function AMparseSexpr(str) { //parses str and returns [node,tailstr,(node)tag]
  var symbol, node, result, result2, i, st,// rightvert = false,
    newFrag = document.createDocumentFragment();
  str = AMremoveCharsAndBlanks(str,0);
  symbol = AMgetSymbol(str);             //either a token or a bracket or empty
  if (symbol == null || symbol.ttype == RIGHTBRACKET)
    return [null,str,null];
  if (symbol.ttype == DEFINITION) {
    str = symbol.output+AMremoveCharsAndBlanks(str,symbol.input.length);
    symbol = AMgetSymbol(str);
    if (symbol == null || symbol.ttype == RIGHTBRACKET)
      return [null,str,null];
  }
  str = AMremoveCharsAndBlanks(str,symbol.input.length);
  switch (symbol.ttype) {
  case SPACE:
    node = AMcreateElementMathML(symbol.tag);
    node.setAttribute(symbol.atname,symbol.atval);
    return [node,str,symbol.tag];
  case UNDEROVER:
    if (isIE) {
      if (symbol.input.substr(0,4) == "\\big") {   // botch for missing symbols
	str = "\\"+symbol.input.substr(4)+str;	   // make \bigcup = \cup etc.
	symbol = AMgetSymbol(str);
	symbol.ttype = UNDEROVER;
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
      }
    }
    return [AMcreateMmlNode(symbol.tag,
			document.createTextNode(symbol.output)),str,symbol.tag];
  case CONST:
    var output = symbol.output;
    if (isIE) {
      if (symbol.input == "'")
	output = "\u2032";
      else if (symbol.input == "''")
	output = "\u2033";
      else if (symbol.input == "'''")
	output = "\u2033\u2032";
      else if (symbol.input == "''''")
	output = "\u2033\u2033";
      else if (symbol.input == "\\square")
	output = "\u25A1";	// same as \Box
      else if (symbol.input.substr(0,5) == "\\frac") {
						// botch for missing fractions
	var denom = symbol.input.substr(6,1);
	if (denom == "5" || denom == "6") {
	  str = symbol.input.replace(/\\frac/,"\\frac ")+str;
	  return [node,str,symbol.tag];
	}
      }
    }
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(output));
    return [node,str,symbol.tag];
  case LONG:  // added by DRW
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    node.setAttribute("minsize","1.5");
    node.setAttribute("maxsize","1.5");
    node = AMcreateMmlNode("mover",node);
    node.appendChild(AMcreateElementMathML("mspace"));
    return [node,str,symbol.tag];
  case STRETCHY:  // added by DRW
    if (isIE && symbol.input == "\\backslash")
	symbol.output = "\\";	// doesn't expand, but then nor does "\u2216"
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    if (symbol.input == "|" || symbol.input == "\\vert" ||
	symbol.input == "\\|" || symbol.input == "\\Vert") {
	  node.setAttribute("lspace","0em");
	  node.setAttribute("rspace","0em");
    }
    node.setAttribute("maxsize",symbol.atval);  // don't allow to stretch here
    if (symbol.rtag != null)
      return [node,str,symbol.rtag];
    else
      return [node,str,symbol.tag];
  case BIG:  // added by DRW
    var atval = symbol.atval;
    if (isIE)
      atval = symbol.ieval;
    symbol = AMgetSymbol(str);
    if (symbol == null)
	return [null,str,null];
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    node = AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output));
    if (isIE) {		// to get brackets to expand
      var space = AMcreateElementMathML("mspace");
      space.setAttribute("height",atval+"ex");
      node = AMcreateMmlNode("mrow",node);
      node.appendChild(space);
    } else {		// ignored in IE
      node.setAttribute("minsize",atval);
      node.setAttribute("maxsize",atval);
    }
    return [node,str,symbol.tag];
  case LEFTBRACKET:   //read (expr+)
    if (symbol.input == "\\left") { // left what?
      symbol = AMgetSymbol(str);
      if (symbol != null) {
	if (symbol.input == ".")
	  symbol.invisible = true;
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
      }
    }
    result = AMparseExpr(str,true,false);
    if (symbol==null ||
	(typeof symbol.invisible == "boolean" && symbol.invisible))
      node = AMcreateMmlNode("mrow",result[0]);
    else {
      node = AMcreateMmlNode("mo",document.createTextNode(symbol.output));
      node = AMcreateMmlNode("mrow",node);
      node.appendChild(result[0]);
    }
    return [node,result[1],result[2]];
  case MATRIX:	 //read (expr+)
    if (symbol.input == "\\begin{array}") {
      var mask = "";
      symbol = AMgetSymbol(str);
      str = AMremoveCharsAndBlanks(str,0);
      if (symbol == null)
	mask = "l";
      else {
	str = AMremoveCharsAndBlanks(str,symbol.input.length);
	if (symbol.input != "{")
	  mask = "l";
	else do {
	  symbol = AMgetSymbol(str);
	  if (symbol != null) {
	    str = AMremoveCharsAndBlanks(str,symbol.input.length);
	    if (symbol.input != "}")
	      mask = mask+symbol.input;
	  }
	} while (symbol != null && symbol.input != "" && symbol.input != "}");
      }
      result = AMparseExpr("{"+str,true,true);
//    if (result[0]==null) return [AMcreateMmlNode("mo",
//			   document.createTextNode(symbol.input)),str];
      node = AMcreateMmlNode("mtable",result[0]);
      mask = mask.replace(/l/g,"left ");
      mask = mask.replace(/r/g,"right ");
      mask = mask.replace(/c/g,"center ");
      node.setAttribute("columnalign",mask);
      node.setAttribute("displaystyle","false");
      if (isIE)
	return [node,result[1],null];
// trying to get a *little* bit of space around the array
// (IE already includes it)
      var lspace = AMcreateElementMathML("mspace");
      lspace.setAttribute("width","0.167em");
      var rspace = AMcreateElementMathML("mspace");
      rspace.setAttribute("width","0.167em");
      var node1 = AMcreateMmlNode("mrow",lspace);
      node1.appendChild(node);
      node1.appendChild(rspace);
      return [node1,result[1],null];
    } else {	// eqnarray
      result = AMparseExpr("{"+str,true,true);
      node = AMcreateMmlNode("mtable",result[0]);
      if (isIE)
	node.setAttribute("columnspacing","0.25em"); // best in practice?
      else
	node.setAttribute("columnspacing","0.167em"); // correct (but ignored?)
      node.setAttribute("columnalign","right center left");
      node.setAttribute("displaystyle","true");
      node = AMcreateMmlNode("mrow",node);
      return [node,result[1],null];
    }
  case TEXT:
      if (str.charAt(0)=="{") i=str.indexOf("}");
      else i = 0;
      if (i==-1)
		 i = str.length;
      st = str.slice(1,i);
      if (st.charAt(0) == " ") {
	node = AMcreateElementMathML("mspace");
	node.setAttribute("width","0.33em");	// was 1ex
	newFrag.appendChild(node);
      }
      newFrag.appendChild(
        AMcreateMmlNode(symbol.tag,document.createTextNode(st)));
      if (st.charAt(st.length-1) == " ") {
	node = AMcreateElementMathML("mspace");
	node.setAttribute("width","0.33em");	// was 1ex
	newFrag.appendChild(node);
      }
      str = AMremoveCharsAndBlanks(str,i+1);
      return [AMcreateMmlNode("mrow",newFrag),str,null];
  case UNARY:
      result = AMparseSexpr(str);
      if (result[0]==null) return [AMcreateMmlNode(symbol.tag,
                             document.createTextNode(symbol.output)),str];
      if (typeof symbol.func == "boolean" && symbol.func) { // functions hack
	st = str.charAt(0);
//	if (st=="^" || st=="_" || st=="/" || st=="|" || st==",") {
	if (st=="^" || st=="_" || st==",") {
	  return [AMcreateMmlNode(symbol.tag,
		    document.createTextNode(symbol.output)),str,symbol.tag];
        } else {
	  node = AMcreateMmlNode("mrow",
	   AMcreateMmlNode(symbol.tag,document.createTextNode(symbol.output)));
	  if (isIE) {
	    var space = AMcreateElementMathML("mspace");
	    space.setAttribute("width","0.167em");
	    node.appendChild(space);
	  }
	  node.appendChild(result[0]);
	  return [node,result[1],symbol.tag];
        }
      }
      if (symbol.input == "\\sqrt") {		// sqrt
	if (isIE) {	// set minsize, for \surd
	  var space = AMcreateElementMathML("mspace");
	  space.setAttribute("height","1.2ex");
	  space.setAttribute("width","0em");	// probably no effect
	  node = AMcreateMmlNode(symbol.tag,result[0])
//	  node.setAttribute("minsize","1");	// ignored
//	  node = AMcreateMmlNode("mrow",node);  // hopefully unnecessary
	  node.appendChild(space);
	  return [node,result[1],symbol.tag];
	} else
	  return [AMcreateMmlNode(symbol.tag,result[0]),result[1],symbol.tag];
      } else if (typeof symbol.acc == "boolean" && symbol.acc) {   // accent
        node = AMcreateMmlNode(symbol.tag,result[0]);
	var output = symbol.output;
	if (isIE) {
		if (symbol.input == "\\hat")
			output = "\u0302";
		else if (symbol.input == "\\widehat")
			output = "\u005E";
		else if (symbol.input == "\\bar")
			output = "\u00AF";
		else if (symbol.input == "\\grave")
			output = "\u0300";
		else if (symbol.input == "\\tilde")
			output = "\u0303";
	}
	var node1 = AMcreateMmlNode("mo",document.createTextNode(output));
	if (symbol.input == "\\vec" || symbol.input == "\\check")
						// don't allow to stretch
	    node1.setAttribute("maxsize","1.2");
		 // why doesn't "1" work?  \vec nearly disappears in firefox
	if (isIE && symbol.input == "\\bar")
	    node1.setAttribute("maxsize","0.5");
	if (symbol.input == "\\underbrace" || symbol.input == "\\underline")
	  node1.setAttribute("accentunder","true");
	else
	  node1.setAttribute("accent","true");
	node.appendChild(node1);
	if (symbol.input == "\\overbrace" || symbol.input == "\\underbrace")
	  node.ttype = UNDEROVER;
	return [node,result[1],symbol.tag];
      } else {			      // font change or displaystyle command
        if (!isIE && typeof symbol.codes != "undefined") {
          for (i=0; i<result[0].childNodes.length; i++)
            if (result[0].childNodes[i].nodeName=="mi" || result[0].nodeName=="mi") {
              st = (result[0].nodeName=="mi"?result[0].firstChild.nodeValue:
                              result[0].childNodes[i].firstChild.nodeValue);
              var newst = [];
              for (var j=0; j<st.length; j++)
                if (st.charCodeAt(j)>64 && st.charCodeAt(j)<91) newst = newst +
                  String.fromCharCode(symbol.codes[st.charCodeAt(j)-65]);
                else newst = newst + st.charAt(j);
              if (result[0].nodeName=="mi")
                result[0]=AMcreateElementMathML("mo").
                          appendChild(document.createTextNode(newst));
              else result[0].replaceChild(AMcreateElementMathML("mo").
          appendChild(document.createTextNode(newst)),result[0].childNodes[i]);
            }
        }
        node = AMcreateMmlNode(symbol.tag,result[0]);
        node.setAttribute(symbol.atname,symbol.atval);
	if (symbol.input == "\\scriptstyle" ||
	    symbol.input == "\\scriptscriptstyle")
		node.setAttribute("displaystyle","false");
	return [node,result[1],symbol.tag];
      }
  case BINARY:
    result = AMparseSexpr(str);
    if (result[0]==null) return [AMcreateMmlNode("mo",
			   document.createTextNode(symbol.input)),str,null];
    result2 = AMparseSexpr(result[1]);
    if (result2[0]==null) return [AMcreateMmlNode("mo",
			   document.createTextNode(symbol.input)),str,null];
    if (symbol.input=="\\root" || symbol.input=="\\stackrel")
      newFrag.appendChild(result2[0]);
    newFrag.appendChild(result[0]);
    if (symbol.input=="\\frac") newFrag.appendChild(result2[0]);
    return [AMcreateMmlNode(symbol.tag,newFrag),result2[1],symbol.tag];
  case INFIX:
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    return [AMcreateMmlNode("mo",document.createTextNode(symbol.output)),
	str,symbol.tag];
  default:
    return [AMcreateMmlNode(symbol.tag,        //its a constant
	document.createTextNode(symbol.output)),str,symbol.tag];
  }
}

function AMparseIexpr(str) {
  var symbol, sym1, sym2, node, result, tag, underover;
  str = AMremoveCharsAndBlanks(str,0);
  sym1 = AMgetSymbol(str);
  result = AMparseSexpr(str);
  node = result[0];
  str = result[1];
  tag = result[2];
  symbol = AMgetSymbol(str);
  if (symbol.ttype == INFIX) {
    str = AMremoveCharsAndBlanks(str,symbol.input.length);
    result = AMparseSexpr(str);
    if (result[0] == null) // show box in place of missing argument
      result[0] = AMcreateMmlNode("mo",document.createTextNode("\u25A1"));
    str = result[1];
    tag = result[2];
    if (symbol.input == "_" || symbol.input == "^") {
      sym2 = AMgetSymbol(str);
      tag = null;	// no space between x^2 and a following sin, cos, etc.
// This is for \underbrace and \overbrace
      underover = ((sym1.ttype == UNDEROVER) || (node.ttype == UNDEROVER));
//    underover = (sym1.ttype == UNDEROVER);
      if (symbol.input == "_" && sym2.input == "^") {
        str = AMremoveCharsAndBlanks(str,sym2.input.length);
        var res2 = AMparseSexpr(str);
	str = res2[1];
	tag = res2[2];  // leave space between x_1^2 and a following sin etc.
        node = AMcreateMmlNode((underover?"munderover":"msubsup"),node);
        node.appendChild(result[0]);
        node.appendChild(res2[0]);
      } else if (symbol.input == "_") {
	node = AMcreateMmlNode((underover?"munder":"msub"),node);
        node.appendChild(result[0]);
      } else {
	node = AMcreateMmlNode((underover?"mover":"msup"),node);
        node.appendChild(result[0]);
      }
      node = AMcreateMmlNode("mrow",node); // so sum does not stretch
    } else {
      node = AMcreateMmlNode(symbol.tag,node);
      if (symbol.input == "\\atop" || symbol.input == "\\choose")
	node.setAttribute("linethickness","0ex");
      node.appendChild(result[0]);
      if (symbol.input == "\\choose")
	node = AMcreateMmlNode("mfenced",node);
    }
  }
  return [node,str,tag];
}

function AMparseExpr(str,rightbracket,matrix) {
  var symbol, node, result, i, tag,
  newFrag = document.createDocumentFragment();
  do {
    str = AMremoveCharsAndBlanks(str,0);
    result = AMparseIexpr(str);
    node = result[0];
    str = result[1];
    tag = result[2];
    symbol = AMgetSymbol(str);
    if (node!=undefined) {
      if ((tag == "mn" || tag == "mi") && symbol!=null &&
	typeof symbol.func == "boolean" && symbol.func) {
			// Add space before \sin in 2\sin x or x\sin x
	  var space = AMcreateElementMathML("mspace");
	  space.setAttribute("width","0.167em");
	  node = AMcreateMmlNode("mrow",node);
	  node.appendChild(space);
      }
      newFrag.appendChild(node);
    }
  } while ((symbol.ttype != RIGHTBRACKET)
        && symbol!=null && symbol.output!="");
  tag = null;
  if (symbol.ttype == RIGHTBRACKET) {
    if (symbol.input == "\\right") { // right what?
      str = AMremoveCharsAndBlanks(str,symbol.input.length);
      symbol = AMgetSymbol(str);
      if (symbol != null && symbol.input == ".")
	symbol.invisible = true;
      if (symbol != null)
	tag = symbol.rtag;
    }
    if (symbol!=null)
      str = AMremoveCharsAndBlanks(str,symbol.input.length); // ready to return
    var len = newFrag.childNodes.length;
    if (matrix &&
      len>0 && newFrag.childNodes[len-1].nodeName == "mrow" && len>1 &&
      newFrag.childNodes[len-2].nodeName == "mo" &&
      newFrag.childNodes[len-2].firstChild.nodeValue == "&") { //matrix
	var pos = []; // positions of ampersands
        var m = newFrag.childNodes.length;
        for (i=0; matrix && i<m; i=i+2) {
          pos[i] = [];
          node = newFrag.childNodes[i];
	  for (var j=0; j<node.childNodes.length; j++)
	    if (node.childNodes[j].firstChild.nodeValue=="&")
	      pos[i][pos[i].length]=j;
        }
	var row, frag, n, k, table = document.createDocumentFragment();
	for (i=0; i<m; i=i+2) {
	  row = document.createDocumentFragment();
	  frag = document.createDocumentFragment();
	  node = newFrag.firstChild; // <mrow> -&-&...&-&- </mrow>
	  n = node.childNodes.length;
	  k = 0;
	  for (j=0; j<n; j++) {
	    if (typeof pos[i][k] != "undefined" && j==pos[i][k]){
	      node.removeChild(node.firstChild); //remove &
	      row.appendChild(AMcreateMmlNode("mtd",frag));
	      k++;
	    } else frag.appendChild(node.firstChild);
	  }
	  row.appendChild(AMcreateMmlNode("mtd",frag));
	  if (newFrag.childNodes.length>2) {
	    newFrag.removeChild(newFrag.firstChild); //remove <mrow> </mrow>
	    newFrag.removeChild(newFrag.firstChild); //remove <mo>&</mo>
	  }
	  table.appendChild(AMcreateMmlNode("mtr",row));
	}
	return [table,str];
    }
    if (typeof symbol.invisible != "boolean" || !symbol.invisible) {
      node = AMcreateMmlNode("mo",document.createTextNode(symbol.output));
      newFrag.appendChild(node);
    }
  }
  return [newFrag,str,tag];
}

function AMparseMath(str) {
  var result, node = AMcreateElementMathML("mstyle");
  if (mathcolor != "") node.setAttribute("mathcolor",mathcolor);
  if (mathfontfamily != "") node.setAttribute("fontfamily",mathfontfamily);
  node.appendChild(AMparseExpr(str.replace(/^\s+/g,""),false,false)[0]);
  node = AMcreateMmlNode("math",node);
  if (showasciiformulaonhover)                      //fixed by djhsu so newline
    node.setAttribute("title",str.replace(/\s+/g," "));//does not show in Gecko
  if (mathfontfamily != "" && (isIE || mathfontfamily != "serif")) {
    var fnode = AMcreateElementXHTML("font");
    fnode.setAttribute("face",mathfontfamily);
    fnode.appendChild(node);
    return fnode;
  }
  return node;
}

function AMstrarr2docFrag(arr, linebreaks) {
  var newFrag=document.createDocumentFragment();
  var expr = false;
  for (var i=0; i<arr.length; i++) {
    if (expr) newFrag.appendChild(AMparseMath(arr[i]));
    else {
      var arri = (linebreaks ? arr[i].split("\n\n") : [arr[i]]);
      newFrag.appendChild(AMcreateElementXHTML("span").
      appendChild(document.createTextNode(arri[0])));
      for (var j=1; j<arri.length; j++) {
        newFrag.appendChild(AMcreateElementXHTML("p"));
        newFrag.appendChild(AMcreateElementXHTML("span").
        appendChild(document.createTextNode(arri[j])));
      }
    }
    expr = !expr;
  }
  return newFrag;
}

function AMprocessNodeR(n, linebreaks) {
  var mtch, str, arr, frg, i;
  if (n.childNodes.length == 0) {
   if ((n.nodeType!=8 || linebreaks) &&
    n.parentNode.nodeName!="form" && n.parentNode.nodeName!="FORM" &&
    n.parentNode.nodeName!="textarea" && n.parentNode.nodeName!="TEXTAREA" &&
    n.parentNode.nodeName!="pre" && n.parentNode.nodeName!="PRE") {
    str = n.nodeValue;
    if (!(str == null)) {
      str = str.replace(/\r\n\r\n/g,"\n\n");
      str = str.replace(/\x20+/g," ");
      str = str.replace(/\s*\r\n/g," ");
// DELIMITERS:
      mtch = (str.indexOf("\$")==-1 ? false : true);
      str = str.replace(/([^\\])\$/g,"$1 \$");
      str = str.replace(/^\$/," \$");	// in case \$ at start of string
      arr = str.split(" \$");
      for (i=0; i<arr.length; i++)
	arr[i]=arr[i].replace(/\\\$/g,"\$");
      if (arr.length>1 || mtch) {
        if (checkForMathML) {
          checkForMathML = false;
          var nd = AMisMathMLavailable();
          AMnoMathML = nd != null;
          if (AMnoMathML && notifyIfNoMathML)
            if (alertIfNoMathML)
              alert("To view the ASCIIMathML notation use Internet Explorer 6 +\nMathPlayer (free from www.dessci.com)\n\
                or Firefox/Mozilla/Netscape");
            else AMbody.insertBefore(nd,AMbody.childNodes[0]);
        }
        if (!AMnoMathML) {
          frg = AMstrarr2docFrag(arr,n.nodeType==8);
          var len = frg.childNodes.length;
          n.parentNode.replaceChild(frg,n);
          return len-1;
        } else return 0;
      }
    }
   } else return 0;
  } else if (n.nodeName!="math") {
    for (i=0; i<n.childNodes.length; i++)
      i += AMprocessNodeR(n.childNodes[i], linebreaks);
  }
  return 0;
}

function AMprocessNode(n, linebreaks, spanclassAM) {
  var frag,st;
  if (spanclassAM!=null) {
    frag = document.getElementsByTagName("span")
    for (var i=0;i<frag.length;i++)
      if (frag[i].className == "AM")
        AMprocessNodeR(frag[i],linebreaks);
  } else {
    try {
      st = n.innerHTML;
    } catch(err) {}
// DELIMITERS:
    if (st==null || st.indexOf("\$")!=-1)
      AMprocessNodeR(n,linebreaks);
  }
  if (isIE) { //needed to match size and font of formula to surrounding text
    frag = document.getElementsByTagName('math');
    for (var i=0;i<frag.length;i++) frag[i].update()
  }
}

var AMbody;
var AMnoMathML = false, AMtranslated = false;

function translate(spanclassAM) {
  if (!AMtranslated) { // run this only once
    AMtranslated = true;
    AMinitSymbols();
    AMbody = document.getElementsByTagName("body")[0];
    AMprocessNode(AMbody, false, spanclassAM);
  }
}

if (isIE) { // avoid adding MathPlayer info explicitly to each webpage
  document.write("<object id=\"mathplayer\"\
  classid=\"clsid:32F66A20-7614-11D4-BD11-00104BD3F987\"></object>");
  document.write("<?import namespace=\"m\" implementation=\"#mathplayer\"?>");
}

// GO1.1 Generic onload by Brothercake
// http://www.brothercake.com/
//onload function (replaces the onload="translate()" in the <body> tag)
function generic()
{
  translate();
};
//setup onload function
if(typeof window.addEventListener != 'undefined')
{
  //.. gecko, safari, konqueror and standard
  window.addEventListener('load', generic, false);
}
else if(typeof document.addEventListener != 'undefined')
{
  //.. opera 7
  document.addEventListener('load', generic, false);
}
else if(typeof window.attachEvent != 'undefined')
{
  //.. win/ie
  window.attachEvent('onload', generic);
}
//** remove this condition to degrade older browsers
else
{
  //.. mac/ie5 and anything else that gets this far
  //if there's an existing onload function
  if(typeof window.onload == 'function')
  {
    //store it
    var existing = onload;
    //add new onload handler
    window.onload = function()
    {
      //call existing onload function
      existing();
      //call generic onload function
      generic();
    };
  }
  else
  {
    //setup onload function
    window.onload = generic;
  }
}
/*]]>*/
</script>
</head>
<body class="book">
<div id="header">
<h1>Thermodynamic Analytics Toolkit (TATi) - Programmer&#8217;s Guide</h1>
<span id="author">Frederik Heber</span><br>
<span id="email" class="monospaced">&lt;<a href="mailto:frederik.heber@gmail.com">frederik.heber@gmail.com</a>&gt;</span><br>
<span id="revnumber">version v0.9.1-0-g994aa50,</span>
<span id="revdate">2018-08-17</span>
<div id="toc">
  <div id="toctitle">Table of Contents</div>
  <noscript><p><b>JavaScript must be enabled in your browser to display the table of contents.</b></p></noscript>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<div class="imageblock" style="text-align:center;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/./pictures/tati_logo.png" alt="TATi logo" width="700">
</div>
</div>
<div class="sidebarblock">
<div class="content">
<div class="title">2018-08-17 thermodynamicanalyticstoolkit: v0.9.1-0-g994aa50</div>
<div class="paragraph"><p>TATi is a software suite written in Python based on <a href="https://www.tensorflow.org/">tensorflow</a>'s
Python API. It brings advanced sampling methods (GLA1 and GLA2, BAOAB, HMC) to
<em>neural network training</em>. Its <strong>tools</strong> allow to assess the loss manifold&#8217;s
topology that depends on the employed neural network and the dataset. Moreover,
its <strong>simulation</strong> module makes applying present sampling Python codes in the
context of neural networks easy and straight-forward. The goal of the software
is to enable the user to analyze and adapt the network employed for a specific
classification problem to best fit her or his needs.</p></div>
<div class="paragraph"><p>TATi has received financial support from a seed funding grant and through a
Rutherford fellowship from the Alan Turing Institute in London (R-SIS-003,
R-RUT-001) and EPSRC grant no. EP/P006175/1 (Data Driven Coarse Graining using
Space-Time Diffusion Maps, B. Leimkuhler PI).</p></div>
<div class="paragraph"><p><em>Frederik Heber</em></p></div>
</div></div>
</div>
</div>
<div class="sect1">
<h2 id="introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph"><p>Performing efficient neural network training or sampling requires many ingredients.
In this programmer&#8217;s guide we would like to equip you with the necessary
knowledge of the abstract concepts of computational graphs, show how to
generally use <a href="https://www.tensorflow.org/">Tensorflow</a> and moreover how to
extend it to advanced sampling methods as it is done in TATi.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">In case you are generally unfamiliar with TATi, we would like to refer you to
the userguide that is also contained in the documentation of this package.</td>
</tr></table>
</div>
<div class="paragraph"><p>In detail, this guide will give introductory details on the inner workings and
major  concepts of tensorflow. Moreover, we give extensive details on what is
needed to perform neural network training using tensorflow including
implementation examples. We conclude with providing details on how tensorflow
was extended in order to allow for advanced sampling methods to be incorporated
in the course of TATi. To this end, we show how general programming concepts
like local and global variables, branching and so on can be executed. This will
allow you to extend TATi with your own ideas taking full advantage of the
Tensorflow performance.</p></div>
</div>
</div>
<div class="sect1">
<h2 id="concepts">2. Concepts</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="concepts.computational_graph">2.1. Computational Graph</h3>
<div class="paragraph"><p>Underlying all of tensorflow and typically most of the Machine Learning (ML)
frameworks is the concept of a <em>computational graph</em>.</p></div>
<div class="paragraph"><p>A graph consists of a set of vertices V and a set of edges E. Edges
$e_{ij} \in E$ connect two nodes $v_i, v_j$ and may be
directed.</p></div>
<div class="paragraph"><p>In the context of a computational graph functions and variables represent the
vertices and directed edges represent dependencies between. In Tensorflow&#8217;s
documentation and tutorials the vertices are referred to as <em>nodes</em>. Hence, we
will use the term nodes in the following, too.</p></div>
<div class="imageblock" id="concepts.computational_graph.figure">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/./pictures/computational_graph.png" alt="./pictures/computational_graph.png" width="250">
</div>
<div class="title">Figure 1. Computational Graph: Sum function node depending on the input of two variable nodes <strong>a</strong> and <strong>b</strong>.</div>
</div>
<div class="paragraph"><p>Let us have a concrete example and take a look at Figure
<a href="#concepts.computational_graph.figure">Computational Graph</a>.
There, we have two variable nodes <strong>a</strong> and <strong>b</strong> and one summation node $\Sigma$
that depends on the two.</p></div>
<div class="paragraph"><p>Assume we want to evaluate the sum function. The function node can be imagined
as a callback whose parameters are supplied by <strong>a</strong> and <strong>b</strong>. Knowing the
dependencies encoded in the edges of the computational graph, we know how to
execute the callback and evaluate the sum.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Graphs are standard concepts in computer science and enjoy a large
variety of algorithms that discover their properties such a shortest paths,
number of connected components, cliques, cycles, and so on. Standard algorithms
such  as Breadth-First Search (BFS) and Depth-First Search (DFS) allow to
explore and enumerate all dependencies. For details, see
<a href="https://www.springer.com/de/book/9783662536216">Graph Theory, Reinhard Diestel</a>
and other textbooks.</td>
</tr></table>
</div>
<div class="paragraph"><p>Naturally, the variables <strong>a</strong> may themselves be functions depending on other
variables. I.e. arbitrary function concatentations are possible. Moreover, even
operations such as assignments are admissible. When the assignment is triggered,
using another node as input, this value is written to the internal state of a
variable node. Finally, nodes can also be combined into groups such that the
execution of the group node triggers the execution of all contained nodes.
This allows to program whole algorithms within the framework of computational
graphs.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">The graph is usually never completely evaluated. When evaluating a certain
node, then only dependent nodes must be evaluated, too. All other nodes are
ignored.</td>
</tr></table>
</div>
<div class="paragraph"><p>Think of the computional graph of another way of writing a computer program.
The program consists of many very tiny functions (nodes) and the edges
encode which function relies/calls which other function. Using the program
means executing certain functions that in turn trigger the execution of
many other functions in order to deliver their output value.</p></div>
<div class="paragraph"><p>This concept has even more advantages:</p></div>
<div class="ulist"><ul>
<li>
<p>
<em>lazy evaluation</em>, i.e., only computes what is necessary and first when it is
 needed.
</p>
</li>
<li>
<p>
the graph can be analysed for independent or only loosely dependent parts
which therefore can be trivially or at least easily parallelized.
</p>
</li>
<li>
<p>
as nodes consist of tiny functions, their computational complexity can be
estimated depending on the size of their input arguments. This allows to
queue the evaluation of nodes very efficiently.
</p>
</li>
</ul></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Tensorflow uses the graph analysis to automatically decide whether CPU
or GPU will execute the necessary computations for evaluating a node. Although
this automatic association can be overriden, ther is usually no need to.
This is different in the dynamic graph setting where the user needs to decide.</td>
</tr></table>
</div>
</div>
<div class="sect2">
<h3 id="concepts.tensorflow">2.2. Tensorflow</h3>
<div class="paragraph"><p>Historically, there were different approaches in setting up and using the
computational graph.
<a href="https://deeplearning.net/software/theano">theano</a>
required to actually compile code that would form the graph.
In <a href="https://www.tensorflow.org/">tensorflow</a> the graph needs to be static
throughout the program: first the graph is constructed, then a session
object is instantiated and nodes are evaluated. This session object contains the
internal state of the graph and of every node. Note that while the graph is
static, the session is not, i.e. the contents of  variables, may naturally
change.
<a href="https://pytorch.org">Pytorch</a> on the other hand strictly believes in a
dynamical graph, i.e. there is no session object containing temporary values
but the internal state is directly encoded with each node.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">From Tensorflow version 1.6 so-called "eager execution" was introduced
to allow for the same dynamical graph use as with PyTorch. Before that adding
more nodes to the graph after the session had been created had a bad effect on
performance. However, at the time of writing (tf1.9) the static graph is
generally faster and generally adapts better to different hardwares and GPU
setups, see tensorflow&#8217;s <a href="https://www.tensorflow.org/community/roadmap">roadmap</a>
on eager execution.
In general, static graphs will always execute faster than dynamic graphs.</td>
</tr></table>
</div>
<div class="sect3">
<h4 id="concepts.tensorflow.graph_construction">2.2.1. Constructing a graph</h4>
<div class="paragraph"><p>Let us directly see how the above graph, see Figure
<a href="#concepts.computational_graph.figure">Computational Graph</a> is constructed
using tensorflow.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> tensorflow as tf

a <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Constant</span></span><span style="color: #990000">(</span><span style="color: #993399">2</span><span style="color: #990000">.)</span>
b <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Constant</span></span><span style="color: #990000">(</span><span style="color: #993399">3</span><span style="color: #990000">.)</span>
sum <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">add</span></span><span style="color: #990000">(</span>a<span style="color: #990000">,</span>b<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>We imported the tensorflow module and then created two nodes containing constant
values. Afterwards, we construct a sum node that depends on the two constant
nodes.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">The above does not perform any actual computation! All we do is construct
objects in memory.</td>
</tr></table>
</div>
<div class="paragraph"><p>In order to actually evaluate the sum in this case, we need to create a
<span class="monospaced">Session</span> object.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>sess <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>sum<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>This will print <strong>5.0</strong> as the result of the operation. The <span class="monospaced">Session</span> object
contains all the temporary memory required for containing information in
nodes.</p></div>
<div class="paragraph"><p>Tensorflow internally has a default graph to which all created nodes are
associated and of which the <span class="monospaced">Session</span> takes hold. The graph can be reset by
calling <span class="monospaced">tf.reset_default_graph()</span>.</p></div>
<div class="paragraph"><p>Tensorflow has a whole assortment of arithmetic, basic mathematical, linear
algebra and similar functions, see <a href="https://www.tensorflow.org/api_guides/python/math_ops">Math Ops</a>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/tip.png" alt="Tip">
</td>
<td class="content">Functions for standard algebraic operations such as sum, difference,
(scalar) multiplication the respective python operators have been overloaded.
To give an example, <span class="monospaced">tf.add(a,b)</span> can also be written as <span class="monospaced">a+b</span>. Moreover,
Tensorflow will automatically convert constant python variables into its
tensors, e.g., <span class="monospaced">2. * a - b</span>. This allows to write mathematical operations
in the same way as if manipulating <span class="monospaced">numpy</span> arrays.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="concepts.tensorflow.variables">2.2.2. Variables</h4>
<div class="paragraph"><p>Constants are given at the graph&#8217;s construction and may not change. However,
there also variables. These are constructed by giving an initial_value, a
(derived) type and a name, i.e., in the same way as the constants. However,
in contrast to constants variables allow <em>assignment</em>.</p></div>
<div class="paragraph"><p>The shape is most important and for variables it is derived from the
<em>initial value</em>. Tensorflow uses the shape to check the consistency when
connecting nodes, i.e. chaining functions. For example, a matrix-vector
multiplication of a (2,2) matrix with a (10,1) vector will not work because
their shapes do not match.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>a <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">zeros</span></span><span style="color: #990000">((</span><span style="color: #993399">784</span><span style="color: #990000">))</span>
b <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random_uniform</span></span><span style="color: #990000">((</span><span style="color: #993399">784</span><span style="color: #990000">,</span><span style="color: #993399">10</span><span style="color: #990000">),</span> minval<span style="color: #990000">=-</span><span style="color: #993399">0.5</span><span style="color: #990000">,</span> maxval<span style="color: #990000">=</span><span style="color: #993399">0.5</span><span style="color: #990000">)</span>
v <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variable</span></span><span style="color: #990000">(</span>a<span style="color: #990000">,</span> name<span style="color: #990000">=</span><span style="color: #FF0000">"zero_vector"</span><span style="color: #990000">)</span>
W <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variable</span></span><span style="color: #990000">(</span>b<span style="color: #990000">,</span> name<span style="color: #990000">=</span><span style="color: #FF0000">"random_matrix"</span><span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>We first construct a vector of 784 components, all zero. Next, we create a
a randon matrix of 784 by 10 components, its values uniformly drawn from the
interval [-0.5,0.5]. These serve as initial values to initialize the two
variables <strong>v</strong> and <strong>W</strong>, we instantiate afterwards.</p></div>
<div class="paragraph"><p>As you notice immediately, tensorflow has functions in likeness very similar
to <span class="monospaced">numpy</span>. First, <span class="monospaced">a</span> is a vector with 784 zero components. Then, a random
(constant)  matrix <span class="monospaced">b</span> is constructed. Finally, both are used as initial values
for variables. Of course, even higher  order tensor can be constructed.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">As these tensors "flow" through the computational graph, this gave rise to
the name "tensorflow".</td>
</tr></table>
</div>
<div class="paragraph"><p>The <em>type</em> can be <span class="monospaced">tf.int</span>, <span class="monospaced">tf.float32</span>, <span class="monospaced">tf.float64</span>, and so on. Tensorflow
will admonish operations where the types are not used consistently. Tensors
must be explicitly converted to another type using <span class="monospaced">tf.cast()</span>.</p></div>
<div class="paragraph"><p>The <em>name</em> identifies the node - in general, each node has a name and this eases
debugging and allows for readable errors messages.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/caution.png" alt="Caution">
</td>
<td class="content">In contrast to constants and the up-coming placeholders, variables
<em>used</em> in evaluation need to be initialized once at the beginning of the
session. Use <span class="monospaced">session.run(tf.global_variables_initializer())</span>.</td>
</tr></table>
</div>
<div class="paragraph"><p>All in all there are actually three different types of variables: <span class="monospaced">tf.constant</span>,
 <span class="monospaced">tf.Variable</span>, and <span class="monospaced">tf.placeholder</span>. The last of which we come to now.</p></div>
</div>
<div class="sect3">
<h4 id="concepts.tensorflow.placeholders">2.2.3. Placeholders</h4>
<div class="paragraph"><p>Placeholders represent a value of a predefined shape that is supplied by the
user lateron. In other words, while constants have to be given at "compile-time"
(when writing the python program), placeholder values are supplied at "run-time"
(when the program is executed).</p></div>
<div class="paragraph"><p>This allows for great flexibility. For example, one could extend the above
example to a small program that would sum two arbitrary values given by the
user in the following way.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> tensorflow as tf
<span style="font-weight: bold"><span style="color: #000080">import</span></span> sys

a <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">placeholder</span></span><span style="color: #990000">(</span>shape<span style="color: #990000">=(),</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span>
b <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">placeholder</span></span><span style="color: #990000">(</span>shape<span style="color: #990000">=(),</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span>
sum <span style="color: #990000">=</span> a <span style="color: #990000">+</span> b
sess <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span>
<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>sum<span style="color: #990000">,</span> feed_dict<span style="color: #990000">={</span>a<span style="color: #990000">:</span> <span style="font-weight: bold"><span style="color: #000000">float</span></span><span style="color: #990000">(</span>sys<span style="color: #990000">.</span>argv<span style="color: #990000">[</span><span style="color: #993399">1</span><span style="color: #990000">]),</span> b<span style="color: #990000">:</span> <span style="font-weight: bold"><span style="color: #000000">float</span></span><span style="color: #990000">(</span>sys<span style="color: #990000">.</span>argv<span style="color: #990000">[</span><span style="color: #993399">2</span><span style="color: #990000">])}))</span></tt></pre></div></div>
<div class="paragraph"><p>As you see the two <span class="monospaced">tf.constant()</span> nodes <strong>a</strong> and <strong>b</strong> have been replaced by
<span class="monospaced">tf.placeholder()</span> where we set the shape to <span class="monospaced">()</span>, signifying a scalar value.
Next, we again create the sum node and instantiate a <span class="monospaced">Session</span> object as before.
In the last line, execute <span class="monospaced">run()</span> on the session. However, there we needed to
supply an additional parameter, the <span class="monospaced">feed_dict</span>.</p></div>
<div class="paragraph"><p>This is because a placeholder is nothing but a promise to tensorflow that we
will provide a value of the designated shape lateron. The means of providing
the value is the feed_dict.</p></div>
<div class="paragraph"><p>This <span class="monospaced">feed_dict</span> is simply a python <span class="monospaced">dict</span> with keys and values. The keys are
are simply the node references themselves and the values are whatever the user
decides to feed in.</p></div>
<div class="paragraph"><p>Here, we use <span class="monospaced">sys.argv[..]</span> to read the first and second command-line
argument if you call this script in a file <span class="monospaced">sum.py</span> as <span class="monospaced">python3 sum.py 2. 3.</span>.
For simplicity of the example we do not catch any errors such as missing
arguments.</p></div>
<div class="paragraph"><p>Of course, this is a silly example! However, it serves a point. Through the
feed_dict all values may enter that the user needs to specify outside of your
algorithm and outside of the tensorflow graph. All parameters that control how
a method executes typically should be placeholders.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Not all values for each placeholder need to be fed each time <span class="monospaced">run()</span> is
executed, but only those which the evaluated node(s) depend on.</td>
</tr></table>
</div>
</div>
<div class="sect3">
<h4 id="concepts.tensorflow.summary">2.2.4. Summary</h4>
<div class="paragraph"><p>This has been a very brief introduction to tensorflow. In case you need more
information, then head over to the tensorflow <a href="https://www.tensorflow.org/">website</a>
where there are plenty of well-written tutorials on light-weight examples such
as seen above. Moreover, there you find the Application Programmer&#8217;s Interface
(API) documentation explaining each and every function.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="concepts.neural_networks">2.3. Neural Networks</h3>
<div class="paragraph"><p>Having briefly explained how computations work in general with tensorflow, we
would like to come to the concrete example of how neural networks are
implemented in such a framework.</p></div>
<div class="paragraph"><p>Let us make a list of what we need to perform training of neural networks:</p></div>
<div class="ulist"><ul>
<li>
<p>
neural network with weights, biases, activation functions and so on
</p>
</li>
<li>
<p>
input pipeline that feeds in the dataset
</p>
</li>
<li>
<p>
elements of training: loss function, gradients, optimizer
</p>
</li>
</ul></div>
<div class="sect3">
<h4 id="concepts.neural_networks.network_topology">2.3.1. Network topology</h4>
<div class="paragraph"><p>The neural network is itself a graph and also consists of nodes and edges.
Each node has a bias value <strong>b</strong> acting as data-independent shifts and an
activation function <strong>f</strong>. Edges connect nodes by stating which output of one
node acts as input to another node. They have weights <strong>w</strong> that act as scaling
factors, i.e. they are data-dependent.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Tensorflow comes with the <a href="https://www.tensorflow.org/tutorials/keras"><span class="monospaced">keras</span></a>
module that is a high-level API providing convenience functions to setup
arbitrary network topologies in a layer-wise fashion. In order to maintain
complete control over our network we will build it up from the bottom ourselves.</td>
</tr></table>
</div>
<div class="paragraph"><p>To directly work on a concrete example, we will
be using the "Hello, world" pendant of the machine learning world, namely the
MNIST dataset: it consists of 55.000 grey-scale images, each 28x28 pixels. These
contain hand-written digits, i.e. there are ten different classes to learn for
the digits 0 to 9.
Each pixel in the image has a single integer value in [0,255] which is usually
converted to a floating point number in [0,1]. We will build a simple
single-layer perceptron having one input and one output layer.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/warning.png" alt="Warning">
</td>
<td class="content">Although we let this example be guided by the dimensions of the MNIST
dataset, we will not actually show how to use MNIST directly here for
simplicity. We catch up on this in a supplement to this section on
<a href="#concepts.neural_networks">[concepts.neural_networks]</a>.</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="concepts.neural_networks.network_topology.input">Input layer</h5>
<div class="paragraph"><p>So, let us build the first layer: the input layer. Inputs to the nodes
of the input layer come from the outside, namely the user feeding in a dataset
or a batch thereof. Having read about placeholders before, otherwise see
<a href="#concepts.tensorflow.placeholders">Placeholders</a>, this is the first place
where we need them.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>x <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Placeholder</span></span><span style="color: #990000">(</span>float32<span style="color: #990000">,</span> <span style="color: #990000">[</span>None<span style="color: #990000">,</span> <span style="color: #993399">784</span><span style="color: #990000">],</span> name<span style="color: #990000">=</span><span style="color: #FF0000">"x"</span><span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Here, the dimension is 28x28=784. Note that we need to specify a type here.
This is used to check that the value later fed into the network in place of
this placeholders matches with the type specified. Moreover, the type is
used in checking consistency when chaining nodes together. Some of the frequent
types are float16, float64, even int16. Moreover, we have set a name for
the node to ease debugging and readability of error messages.</p></div>
<div class="paragraph"><p>What&#8217;s the purpose of <span class="monospaced">None</span>? Remember that we do not have just one image but
55.000 images. Typically, they are fed in batches into the network. As the
<span class="monospaced">batch_size</span> is not known a-priori but depends on the choice of the user and
tensorflow does not allow a placeholder in the definition of the shape, it
makes up for it by allowing to specify <span class="monospaced">None</span> for all dimensions not known
a-priori.</p></div>
<div class="paragraph"><p>In TATi an additional layer is place in between where a subselection of
features can be used and further allowing for transformation with trigonometric
functions, i.e. <strong>sin(x1)</strong> would use the sine of the first input dimension as
one input to the network.</p></div>
<div class="paragraph"><p>This is possible as placeholders (and also variables) can be spliced in a
pythonic fashion. Let us take a look at how this is done.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>list_of_nodes <span style="color: #990000">=</span> <span style="color: #990000">[</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sin</span></span><span style="color: #990000">(</span>x<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">])]</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we simply give a list containing only the first component of the
placeholder <span class="monospaced">x</span> we have constructed before and of which we take the sine.
Naturally, this list could contain more values.
Next, all these are stacked together to form a single node.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>x <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">transpose</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">stack</span></span><span style="color: #990000">(</span>list_of_nodes<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>We need to <span class="monospaced">tf.transpose()</span> the result additionally because of the choice
of the shape of our input layer <span class="monospaced">x</span> as <span class="monospaced">[None, 784]</span>. If it were the other
way round, there would be no need for it.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">In TATi we have chosen the dimension such that they match when writing
them down from left to right. For example, matrix vector multiplication of
a vector of <span class="monospaced">[None, 784]</span> with a matrix of <span class="monospaced">[784,10]</span> matches.</td>
</tr></table>
</div>
<div class="paragraph"><p>Naturally, transforming the input like this may change the input dimension
which is important for the subsequent layer. <span class="monospaced">x.get_shape()</span> returns an array
of all its dimensions.</p></div>
</div>
<div class="sect4">
<h5 id="concepts.neural_networks.network_topology.output">Output layer</h5>
<div class="paragraph"><p>Then we continue with the next layer, the output layer. In essence, our network
computes the function $f(Wx+b)$, where <strong>W</strong> is the weight matrix, <strong>x</strong>
is the input vector and <strong>b</strong> is a bias vector.</p></div>
<div class="paragraph"><p>Weights and biases are represented by variables <span class="monospaced">tf.Variable()</span>, an activation
function could be <span class="monospaced">tf.nn.relu()</span>. Edges are obtained through algebraic
operations such as addition and multiplication and of course concatenation of
functions. Naturally, tensorflow offers a whole flock of different activation
functions.</p></div>
<div class="paragraph"><p>In contrast to placeholders, variables and constants need to be given their
initial value right from the start. So, how do we initialize the weights and
biases?
Tensorflow has of course <em>random numbers</em> and we simply generate a matrix
containing random numbers. The bias vector is usually set to a small, non-zero
value, here <strong>0.1</strong>.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/caution.png" alt="Caution">
</td>
<td class="content">Taking control of the seed of the random number generator will be a
critical step and is the main reason for building networks up from the bottom.
Although tensorflow offers setting <span class="monospaced">tf.set_global_seed()</span> which derives seeds
for all internal random number sequences in a deterministic fashion, this
fashion changes when a single node (even non-dependent) is added to the
computational graph.
This is therefore <strong>unusable in a scientific context</strong> where reproducible
experiments are a top concern.</td>
</tr></table>
</div>
<div class="paragraph"><p>All together, we construct the output layer.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>seed<span style="color: #990000">=</span><span style="color: #993399">426</span>
w_init <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random_uniform</span></span><span style="color: #990000">((</span><span style="color: #993399">784</span><span style="color: #990000">,</span><span style="color: #993399">10</span><span style="color: #990000">),</span> minval<span style="color: #990000">=-</span><span style="color: #993399">0.5</span><span style="color: #990000">,</span> maxval<span style="color: #990000">=</span><span style="color: #993399">0.5</span><span style="color: #990000">,</span> seed<span style="color: #990000">=</span>seed<span style="color: #990000">,</span>
  dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span>
b_init <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">constant</span></span><span style="color: #990000">(</span><span style="color: #993399">0.1</span><span style="color: #990000">,</span> shape<span style="color: #990000">=</span>shape<span style="color: #990000">,</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span>
W <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variabel</span></span><span style="color: #990000">(</span>w_init<span style="color: #990000">,</span> type<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span>
b <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variable</span></span><span style="color: #990000">(</span>b_init<span style="color: #990000">,</span> type<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span>
pre_activation <span style="color: #990000">=</span> W<span style="color: #990000">*</span>x<span style="color: #990000">+</span>b
output <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">linear</span></span><span style="color: #990000">(</span>pre_activation<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>We fix an arbitrary seed which is used for the random number generator
producing the components of the initial weight matrix uniformly in the range
[-0.5, 0.5]. Moreover, we have a vector with constant components of 0.1. From
these two we create two variables.
Next, we have created a node <span class="monospaced">pre_act</span> containing the input to the activation
function obtained from algebraic operations on these variables. Then, we obtain
the <span class="monospaced">output</span> as the output of the activation function with this input.</p></div>
</div>
<div class="sect4">
<h5 id="concepts.neural_networks.network_topology.hidden">Hidden layers</h5>
<div class="paragraph"><p>Adding a <em>hidden layer</em> is then as simple as adding another weight matrix where
we need to use a <strong>different</strong> seed for its random numbers and setting up the
function $f_o(W_o f_h(W_h x+b_h) + b_o)$, where we used indices <strong>h</strong>
and <strong>o</strong> for the hidden and output layer.</p></div>
<div class="paragraph"><p>This way we can construct arbitrary feed-forward networks.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Variables are internally marked as <em>trainable</em> by default.
This means that they go into a special internal collection and we will later
see where this is needed when we get to the training part. One can exclude a
variable by either stating <span class="monospaced">..., trainable=False,...</span> in its construction or
by removing it from the collection lateron. A reference to it can be obtained
by <span class="monospaced">tf.get_collection_ref(...)</span>. Note that <span class="monospaced">tf.get_collection()</span> only returns a
copy.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="concepts.neural_networks.input_pipeline">2.3.2. Input pipeline</h4>
<div class="paragraph"><p>The next step is a bit more involved: Datasets are usually stored on hard
drives. We need to parse the files and prepare them. In the case of tensorflow,
we need to convert them to <span class="monospaced">numpy</span> arrays or directly to tensorflow tensors.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">This input pipeline crucially determines the performance of the network
overall. If the data is not fed in fast enough, we encounter "data-starvation",
i.e. the actual training is idling, waiting for the data to be copied to the
right internal places.</td>
</tr></table>
</div>
<div class="paragraph"><p>Tensorflow, since version 1.4, offers the <span class="monospaced">tf.data</span> module with its <span class="monospaced">Dataset</span>
class. This module allows to perform all the preprocessing <em>within the
computational graph</em> and conceptually uses three ingredients: tensors,
transformations, and the iterator. We&#8217;ll discuss each of them.</p></div>
<div class="paragraph"><p>We discuss how to parse CSV files in the following in three steps:</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
Reading the files line by line
</p>
</li>
<li>
<p>
Restructuring the parsed data into arrays
</p>
</li>
<li>
<p>
Splitting the full dataset into batches for training
</p>
</li>
<li>
<p>
Iterating over the batches to feed each into the network
</p>
</li>
</ol></div>
<div class="sect4">
<h5 id="concepts.neural_networks.input_pipeline.file_parsing">Parsing in the files</h5>
<div class="paragraph"><p>First of all, we need to tell tensorflow what those files actually are, i.e.
we need to provide filenames.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>filenames <span style="color: #990000">=</span> <span style="color: #990000">[</span><span style="color: #FF0000">"test.csv"</span><span style="color: #990000">]</span> <span style="font-style: italic"><span style="color: #9A1900"># provide list of strings with (CSV) filenames</span></span>
dataset <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>data<span style="color: #990000">.</span>Dataset<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">from_tensor_slices</span></span><span style="color: #990000">(</span>filenames<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>This is the initial information, the <em>tensor</em> ingredient, that the Dataset
module needs. Here, some dataset in a file <span class="monospaced">test.csv</span>.
However, we still need to parse the files. Tensorflow has the specialized
classes <span class="monospaced">tf.data.TextLineDataset</span> and  <span class="monospaced">tf.data.TFRecordDataset</span> for this
purpose. They parse text and tfrecored files. To allow using this in parallel
tensorflow offers <span class="monospaced">tf.data.Dataset.interleave()</span>, our first <em>transformation</em>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>dataset <span style="color: #990000">=</span> dataset<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">interleave</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #0000FF">lambda</span></span> filename<span style="color: #990000">:</span> <span style="color: #990000">(</span>
                    tf<span style="color: #990000">.</span>data<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">TextLineDataset</span></span><span style="color: #990000">(</span>filename<span style="color: #990000">)</span>
                        <span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">skip</span></span><span style="color: #990000">(</span><span style="color: #993399">1</span><span style="color: #990000">)</span>
                        <span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">filter</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #0000FF">lambda</span></span> line<span style="color: #990000">:</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">not_equal</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">substr</span></span><span style="color: #990000">(</span>line<span style="color: #990000">,</span> <span style="color: #993399">0</span><span style="color: #990000">,</span> <span style="color: #993399">1</span><span style="color: #990000">),</span> <span style="color: #FF0000">'#'</span><span style="color: #990000">))),</span>
                cycle_length<span style="color: #990000">=</span>self<span style="color: #990000">.</span>NUM_PARALLEL_CALLS<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>You see a <span class="monospaced">lambda</span> expression in the interleave arguments. This unnamed
function does thr following: First it instantiates one <span class="monospaced">TextLineDataset</span> class.
Subsequently, we add transformations to the text line dataset. <span class="monospaced">skip()</span> tells it
to remove the very first line (containing the header). Next, we <span class="monospaced">filter()</span> all
lines that start with a comment sign (<em>#</em>). The function is called for each
filename in <span class="monospaced">dataset</span>. The resulting file contents are weaved together allowing
for parallel execution. <span class="monospaced">cycle_length</span> states how many threads are used. (From
tf1.6 the API has changed to <span class="monospaced">parallel_interleave()</span>).
The result is that we now have a dataset consisting of single lines read from the
files.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Do not be misled: We do not have a dataset in memory now! We have only
added more nodes to the computational graph that <em>would</em> parse in the files and
split them up into single lines, when triggered to do so in graph evaluation.</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="concepts.neural_networks.input_pipeline.restructuring">Restructuring the dataset</h5>
<div class="paragraph"><p>These parsed lines need to be split up (using the "," as separator) and sorted
into features and labels. For this we need a small function that requires the
number of feature and label columns (this can be provided from the header as
well if it conforms to a certain column naming convention).</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">decode_csv_line</span></span><span style="color: #990000">(</span>line<span style="color: #990000">,</span> defaults<span style="color: #990000">,</span> input_dimension<span style="color: #990000">,</span> output_dimension<span style="color: #990000">):</span>
  items <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">decode_csv</span></span><span style="color: #990000">(</span>line<span style="color: #990000">,</span> <span style="font-weight: bold"><span style="color: #000000">list</span></span><span style="color: #990000">(</span>defaults<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">values</span></span><span style="color: #990000">()))</span>

  <span style="font-style: italic"><span style="color: #9A1900"># reshape into proper tensors</span></span>
  features <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">stack</span></span><span style="color: #990000">(</span>items<span style="color: #990000">[</span><span style="color: #993399">0</span><span style="color: #990000">:</span>input_dimension<span style="color: #990000">])</span>
  label <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">reshape</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">convert_to_tensor</span></span><span style="color: #990000">(</span>items<span style="color: #990000">[</span>input_dimension<span style="color: #990000">:</span> <span style="color: #990000">\</span>
    input_dimension<span style="color: #990000">+</span>output_dimension<span style="color: #990000">],</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>int32<span style="color: #990000">),</span> <span style="color: #990000">[</span>output_dimension<span style="color: #990000">])</span>

  <span style="font-style: italic"><span style="color: #9A1900"># return last element as label, rest as features</span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> features<span style="color: #990000">,</span> labels</tt></pre></div></div>
<div class="paragraph"><p>Note that <span class="monospaced">tf.decode_csv</span> is a tensorflow-internal function for splitting
up csv files. This function will fill missing values by using the <span class="monospaced">defaults</span>
which is a dict with every column name as key and a respective default value.</p></div>
<div class="paragraph"><p>This function is going to be used in the second "transformation", <span class="monospaced">map()</span>.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>dataset <span style="color: #990000">=</span> self<span style="color: #990000">.</span>dataset<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">map</span></span><span style="color: #990000">(</span>
                functools<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">partial</span></span><span style="color: #990000">(</span>decode_csv_line<span style="color: #990000">,</span> defaults<span style="color: #990000">=</span>defaults<span style="color: #990000">,</span>
                                  input_dimension<span style="color: #990000">=</span>input_dimension<span style="color: #990000">,</span>
                                  output_dimension<span style="color: #990000">=</span>output_dimension<span style="color: #990000">),</span>
                num_parallel_calls<span style="color: #990000">=</span>self<span style="color: #990000">.</span>NUM_PARALLEL_CALLS<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we used <span class="monospaced">functools.partial</span> to partially fix some of the arguments of
<span class="monospaced">decode_csv</span> such that it can be used as functor inside the <span class="monospaced">map()</span> call.
Notice that we may again perform this transformation in parallel when setting
<span class="monospaced">num_parallel_calls</span> appropriately.</p></div>
</div>
<div class="sect4">
<h5 id="concepts.neural_networks.input_pipeline.batching">Splitting into batches</h5>
<div class="paragraph"><p>Now, we are almost finished. The data is already available in the format
recognized by tensorflow. However, we still need to do the typical tasks of
shuffling, batching, and so on.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>dataset <span style="color: #990000">=</span> dataset<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">shuffle</span></span><span style="color: #990000">().</span><span style="font-weight: bold"><span style="color: #000000">batch</span></span><span style="color: #990000">(</span>batch_size<span style="color: #990000">)</span>
dataset <span style="color: #990000">=</span> dataset<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">repeat</span></span><span style="color: #990000">(</span><span style="font-weight: bold"><span style="color: #000000">ceil</span></span><span style="color: #990000">(</span>max_steps<span style="color: #990000">*</span>batch_size<span style="color: #990000">/</span>dimension<span style="color: #990000">))</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we need to two more pieces of informations from the user, <span class="monospaced">batch_size</span>
and <span class="monospaced">max_steps</span> which give the size of the portions of the dataset used for
feeding and the number of feeding steps in total.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/tip.png" alt="Tip">
</td>
<td class="content"><span class="monospaced">cache()</span> stores parsed in files and transformations and therefore greatly
speeds up feeding after the first epoch (after the whole dataset has been
parsed). This will give one or two orders of magnitude in performance. The best
place is right after the last <span class="monospaced">map()</span>.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/tip.png" alt="Tip">
</td>
<td class="content"><span class="monospaced">prefetch()</span> will tell tensorflow to interleave operations of fetching
and transforming data with subsequent matrix algebra tasks for a better
parallel workload. This typically gives another factor of 2 in performance. The
best place is at the very end of the <span class="monospaced">dataset</span> construction.</td>
</tr></table>
</div>
</div>
<div class="sect4">
<h5 id="concepts.neural_networks.input_pipeline.iterator">Iterating over the batches</h5>
<div class="paragraph"><p>Last but not least, we need an <em>iterator</em>. Iterators are a well-known concept
in C++ and they are the same in tensorflow. Iterators point to specific
batches of the dataset and deliver the batch on evaluation and advance, i.e.
think of it as <span class="monospaced">i++</span> in C++ lingo.</p></div>
<div class="paragraph"><p>Let us construct the iterator that will produce the batch of features (and
labels) on evaluation which we need to feed into the network.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>iterator <span style="color: #990000">=</span> dataset<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">make_initializable_iterator</span></span><span style="color: #990000">()</span>
batch_next <span style="color: #990000">=</span> iterator<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_next</span></span><span style="color: #990000">()</span></tt></pre></div></div>
<div class="paragraph"><p>We first construct an (initializable) iterator for our dataset and then we
create the node that will produce the next batch of features and labels.</p></div>
<div class="paragraph"><p>There are different flavours of iterators:</p></div>
<div class="ulist"><ul>
<li>
<p>
The standard iterator may run through the dataset just once.
</p>
</li>
<li>
<p>
The (re-)initializable iterator can go over the dataset multiple times if it
is reset.
</p>
</li>
<li>
<p>
Last, there are feedable iterators that can work on different datasets, which
allow switching between train and test dataset.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Instantiating a session object, we could now take a look at the first batch as
follows:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
  sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>iterator<span style="color: #990000">.</span>initializer<span style="color: #990000">)</span>
  features<span style="color: #990000">,</span> labels <span style="color: #990000">=</span> sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>batch_next<span style="color: #990000">)</span>
  <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">([</span>features<span style="color: #990000">,</span> labels<span style="color: #990000">])</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we first have to initialize the iterator (which also resets it to the
beginning), next we evaluate and print the first batch.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">One could also have provided the filenames through placeholders. In this
case one needs to supply additionally <span class="monospaced">feed_dict={filenames: ["..."]}</span> with
the list of files.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="concepts.neural_networks.training">2.3.3. Training</h4>
<div class="paragraph"><p>Now, we come to the last ingredient to neural network training, namely how to
do the training itself.</p></div>
<div class="paragraph"><p>Training is done by minimizing a function. The function in this context here
is called the <em>loss</em>. It compares the predicted output of the network with
the labels of the dataset.
Moreover, we need <em>gradients</em>, i.e. derivatives of the loss function with
respect to degrees of freedom of the network, weights and biases.
Finally, we need an <em>update method</em> or optimizer that refines these degrees
making use of gradients, e.g., Gradient Descent.</p></div>
<div class="paragraph"><p>Let us look at the <em>optimizer</em> first and then fill in the other gaps.
Furthermore, let us consider a concrete example, namely we want to find the
approximate minimum of the convex function $2x^2-5x+4$. Therefore,
we know it has a single global extremum (a minimum).</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> tensorflow as tf

<span style="font-style: italic"><span style="color: #9A1900"># prepare the computational graph with x and f(x) ...</span></span>
x <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variable</span></span><span style="color: #990000">(</span><span style="color: #993399">10.0</span><span style="color: #990000">)</span>
f_x <span style="color: #990000">=</span> <span style="color: #993399">2</span><span style="color: #990000">.</span> <span style="color: #990000">*</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">pow</span></span><span style="color: #990000">(</span>x<span style="color: #990000">,</span><span style="color: #993399">2</span><span style="color: #990000">.)</span> <span style="color: #990000">-</span> <span style="color: #993399">5</span><span style="color: #990000">.</span> <span style="color: #990000">*</span> x <span style="color: #990000">+</span> <span style="color: #993399">4</span><span style="color: #990000">.</span>

<span style="font-style: italic"><span style="color: #9A1900"># ... and training</span></span>
opt <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>train<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">GradientDescentOptimizer</span></span><span style="color: #990000">(</span><span style="color: #993399">0.1</span><span style="color: #990000">).</span><span style="font-weight: bold"><span style="color: #000000">minimize</span></span><span style="color: #990000">(</span>f_x<span style="color: #990000">)</span>

<span style="font-style: italic"><span style="color: #9A1900"># Then, performing training using the constructed graph</span></span>
with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
    sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">global_variables_initializer</span></span><span style="color: #990000">())</span>
    <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="color: #993399">50</span><span style="color: #990000">):</span>
        <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">([</span>opt<span style="color: #990000">,</span> x<span style="color: #990000">,</span> f_x<span style="color: #990000">]))</span></tt></pre></div></div>
<div class="paragraph"><p>We first set up the variable <span class="monospaced">x</span> that is modified during the optimization.
Next, we set up the function <span class="monospaced">f_x</span> to minimize. We instantiate the
<span class="monospaced">tf.train.GradientDescentOptimizer</span> class, giving it a learning rate or step
width of <strong>0.1</strong>, and we use <span class="monospaced">minimize()</span> to tell it which function we actually
want to minimize.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">In place of the value <strong>0.1</strong> we could also give the class a placeholder
as the learning rate. Then, we may flexibly provide the learning rate in each
iteration step and could for example tune it adaptively.</td>
</tr></table>
</div>
<div class="paragraph"><p>Notice something? We did not give the variable <span class="monospaced">x</span> to the <span class="monospaced">minimize()</span> call.
The reason is that tensorflow has an internal collection called "trainables",
use <span class="monospaced">tf.trainable_variables()</span>.
All variables are automatically added to this collection unless excluded (see
end of <a href="#concepts.neural_networks.network_topology">[concepts.neural_networks.network_topology]</a>). Gradients will always be
calculated with respect to all variables in this collection.</p></div>
<div class="paragraph"><p>Running the example for 50 steps, we obtain an approximate solution
<span class="monospaced">x=1.2500001</span> with a function value of <span class="monospaced">f(x)=0.87500024</span>. In other words, the
optimizer has been doing its job properly.</p></div>
<div class="paragraph"><p>The gradients are determined completely internally. Each (activation) function
has an encoded analytical derivative. Through chain rule, also known as back-
propagation, the gradients are computed. There is <em>no</em> numerical derivation
taking place.</p></div>
<div class="paragraph"><p>Similar to the above example, we will now also train our neural network.
The only thing we need to change is the loss function. Here, we will use the
mean squared loss $||F_D(x) - y||^2$, where $F_D(x)$ is
the black-box neural network function depending implicitly on the dataset. It
returns a vector (or matrix) of all predictions. These are compared to <strong>y</strong>, the
labels of the dataset.</p></div>
<div class="paragraph"><p>Let us setup the loss function and instantiate the optimizer.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>dataset_labels <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Placeholder</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">,</span> <span style="color: #990000">[</span>None<span style="color: #990000">,</span> <span style="color: #993399">10</span><span style="color: #990000">],</span> name<span style="color: #990000">=</span><span style="color: #FF0000">"labels"</span><span style="color: #990000">)</span>
mean_squared <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>losses<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">mean_squared_error</span></span><span style="color: #990000">(</span>labels<span style="color: #990000">=</span>dataset_labels<span style="color: #990000">,</span> predictions<span style="color: #990000">=</span>output<span style="color: #990000">)</span>
learning_rate <span style="color: #990000">-</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Placeholder</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">,</span> name<span style="color: #990000">=</span><span style="color: #FF0000">"learning_rate"</span><span style="color: #990000">)</span>
train_step <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>train<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">GradientDescentOptimizer</span></span><span style="color: #990000">(</span>learning_rate<span style="color: #990000">).</span><span style="font-weight: bold"><span style="color: #000000">minimize</span></span><span style="color: #990000">(</span>mean_squared<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>First, we need a placeholder for the labels of the dataset in the same way we
needed placeholders for the features of the dataset for the input layer.
Next, we have created the loss function <span class="monospaced">mean_squared</span> that depends on these
<span class="monospaced">dataset_labels</span> and the <span class="monospaced">output</span> layer of our network that was created
previously.
As a last step, we add the training method which receives the placeholder for
the learning rate and tell it to minimize the loss function.</p></div>
<div class="paragraph"><p>The <span class="monospaced">minimize()</span> call naturally does not perform any minimization itself but
again just returns a node in the computational graph named <span class="monospaced">train_step</span>.
This function call has added a lot of nodes to the graph that trigger
calculating the gradients and modifying the weights and biases via the update
rule. We will see how this works in detail in <a href="#extensions.samplers">[extensions.samplers]</a> covering
how to extend the capabilities of tensorflow to allow for new sampling methods.</p></div>
<div class="paragraph"><p>Evaluating <span class="monospaced">train_step</span> will trigger a single update step. Triggering it again
will cause another iteration step. Of course, this calls for a loop.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
  <span style="font-weight: bold"><span style="color: #0000FF">try</span></span><span style="color: #990000">:</span>
    <span style="font-weight: bold"><span style="color: #0000FF">while</span></span><span style="color: #990000">(</span>True<span style="color: #990000">):</span>
      <span style="font-style: italic"><span style="color: #9A1900"># parse next batch of dataset</span></span>
      batch_features<span style="color: #990000">,</span> batch_labels <span style="color: #990000">=</span> session<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">([</span>features<span style="color: #990000">,</span>labels<span style="color: #990000">])</span>
      <span style="font-style: italic"><span style="color: #9A1900"># perform single update step printing the current loss</span></span>
      <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>session<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">([</span>train_step<span style="color: #990000">,</span>mean_squared<span style="color: #990000">],</span>
        feed_dict<span style="color: #990000">={</span>
          input<span style="color: #990000">:</span> batch_features<span style="color: #990000">,</span>
          dataset_labels<span style="color: #990000">:</span> batch_labels<span style="color: #990000">,</span>
          learning_rate<span style="color: #990000">:</span> <span style="color: #993399">0.1</span><span style="color: #990000">,</span>
          <span style="color: #990000">}))</span>
  <span style="font-weight: bold"><span style="color: #0000FF">except</span></span> tf<span style="color: #990000">.</span>errors<span style="color: #990000">.</span>OutOfRangeError<span style="color: #990000">:</span>
    <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"End of dataset"</span><span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>We instantiate a session object and perform basically an infinite loop. In the
loop body we first evaluate the nodes <span class="monospaced">features</span> and <span class="monospaced">labels</span> which provide
the current batch from the dataset in return. These go into the <span class="monospaced">feed_dict</span> for
the second <span class="monospaced">session.run()</span> that performs a single update step.</p></div>
<div class="paragraph"><p>As the iterator will raise a <span class="monospaced">tf.errors.OutOfRangeError</span> exception when it
reaches the end of the dataset and we have told the dataset to repeat itself
in such a way that we may obtain <span class="monospaced">max_steps</span> batches, it will perform exactly
those number of steps and then exit the loop.</p></div>
<div class="sect4">
<h5 id="concepts.neural_networks.training.chained_pipeine">Chaining feeding and training</h5>
<div class="paragraph"><p>As a last note, we would like to show that it is possible to combine the two
<span class="monospaced">run()</span> calls in the example training loop above into a single call.</p></div>
<div class="paragraph"><p>As the <span class="monospaced">tf.data.Dataset</span> module simply adds nodes to the computational
graph that perform the parsing of the data and therefore <span class="monospaced">features</span> and
<span class="monospaced">labels</span> are nothing but nodes in this graph, we may connect them directly to
the input layer.</p></div>
<div class="paragraph"><p>In place of the input layer placeholders <span class="monospaced">x</span> we use <span class="monospaced">features</span> directly to
construct our single-layer perceptron network.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-style: italic"><span style="color: #9A1900"># ...</span></span>
pre_act <span style="color: #990000">=</span> W <span style="color: #990000">*</span> features <span style="color: #990000">+</span>b
output <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>nn<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">linear</span></span><span style="color: #990000">(</span>pre_act<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Furthermore, when creating the loss function, instead of giving the placeholder
<span class="monospaced">dataset_labels</span> for the labels we use <span class="monospaced">labels</span> directly.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-style: italic"><span style="color: #9A1900"># ...</span></span>
mean_squared <span style="color: #990000">=</span> tf<span style="color: #990000">.</span>losses<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">mean_squared_error</span></span><span style="color: #990000">(</span>labels<span style="color: #990000">=</span>labels<span style="color: #990000">,</span> predictions<span style="color: #990000">=</span>output<span style="color: #990000">)</span>
<span style="font-style: italic"><span style="color: #9A1900"># ...</span></span></tt></pre></div></div>
<div class="paragraph"><p>The loop looks then much simpler.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>feed_dict <span style="color: #990000">=</span> <span style="color: #990000">{</span> learning_rate<span style="color: #990000">:</span> <span style="color: #993399">0.1</span><span style="color: #990000">}</span>
with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
  <span style="font-weight: bold"><span style="color: #0000FF">try</span></span><span style="color: #990000">:</span>
    <span style="font-weight: bold"><span style="color: #0000FF">while</span></span><span style="color: #990000">(</span>True<span style="color: #990000">):</span>
      <span style="font-style: italic"><span style="color: #9A1900"># get next batch and perform training step on it</span></span>
      <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>session<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">([</span>train_step<span style="color: #990000">,</span>mean_squared<span style="color: #990000">],</span>
        feed_dict<span style="color: #990000">=</span>feed_dict<span style="color: #990000">))</span>
  <span style="font-weight: bold"><span style="color: #0000FF">except</span></span> tf<span style="color: #990000">.</span>errors<span style="color: #990000">.</span>OutOfRangeError<span style="color: #990000">:</span>
    <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"End of dataset"</span><span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Note that we still need a <span class="monospaced">feed_dict</span> for values such as <span class="monospaced">learning_rate</span>.
However, as the dict does not change per iteration step we may construct it
beforehand.</p></div>
<div class="paragraph"><p>In principle, this second version should save an additional copy in memory
when the provided datsaset batch is copied into the <span class="monospaced">batch_features</span> and
<span class="monospaced">batch_labels</span> variables and then into the network&#8217;s session instead of
being directly referenced within the network&#8217;s session.
However, there is no change in performance. Both versions are equally fast.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/tip.png" alt="Tip">
</td>
<td class="content">Even though in many places in the tensorflow tutorials the use of
<span class="monospaced">feed_dict</span> for feeding in the dataset is strongly discouraged and therefore
one would expect this second approach to work much faster, we have not noticed
any difference in performance between either version despite extensive tests
and measurements.</td>
</tr></table>
</div>
<div class="paragraph"><p>We presume that <span class="monospaced">batch_features</span> and <span class="monospaced">batch_labels</span> are references to the
internal arrays within the session object. This would explain why there is
no difference in performance.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/caution.png" alt="Caution">
</td>
<td class="content">The second version has the drawback that the input pipeline has to
be created first and the network crucially depends on it. It is not so
easy anymore to exchange datasets as when using placeholders to feed the
dataset. To some extent it is still possible using feedable iterators.
Hence, the second approach sacrifices flexibility of the network.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/warning.png" alt="Warning">
</td>
<td class="content">The second version has another drawback: Whenever a node is evaluated
in a separate <span class="monospaced">run()</span> call that depends on <span class="monospaced">features</span> or <span class="monospaced">labels</span> it will
trigger the dataset&#8217;s iterator to advance!</td>
</tr></table>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="extensions">3. Tensorflow extensions</h2>
<div class="sectionbody">
<div class="paragraph"><p>In this section we will elaborate on different concepts in order to extend
tensorflow to enable implementation of sampling methods. We will look ath
these concepts on a concrete example: implementing a Geometric Langevin
Algorithm sampler of 2nd order.</p></div>
<div class="sect2">
<h3 id="extensions.samplers">3.1. Sampling Methods</h3>
<div class="paragraph"><p>In the previous chapters we have learned what a computational graph is and
how tensorflow, that is based on this concept, works in principle. Moreover,
we have seen how to build a simple single-layer perceptron, how to parse
and feed the dataset, and how to perform the actual training of its weights
and biases.</p></div>
<div class="paragraph"><p>Now, we want to take this one step further by explaining how to implement
advanced sampling methods such as <a href="#SGLD">[SGLD]</a>, <a href="#GLA">[GLA]</a>, or <a href="#HMC">[HMC]</a> using
tensorflow.</p></div>
<div class="paragraph"><p>We split this part of the guide into three sections.</p></div>
<div class="olist arabic"><ol class="arabic">
<li>
<p>
First, we need to explain how an optimizer is implemented in tensorflow,
namely we will be looking at the <span class="monospaced">tf.train.GradientDescentOptimizer</span>
implementation.
</p>
</li>
<li>
<p>
then, we will talk how to extend this implementation by deriving a new
class from it which overrides certain functions.
</p>
</li>
<li>
<p>
finally, we elaborate on how to store information in local and global
variables,
</p>
</li>
<li>
<p>
and how to do branching.
</p>
</li>
</ol></div>
<div class="sect3">
<h4 id="extensions.samplers.optimizers">3.1.1. Optimizers in Tensorflow</h4>
<div class="paragraph"><p>A <a href="#GD">Gradient Descent</a> optimizer is implemented in the class
<span class="monospaced">tf.train.GradientDescentOptimizer</span>.
We first look at the base class and then see how in the framework given by the
base class a Gradient Descent update step is realized.</p></div>
<div class="sect4">
<h5 id="extensions.samplers.optimizers.base">Base class Optimizer</h5>
<div class="paragraph"><p>The base class <span class="monospaced">Optimizer</span> contains several functions, see
<span class="monospaced">tensorflow/python/training/optimizer.py</span> in your local tensorflow installation.
We will describe some of those functions briefly. However, it is not necessary
to know them by heart. We only go through this to elucidate the general
setup as given by the framework.</p></div>
<div class="paragraph"><p>In the following, the (list of) variables stands for the variables which are
modified in order to minimize the loss, i.e. the list of trainables. The
gradients are derivatives of said loss function with respect to these
variables.</p></div>
<div class="ulist"><ul>
<li>
<p>
<span class="monospaced">__init__()</span>
</p>
<div class="paragraph"><p>Instantiates this class. This may be overridden to add more variables to the
class that are needed for the sampling method, e.g., the inverse temperature
might be supplied as a placeholder.</p></div>
</li>
<li>
<p>
<span class="monospaced">minimize()</span>
</p>
<div class="paragraph"><p>Calls first <span class="monospaced">compute_gradients()</span> on the list of variables, afterwards these
are fed into <span class="monospaced">apply_gradients()</span>.</p></div>
</li>
<li>
<p>
<span class="monospaced">compute_gradients()</span>
</p>
<div class="paragraph"><p>Computes the gradients.</p></div>
</li>
<li>
<p>
<span class="monospaced">apply_gradients()</span>
</p>
<div class="paragraph"><p>Performs the update step by calling <span class="monospaced">_apply_dense()</span> (and related functions)
for each variable with its respective gradient.</p></div>
</li>
<li>
<p>
<span class="monospaced">_prepare()</span>
</p>
<div class="paragraph"><p>This is called inside <span class="monospaced">_apply_gradients()</span> and used to convert any given python
values in <span class="monospaced">__init__()</span> call to valid tensorflow tensors.</p></div>
</li>
<li>
<p>
<span class="monospaced">_apply_dense()</span>
</p>
<div class="paragraph"><p>Adds nodes to the computational graph to perform the update of the a particular
set of variables for a single step.
There are different variants of this function such as <span class="monospaced">__apply_sparse()</span> or
<span class="monospaced">_resource_apply_dense()</span> that distinguish between the character of the variable
that is modified: Is it a dense or sparse tensor, is it a "special" resource
variable and so on.</p></div>
</li>
<li>
<p>
<span class="monospaced">_create_slots()</span>
</p>
<div class="paragraph"><p>Creates a new variable that is directly associated with a present tensor. These
are called <em>slots</em>. Here, this tensor is typically one of the trainable
variables.</p></div>
</li>
<li>
<p>
<span class="monospaced">_zeros_slot()</span>
</p>
<div class="paragraph"><p>Creates a new variable as does <span class="monospaced">_create_slot()</span> but also sets its components to
zero intially.</p></div>
</li>
<li>
<p>
<span class="monospaced">get_slot()</span>
Returns a created slot by its (unique) name. The name is the identifier of each
slot variable.
</p>
</li>
</ul></div>
<div class="paragraph"><p>Note that there are three types of functions.</p></div>
<div class="ulist"><ul>
<li>
<p>
initialization
</p>
</li>
<li>
<p>
computing gradients and applying update
</p>
</li>
<li>
<p>
creating extra variables ("slots")
</p>
</li>
</ul></div>
<div class="paragraph"><p>Moreover, there is a certain hierarchy of functions for computing gradients and
applyting them.
<span class="monospaced">minimize()</span> is the function that we have used already. It simply combines the
calls of two other functions. Then, we are leaving the official part of the
Python interface and come to functions that are considered private.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Python functions starting with an underscore are private by naming
convention but they are not protected from any actual access.</td>
</tr></table>
</div>
<div class="paragraph"><p>One of the these two called functions is <span class="monospaced">_apply_gradients()</span> which uses
<span class="monospaced">_prepare()</span> and <span class="monospaced">_apply_dense()</span> (we will ignore the other variants in
this guide as the changes would be equivalent). The latter is the main work
horse.</p></div>
<div class="paragraph"><p><strong>It is of utmost importance to fully realize the following bits:</strong></p></div>
<div class="ulist"><ul>
<li>
<p>
<span class="monospaced">_apply_dense()</span> is just called once per variable, e.g., for single-layer
perceptron it will be called two times: once for the weights <strong>W</strong> and once for
the biases <strong>b</strong>. That&#8217;s it. In other words, you cannot perform any calculations
in Python code which need to be done every step. You can, however, perform
calculations that are required initially, i.e. before all steps.
</p>
</li>
<li>
<p>
<span class="monospaced">_apply_dense()</span> is still called multiple times, once for each trainable
variable. This means that you cannot distinguish in that function between
computations you only want done for the biases and not for the weights. The
same function will be called for both weights and biases.
</p>
</li>
</ul></div>
</div>
<div class="sect4">
<h5 id="extensions.samplers.optimizers.gradient_descent">GradientDescentOptimizer</h5>
<div class="paragraph"><p>Let us now turn to the implementation of the Gradient Descent optimizer in the
tensorflow code base, see <span class="monospaced">tensorflow/python/training/gradient_descent.py</span>.</p></div>
<div class="ulist"><ul>
<li>
<p>
<span class="monospaced">__init__()</span>
</p>
<div class="paragraph"><p>Overrides the base function to store the variable <span class="monospaced">learning_rate</span> received as
parameter in a member variable.</p></div>
</li>
<li>
<p>
<span class="monospaced">_prepare()</span>
</p>
<div class="paragraph"><p>Converts the received variable into a proper tensorflow tensor using
<span class="monospaced">convert_to_tensor()</span>. This step is necessary as the <span class="monospaced">GradientDescentOptimizer</span>
can also be a given a python <span class="monospaced">float</span> as the learning rate.</p></div>
</li>
<li>
<p>
<span class="monospaced">_apply_dense()</span>
</p>
<div class="paragraph"><p>Implements the actual update step.</p></div>
</li>
</ul></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">We again ignore all the other implementations in the functions related
to <span class="monospaced">_apply_dense()</span></td>
</tr></table>
</div>
<div class="paragraph"><p>The actual code inside <span class="monospaced">_apply_dense()</span> is not very illustrative. There is a lot
of abstraction code to switch between eager and static execution and so forth.
Eventually and hidden deep inside the tensorflow code base, the code
makes use of a very efficient implementation in C++ to perform the actual
update step that performs well on both CPUs and GPUs.</p></div>
<div class="paragraph"><p>Therefore, let&#8217;s do the (python) implementation ourselves in the following
example. Remember that the update step in Gradient Descent is
$x_{n+1} = x_n - \lambda \nabla_{x} L(x_n)$, where we have called the
current variable tensor <strong>x</strong>, $\lambda$ is the scalar learning rate,
and $\nabla_x L(x_n)$ is the gradient of the loss function <strong>L</strong> with
respect t the variable tensor.</p></div>
<div class="listingblock">
<div class="title">Gradient Descent optimizer</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">from</span></span> tensorflow<span style="color: #990000">.</span>python<span style="color: #990000">.</span>ops <span style="font-weight: bold"><span style="color: #000080">import</span></span> control_flow_ops
<span style="font-weight: bold"><span style="color: #000080">from</span></span> tensorflow<span style="color: #990000">.</span>python<span style="color: #990000">.</span>ops <span style="font-weight: bold"><span style="color: #000080">import</span></span> state_ops
<span style="font-weight: bold"><span style="color: #000080">from</span></span> tensorflow<span style="color: #990000">.</span>python<span style="color: #990000">.</span>training <span style="font-weight: bold"><span style="color: #000080">import</span></span> optimizer

<span style="font-weight: bold"><span style="color: #0000FF">class</span></span> <span style="font-weight: bold"><span style="color: #000000">GradientDescentOptimizer</span></span><span style="color: #990000">(</span>optimizer<span style="color: #990000">):</span>
<span style="color: #990000">...</span>
  <span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">_apply_dense</span></span><span style="color: #990000">(</span>self<span style="color: #990000">,</span> grad<span style="color: #990000">,</span> var<span style="color: #990000">):</span>
    scaled_gradient <span style="color: #990000">=</span> grad <span style="color: #990000">*</span> self<span style="color: #990000">.</span>_learning_rate_tensor <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
    var_update_t <span style="color: #990000">=</span> state_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign_sub</span></span><span style="color: #990000">(</span>var<span style="color: #990000">,</span> scaled_gradient<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
    <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> control_flow_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">group</span></span><span style="color: #990000">(*[</span>var_update_t<span style="color: #990000">])</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
<span style="color: #990000">...</span></tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
First, inside <span class="monospaced">_apply_dense()</span> we create a helper node that is referenced
by <span class="monospaced">scaled_gradient</span> where we simply multiply the converted learning rate
with the gradient tensor <span class="monospaced">grad</span>.
</p>
</li>
<li>
<p>
Next, we perform the actual update of the variable tensor <span class="monospaced">var</span> using an
assignment operation. Here, we use a special variant of it that subtracts
the given value from the tensor.
</p>
</li>
<li>
<p>
The last steps consists of returning a list of nodes that need to be
executed to fully perform the update on this particular variable tensor. In our
case it is just a single node, namely <span class="monospaced">var_update_t</span> that is referencing the
update stop.
</p>
</li>
</ol></div>
<div class="paragraph"><p>Note that <span class="monospaced">var_update_t</span> depends on <span class="monospaced">scaled_grad</span>. Hence, we do not have to
give the latter in the returned list of nodes as it will be evaluated (and
updated) automatically.</p></div>
<div class="paragraph"><p>Again, this function is executed once and only once per variable adding all
the nodes to the computational graph to perform a single training step.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="extensions.samplers.simple">3.1.2. Techniques for simple Samplers</h4>
<div class="paragraph"><p>We have seen how the <a href="#GD">[GD]</a> optimizer was implemented in the last section.
Next, we will implement our first sampler. In order to make this a little
interesting, we look at <a href="#GLA">[GLA]</a>2, see <a href="#Trstanova2016">[Trstanova, 2016,
(1.59)</a>].</p></div>
<div class="paragraph"><p>Let us first look at the formulas that perform the descrete time integration.</p></div>
<div class="openblock" id="extensions.samplers.simple.gla2">
<div class="title">Geometric Langevin Algorithm 2nd order</div>
<div class="content">
<div class="olist arabic"><ol class="arabic">
<li>
<p>
$p_{n+\tfrac 1 2} = p_n - \tfrac {\lambda}{2} \nabla_x L(x_n)$
</p>
</li>
<li>
<p>
$x_{n+1} = x_n + \lambda p_{n+\tfrac 1 2}$
</p>
</li>
<li>
<p>
$\widehat{p}_{n+1} = p_{n+\tfrac 1 2} - \tfrac {\lambda}{2} \nabla_x L(x_{n+1})$
</p>
</li>
<li>
<p>
$p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n$
</p>
</li>
</ol></div>
</div></div>
<div class="paragraph"><p>Here, $\eta_n$ is the random noise at step <strong>n</strong>, $p_n$
designates momentum, and moreover we have $\alpha = \exp(-\gamma \cdot \lambda)$
with the so-called friction constant $\gamma$.</p></div>
<div class="sect4">
<h5 id="extensions.samplers.simple.slots">Variables of the first kind: slots</h5>
<div class="paragraph"><p>First of all, we need more variables. $x_n$ is given by <span class="monospaced">var</span>
and $\nabla_x L(x_n)$ is given by <span class="monospaced">grad</span>. Both are simply parameters
to the <span class="monospaced">_apply_dense()</span> function. However, we lack a variable to store the
momenta $p_n$.</p></div>
<div class="paragraph"><p>For this purpose, we can use <em>slots</em>. Slots are an official mechanism of
tensorflow&#8217;s <span class="monospaced">Optimizer</span> for this particular purpose as there are other, more
advanced optimizers such as ADAM that actually use momenta as well. We will
see how these are constructed in the new <span class="monospaced">_prepare()</span> function.</p></div>
<div class="paragraph"><p>However before that, we still need more variables in the form of placeholders:
inverse temperature $\beta$,
friction constant $\gamma$
and the learning rate $\lambda$
which we will call <em>step width</em> to distinguish samplers from optimizers.</p></div>
<div class="paragraph"><p>and there is one more: GLA2 contains a noise term $\eta_n$.
To this end, we require a random number generator that produces random numbers
of the same <em>shape</em> as <span class="monospaced">var</span> (or equivalently <span class="monospaced">grad</span>). These are created by <span class="monospaced">tf.random_uniform()</span>
as you will see further below in <span class="monospaced">_prepare_dense()</span>.
To reproducibly create the noise, this function takes a random number seed.
This seed value needs to come from an extra placeholder.</p></div>
<div class="paragraph"><p>Then, all of these are handed on to our new class <span class="monospaced">GLA2Sampler</span> in its
constructor.</p></div>
<div class="listingblock">
<div class="title">Constructor for class <span class="monospaced">GLA2Sampler</span></div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">__init__</span></span><span style="color: #990000">(</span>self<span style="color: #990000">,</span> step_width<span style="color: #990000">,</span> inverse_temperature<span style="color: #990000">,</span> friction_constant<span style="color: #990000">,</span>
    seed<span style="color: #990000">=</span>None<span style="color: #990000">,</span> use_locking<span style="color: #990000">=</span>False<span style="color: #990000">,</span> name<span style="color: #990000">=</span><span style="color: #FF0000">"GLA2"</span><span style="color: #990000">):</span>
    <span style="font-weight: bold"><span style="color: #000000">super</span></span><span style="color: #990000">(</span>GLA2Sampler<span style="color: #990000">,</span> self<span style="color: #990000">).</span><span style="font-weight: bold"><span style="color: #000000">__init__</span></span><span style="color: #990000">(</span>use_locking<span style="color: #990000">,</span> name<span style="color: #990000">)</span>
    self<span style="color: #990000">.</span>_friction_constant <span style="color: #990000">=</span> friction_constant
    self<span style="color: #990000">.</span>_step_width <span style="color: #990000">=</span> step_width
    self<span style="color: #990000">.</span>_seed <span style="color: #990000">=</span> seed
    self<span style="color: #990000">.</span>_inverse_temperature <span style="color: #990000">=</span> inverse_temperature</tt></pre></div></div>
<div class="paragraph"><p>You notice that we simply store all the obtained parameters using (private)
member variables.</p></div>
<div class="paragraph"><p>As these parameters could have been either python constants or tensorflow
tensors, we convert each into a valid tensor in <span class="monospaced">_prepare()</span>.</p></div>
<div class="listingblock">
<div class="title">Converting value to tensors</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">_prepare</span></span><span style="color: #990000">():</span>
  self<span style="color: #990000">.</span>_step_width_t <span style="color: #990000">=</span> ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">convert_to_tensor</span></span><span style="color: #990000">(</span>self<span style="color: #990000">.</span>_step_width<span style="color: #990000">,</span>
      name<span style="color: #990000">=</span><span style="color: #FF0000">"step_width"</span><span style="color: #990000">)</span>
  self<span style="color: #990000">.</span>_friction_constant <span style="color: #990000">=</span> ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">convert_to_tensor</span></span><span style="color: #990000">(</span>self<span style="color: #990000">.</span>_friction_constant<span style="color: #990000">,</span>
      name<span style="color: #990000">=</span><span style="color: #FF0000">"friction_constant"</span><span style="color: #990000">)</span>
  self<span style="color: #990000">.</span>_inverse_temperature_t <span style="color: #990000">=</span> ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">convert_to_tensor</span></span><span style="color: #990000">(</span>self<span style="color: #990000">.</span>_inverse_temperature<span style="color: #990000">,</span>
      name<span style="color: #990000">=</span><span style="color: #FF0000">"inverse_temperature"</span><span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Preparation is done in two steps:
In the first step, see <span class="monospaced">_prepare()</span> above, we have converted any python value
which we might have received in the <span class="monospaced">__init__()</span> call into full tensorflow
tensors.
In the second call, we cast tensors to the right type, e.g., <span class="monospaced">tf.float32</span> and
produce the random number tensor which we need for the noise. Let us take
a look.</p></div>
<div class="listingblock">
<div class="title">Casting to the correct type and random number tensor</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">_prepare_dense</span></span><span style="color: #990000">():</span>
  step_width_t <span style="color: #990000">=</span> math_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">cast</span></span><span style="color: #990000">(</span>self<span style="color: #990000">.</span>_step_width_t<span style="color: #990000">,</span> var<span style="color: #990000">.</span>dtype<span style="color: #990000">.</span>base_dtype<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  friction_constant_t <span style="color: #990000">=</span> math_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">cast</span></span><span style="color: #990000">(</span>self<span style="color: #990000">.</span>_friction_constant_t<span style="color: #990000">,</span>
      var<span style="color: #990000">.</span>dtype<span style="color: #990000">.</span>base_dtype<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  inverse_temperature_t <span style="color: #990000">=</span> math_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">cast</span></span><span style="color: #990000">(</span>self<span style="color: #990000">.</span>_inverse_temperature_t<span style="color: #990000">,</span>
      var<span style="color: #990000">.</span>dtype<span style="color: #990000">.</span>base_dtype<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">if</span></span> self<span style="color: #990000">.</span>_seed <span style="font-weight: bold"><span style="color: #0000FF">is</span></span> None<span style="color: #990000">:</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
      random_noise_t <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random_normal</span></span><span style="color: #990000">(</span>grad<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_shape</span></span><span style="color: #990000">(),</span> mean<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">.,</span>stddev<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">.,</span>
          dtype<span style="color: #990000">=</span>var<span style="color: #990000">.</span>dtype<span style="color: #990000">.</span>base_dtype<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">else</span></span><span style="color: #990000">:</span>
      <span style="font-style: italic"><span style="color: #9A1900"># increment such that we use different seed for each random tensor</span></span>
      self<span style="color: #990000">.</span>_seed <span style="color: #990000">+=</span> <span style="color: #993399">1</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span>
      random_noise_t <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random_normal</span></span><span style="color: #990000">(</span>grad<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_shape</span></span><span style="color: #990000">(),</span> mean<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">.,</span> stddev<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">.,</span>
          dtype<span style="color: #990000">=</span>var<span style="color: #990000">.</span>dtype<span style="color: #990000">.</span>base_dtype<span style="color: #990000">,</span> seed<span style="color: #990000">=</span>self<span style="color: #990000">.</span>_seed<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> step_width_t<span style="color: #990000">,</span> inverse_temperature_t<span style="color: #990000">,</span> friction_constant_t<span style="color: #990000">,</span> random_noise_t</tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
In the first three lines we simply use <span class="monospaced">math_ops.cast()</span> to convert the
given parameter tensor to the same type as <span class="monospaced">var</span>. This type is contained in
<span class="monospaced">var.dtype.base_dtype</span>.
</p>
</li>
<li>
<p>
we branch depending on whether the given <span class="monospaced">seed</span> value, stored in the
member variable <span class="monospaced">_seed</span> is <span class="monospaced">None</span> or not.
</p>
</li>
<li>
<p>
If the seed is <span class="monospaced">None</span>, i.e. not set, then we create the random noise tensor
<span class="monospaced">random_noise_t</span> without specifying a seed. This will cause the random number
generator to pick a different seed every time the program is executed.
</p>
</li>
<li>
<p>
If a seed is given on the other hand, then it is used in the constructor
call using <span class="monospaced">tf.random_normal(..)</span> which returns a normally distributed set of
random variables of the same shape as <span class="monospaced">grad</span>. The shape is obtained through
<span class="monospaced">grad.get_shape()</span>. Each variable has a <span class="monospaced">mean of *0.* and a standard deviation
`stddev</span> of <strong>1.</strong>.
Moreover, we use again the same type as that of <span class="monospaced">var</span>.
</p>
</li>
<li>
<p>
We increment the seed as <span class="monospaced">_prepare_dense()</span> is called multiple times, ones
per trainable variable just as <span class="monospaced">_apply_dense()</span> and we need to use different
random numbers each time.
</p>
</li>
</ol></div>
</div>
<div class="sect4">
<h5 id="extensions.samplers.simple.rearranging">Rearranging integration steps</h5>
<div class="paragraph"><p>Next, having now all variables at hand, we may come to actual implementation
of the time integration steps, see <a href="#extensions.samplers.simple.gla2">GLA2</a>
for the formulas.</p></div>
<div class="paragraph"><p>However, if we want to translate the set of equations directly into tensorflow
instructions inside the body of <span class="monospaced">_apply_dense()</span> we face a restriction.
Remember that <span class="monospaced">_apply_dense()</span> is called inside <span class="monospaced">_apply_gradients()</span> which is
called after <span class="monospaced">compute_gradients()</span>.
In other words, the gradients have been computed just before.</p></div>
<div class="paragraph"><p>Hence, in the initial step we have an evaluation of term $\nabla_x L(x_n)$
and we cannot trigger a second evaluation inside <span class="monospaced">_apply_dense()</span> to compute
$\nabla_x L(x_{n+1})$. If we wanted to do this, we would have to
completely rewrite the base <span class="monospaced">Optimizer</span> class.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/warning.png" alt="Warning">
</td>
<td class="content">Rewriting tensorflow classes is perfectly possible. However, tensorflow
only tries to keep the official public part of API unchanged. As most of
the <span class="monospaced">Optimizer</span> class functions are private, their bodies and their signature
may change with future versions of tensorflow and even without official notice.
Tensorflow updates roughly every two months. In general, this would  lock the
applicability of such an implementation to a very specific tensorflow version.</td>
</tr></table>
</div>
<div class="paragraph"><p>However, there is another solution to this. Remember that the list of four
steps above is continued for a certain number of steps, i.e. after step <em>4.</em> in
<a href="#extensions.samplers.simple.gla2">GLA2</a> at iteration <strong>n</strong> follows step <em>1.</em>
at <strong>n+1</strong>.
In other words, we may cyclically rearrange the steps, ignoring that the first
step is then no longer correct.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/caution.png" alt="Caution">
</td>
<td class="content">It is still important <em>where</em> to evaluate quantities such as kinetic
energy. I.e. their evaluation must be cyclically rearranged in accordance.</td>
</tr></table>
</div>
<div class="paragraph"><p>We will implement the steps in the following order: 3., 4., evaluate kinetic
energy and so on, then 1. and 2. In other words, instead of BABO we execute
BOBA. See <a href="#Leimkuhler2012">[Leimkuhler2012]</a> for the nomenclature of the steps <strong>A</strong>, <strong>B</strong>, <strong>O</strong>.</p></div>
<div class="listingblock">
<div class="title">Implementing GLA2 with tensorflow</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">_apply_dense</span></span><span style="color: #990000">(</span>self<span style="color: #990000">,</span> grad<span style="color: #990000">,</span> var<span style="color: #990000">):</span>
  step_width<span style="color: #990000">,</span> beta<span style="color: #990000">,</span> gamma<span style="color: #990000">,</span> random_noise_t <span style="color: #990000">=</span> self<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">_prepare_dense</span></span><span style="color: #990000">(</span>grad<span style="color: #990000">,</span> var<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  momentum <span style="color: #990000">=</span> self<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_slot</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"momentum"</span><span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
  scaled_gradient <span style="color: #990000">=</span> grad <span style="color: #990000">*</span> step_width_t <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 3. \widehat{p}_{n+1} = p_{n+\tfrac 1 2} - \tfrac {\lambda}{2} \nabla_x L(x_{n+1})</span></span>
  momentum_half_step <span style="color: #990000">=</span> momentum <span style="color: #990000">-</span> <span style="color: #993399">0.5</span> <span style="color: #990000">*</span> scaled_gradient <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 4. p_{n+1} = \alpha \widehat{p}_{n+1} + \sqrt{\frac{1-\alpha^2}{\beta}} \cdot \eta_n</span></span>
  alpha <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">exp</span></span><span style="color: #990000">(-</span>friction_constant <span style="color: #990000">*</span> step_width<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>
  noise_scale <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">sqrt</span></span><span style="color: #990000">((</span><span style="color: #993399">1</span><span style="color: #990000">.-</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">pow</span></span><span style="color: #990000">(</span>alpha<span style="color: #990000">,</span> <span style="color: #993399">2</span><span style="color: #990000">))/</span>inverse_temperature<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>
  scaled_noise <span style="color: #990000">=</span> noise_scale <span style="color: #990000">*</span> random_noise_t <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>
  momentum_noise_step <span style="color: #990000">=</span> alpha <span style="color: #990000">*</span> momentum_half_step <span style="color: #990000">+</span> scaled_noise <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 1. p_{n+\tfrac 1 2} = p_n - \tfrac {\lambda}{2} \nabla_x L(x_n)</span></span>
  momentum_t <span style="color: #990000">=</span> momentum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>momentum_noise_step <span style="color: #990000">-</span> <span style="color: #993399">0.5</span> <span style="color: #990000">*</span> scaled_gradient<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 2. x_{n+1} = x_n + \lambda p_{n+\tfrac 1 2}</span></span>
  var_update_t <span style="color: #990000">=</span> state_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign_add</span></span><span style="color: #990000">(</span>var<span style="color: #990000">,</span> step_width <span style="color: #990000">*</span> momentum_t<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;6&gt;</b></span></span>

  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> control_flow_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">group</span></span><span style="color: #990000">(*[</span>var_update<span style="color: #990000">])</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;7&gt;</b></span></span></tt></pre></div></div>
<div class="paragraph"><p>As this is quite a large example, let&#8217;s go through the lines step by step.</p></div>
<div class="colist arabic"><ol>
<li>
<p>
First, we use <span class="monospaced">_prepare_dense()</span> to obtain the three tensors containing
our parameters <span class="monospaced">step_width</span> $\delta t$, <span class="monospaced">beta</span> $\beta$,
and <span class="monospaced">gamma</span> $\gamma$. Moreover, we get the random number
tensor that is our source of normally distributed noise $\eta_n$.
</p>
</li>
<li>
<p>
Next, we get the slot variable <span class="monospaced">momentum</span>. We have obtained <span class="monospaced">grad</span> and
<span class="monospaced">var</span> as parameters to <span class="monospaced">_apply_dense()</span>.
</p>
</li>
<li>
<p>
Then comes the first momentum integration step, <strong>B</strong>. We create a helper
node <span class="monospaced">scaled_gradient</span> that contains the gradient tensor scaled by the step
width. Then, we assign the update to a temporary tensor named
<span class="monospaced">momentum_half_step</span>.
</p>
</li>
<li>
<p>
For the following <strong>O</strong> step, we need to compute $\alpha$. To this
end, we create several temporary nodes <span class="monospaced">alpha</span>, <span class="monospaced">noise_scale</span>, and finally
<span class="monospaced">scaled_noise</span> to obtain the factor in the first in <span class="monospaced">alpha</span> and the second term
in <span class="monospaced">scaled_noise</span> of step <strong>4.</strong> in <a href="#extensions.samplers.simple.gla2">GLA2</a>.
</p>
</li>
<li>
<p>
Now, we are actually at step <strong>1.</strong> of the split formulation for the
Langevin dynamics time integration. We perform another <strong>B</strong> step. However, this
time we assign the result back into the <span class="monospaced">momentum</span> slot. This action will be
done whenever we evaluate <span class="monospaced">momentum_t</span>: it will <em>both</em> return the momentum after
the second <strong>B</strong> step <em>and</em> assign its value to the slot <span class="monospaced">momentum</span>!
</p>
</li>
<li>
<p>
Finally, we come to the <strong>A</strong> step where the variables in <span class="monospaced">var</span> are updated
using the updated momentum in <span class="monospaced">momentum_t</span> just as in step <strong>2.</strong>.
</p>
</li>
<li>
<p>
The very last step consists of returning a list of nodes, here
<span class="monospaced">[var_update_t]</span> to trigger the actual evaluation.
</p>
</li>
</ol></div>
<div class="paragraph"><p>Let us elaborate a bit why in the last step we only need a single tensor in the
list: <span class="monospaced">var_update_t</span> depends on <span class="monospaced">momentum_t</span>. Hence, it triggers the evaluation
of that node. <span class="monospaced">momentum_t</span> depends on <span class="monospaced">momentum_noise_stept</span> and <span class="monospaced">scaled_gradient</span>.
Hence, it will trigger those two nodes. <span class="monospaced">momentum_noise_step</span> in turn triggers
evaluation of <span class="monospaced">alpha</span>, <span class="monospaced">momentum_half_step</span>, and <span class="monospaced">scaled_noise</span>. Finally,
<span class="monospaced">scaled_noise</span> depends on <span class="monospaced">noise_scale</span> and <span class="monospaced">random_noise_t</span>.</p></div>
<div class="paragraph"><p>So, all created nodes are actually evaluated when <span class="monospaced">var_update_t</span> 's evaluation
is triggered.</p></div>
<div class="paragraph"><p>And that&#8217;s it. We are done with adding a GLA2 sampler.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">It is a good idea to adhere to a certain naming convention: Nodes that
actually modify the state&#8201;&#8212;&#8201;by assigning a new value to a slot or by
calling a random number generator which modifies its internal state, &#8230;&#8201;&#8212;&#8201;are
suffixed by <span class="monospaced">.._t</span> as a reminder that these are tensorflow <strong>tensors</strong>. All
helper nodes in the above example do not have this suffix.</td>
</tr></table>
</div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/caution.png" alt="Caution">
</td>
<td class="content">The above lines could principally be in any order as the order of
execution comes purely from their dependence on one another in the computational
graph. They are still in order because during construction we need the right
python variables referencing tensorflow nodes at hand.
We will come to this point in the next session.</td>
</tr></table>
</div>
</div>
</div>
<div class="sect3">
<h4 id="extensions.samplers.variables">3.1.3. Local and global variables</h4>
<div class="paragraph"><p>Having the sampling method implemented, we would like to test it. However, we
can only access the loss at the moment. We do not know the kinetic energy or
other quantities of interest because they are hidden away in some nodes in the
computational graph.</p></div>
<div class="paragraph"><p>Tensorflow maintains a dictionary of all nodes in its graph by which we could
try to access the <span class="monospaced">momentum</span> slots. The key is always the <span class="monospaced">name</span> of the node.
Therefore, we could access momenta by an additional <span class="monospaced">Session.run()</span> call after
the sampling step.
However, then we cannot evaluate the kinetic energy at the right point, namely
just before step 5 and not after step 7 or before step 1.</p></div>
<div class="paragraph"><p>In essence, we need <em>more variables</em> where we can store the contribution to the
kinetic energy of the respective trainable variable and access it later on.</p></div>
<div class="paragraph"><p>There will be four pieces to this puzzle which we discuss one by one.</p></div>
<div class="ulist"><ul>
<li>
<p>
create the variable
</p>
</li>
<li>
<p>
access and assign the variable
</p>
</li>
<li>
<p>
evaluate the variable
</p>
</li>
<li>
<p>
"reset" the variable (which is the same as assigning it)
</p>
</li>
</ul></div>
<div class="sect4">
<h5 id="extensions.samplers.variables.create_resource">Creating a resource variable</h5>
<div class="paragraph"><p>While the <span class="monospaced">momentum</span> slot is <em>local</em> to the specific <span class="monospaced">_apply_dense()</span> context,
these variables will have a <em>global</em> character to them. Therefore, they are
created outside of the <span class="monospaced">_apply_dense()</span> somewhere before we instantiate the
sampler <span class="monospaced">GLA2Sampler</span>.</p></div>
<div class="listingblock">
<div class="title">Resource variable</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">variable_scope</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"accumulate"</span><span style="color: #990000">,</span> reuse<span style="color: #990000">=</span>False<span style="color: #990000">):</span>
      tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_variable</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"kinetic_energy"</span><span style="color: #990000">,</span> shape<span style="color: #990000">=[],</span> trainable<span style="color: #990000">=</span>False<span style="color: #990000">,</span>
                      initializer<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>zeros_initializer<span style="color: #990000">,</span>
                      use_resource<span style="color: #990000">=</span>True<span style="color: #990000">,</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Here, we first set a variable scope: all variables in that scope will have
their names prefixed with "accumulate/", i.e. the name of the kinetic energy
variable is "accumulate/kinetic_energy".
We mark it as not trainable, after all it is designed as pure storage.
Its shape defines it as a scalar quantity. Its <span class="monospaced">dtype</span> is simply <span class="monospaced">tf.float32</span>.
Note this should match with the type of the trainable variables, see
<a href="#concepts.neural_networks.network_topology.output">Output layer</a> where we
created them for the first time.
Moreover, we initialize it to zero. See <a href="#concepts.tensorflow.variables">Variables</a>
on a reminder that variables need to be set to a specific value initially.</p></div>
<div class="paragraph"><p>There is one element we have not encountered: <span class="monospaced">use_resource</span>.
At the time of writing it is not clear to the author what this really does.
Empirical evidence showed that it made using these variables as global variables
more robust. Tensorflow&#8217;s API referenced them for a long time as "experimental".
Other variables are not created as <em>resource</em> variables by default.</p></div>
</div>
<div class="sect4">
<h5 id="extensions.samplers.variables.assign_resource">Assigning to a resource variable</h5>
<div class="paragraph"><p>We can get hold of this variable in much the same way we got a slot before.</p></div>
<div class="listingblock">
<div class="title">Getting a resource variable&#8217;s reference</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">variable_scope</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"accumulate"</span><span style="color: #990000">,</span> reuse<span style="color: #990000">=</span>True<span style="color: #990000">):</span>
      kinetic_energy <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_variable</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"kinetic_energy"</span><span style="color: #990000">,</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Before you strain your eyes too hard to spot the <em>important difference</em>, let us
hightlight it: <span class="monospaced">reuse=True</span> means that if the variable is already
present, tensorflow simply returns a reference. Moreover, this time we store
the reference that <span class="monospaced">tf.get_variable()</span> returns in <span class="monospaced">kinetic_energy</span>.</p></div>
<div class="paragraph"><p>Then, we may extend our present implementation of GLA2 in <span class="monospaced">_apply_dense()</span> in
the following way to also accumulate the kinetic energy.</p></div>
<div class="listingblock">
<div class="title">Adding kinetic energy accumulation to GLA2.</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">_apply_dense</span></span><span style="color: #990000">(</span>self<span style="color: #990000">,</span> grad<span style="color: #990000">,</span> var<span style="color: #990000">):</span>
  <span style="color: #990000">...</span>
  momentum_noise_step <span style="color: #990000">=</span> alpha_t <span style="color: #990000">*</span> momentum_half_step_t <span style="color: #990000">+</span> scaled_noise

  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">variable_scope</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"accumulate"</span><span style="color: #990000">,</span> reuse<span style="color: #990000">=</span>True<span style="color: #990000">):</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
    kinetic_energy <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">get_variable</span></span><span style="color: #990000">(</span><span style="color: #FF0000">"kinetic_energy"</span><span style="color: #990000">,</span> dtype<span style="color: #990000">=</span>dds_basetype<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
    <span style="font-style: italic"><span style="color: #9A1900"># 1/2 p_{n}^t p_{n}</span></span>
    momentum_sq <span style="color: #990000">=</span> <span style="color: #993399">0.5</span> <span style="color: #990000">*</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">reduce_sum</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">multiply</span></span><span style="color: #990000">(</span>momentum_noise_step<span style="color: #990000">,</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
      momentum_noise_step<span style="color: #990000">))</span>
    kinetic_energy_t <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign_add</span></span><span style="color: #990000">(</span>kinetic_energy<span style="color: #990000">,</span> momentum_sq<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>

  <span style="font-style: italic"><span style="color: #9A1900"># 1. p_{n+\tfrac 1 2} = p_n - \tfrac {\lambda}{2} \nabla_x L(x_n)</span></span>
  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">control_dependencies</span></span><span style="color: #990000">([</span>kinetic_energy_t<span style="color: #990000">]):</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
      momentum_t <span style="color: #990000">=</span> momentum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>momentum_noise_step <span style="color: #990000">-</span> <span style="color: #993399">0.5</span> <span style="color: #990000">*</span> scaled_gradient<span style="color: #990000">)</span>
  <span style="color: #990000">....</span>
  <span style="font-style: italic"><span style="color: #9A1900"># 2. x_{n+1} = x_n + \lambda p_{n+\tfrac 1 2}</span></span>
  var_update_t <span style="color: #990000">=</span> state_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign_add</span></span><span style="color: #990000">(</span>var<span style="color: #990000">,</span> step_width_t <span style="color: #990000">*</span> momentum_t<span style="color: #990000">)</span>

  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> control_flow_ops<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">group</span></span><span style="color: #990000">(*[</span>var_update<span style="color: #990000">,</span> kinetic_energy_t<span style="color: #990000">])</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span></tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
We obtain a reference to the already created resource variable.
</p>
</li>
<li>
<p>
Next, we add the contribution from $\tfrac 1 2 p_{n}^t p_{n}$ to
the resource variable.
</p>
</li>
<li>
<p>
Adding a <span class="monospaced">tf.control_dependencies()</span> in a <span class="monospaced">with</span> context
instructs tensorflow that all nodes in the list have to be evaluated before any
of the statements inside the context are evaluated. This ensures that we add
the kinetic energy contribution before continuing with step <strong>1.</strong> in
<a href="#extensions.samplers.simple.gla2">GLA2</a>.
Remember that evaluation is determined by dependence and not by order of
appearance in the python code.
</p>
</li>
<li>
<p>
We need to make sure that the assignment actually takes place by adding
it to the list of returned variables, as <span class="monospaced">var_update_t</span> does not depend on it.
</p>
</li>
</ol></div>
</div>
<div class="sect4">
<h5 id="extensions.samplers.variables.evaluating_resource">Evaluating a resource variable</h5>
<div class="paragraph"><p>Evaluating the variable is simply the same as any other node in tensorflow,
using the <span class="monospaced">run()</span> statement of a session object. Assume we have obtained the
reference to the kinetic energy as <span class="monospaced">kinetic_energy</span>, see
<a href="#extensions.samplers.variables.assign_resource">Assigning a resource</a>.</p></div>
<div class="listingblock">
<div class="title">Evaluating the kinetic energy</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
  <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>kinetic_energy<span style="color: #990000">))</span></tt></pre></div></div>
</div>
<div class="sect4">
<h5 id="extensions.samplers.variables.zero_resource">Zeroing to a resource variable</h5>
<div class="paragraph"><p>There is one last step: If we want to have the current kinetic energy per step
and not some average, then we need to zero the resource variable before
continuing the next sampling step.</p></div>
<div class="paragraph"><p>To this end, we need to create an assignment node <em>before</em> we instantiate the
<span class="monospaced">Session</span>. The assignment will place the constants scalar of <strong>0.</strong> in the
resource variable whenever it is evaluated. Again, we assume having a reference
to the variable in <span class="monospaced">kinetic_energy</span> already present.</p></div>
<div class="listingblock">
<div class="title">Evaluating the kinetic energy</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>zero_kinetic_energy_t <span style="color: #990000">=</span> kinetic_energy<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span><span style="color: #993399">0</span><span style="color: #990000">.)</span>

with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
  sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>zero_kinetic_energy_t<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>Now, that you can access the kinetic energy, do try to access other
quantities. Running averages are simply obtained by not setting to zero and
dividing by the number of steps on output.
Virials can be computed by looking at the scalar product of gradients and
variables, <span class="monospaced">tf.reduce_sum(tf.multiply(grad,var))</span>. Norms of gradients, noise,
momenta, and so on can be obtained in the same way. For each a diferent
resource fvariable is required.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="extensions.samplers.branching">3.1.4. Branching</h4>
<div class="paragraph"><p>As the last topic in this guide we come to an issue that has some peculiarities
about it.
We would like to add nodes to the graph such that either one node or another
is evaluated depending on a given condition.</p></div>
<div class="paragraph"><p>Let us look at a simple example in familiar python code.</p></div>
<div class="listingblock">
<div class="title">Branching in python</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #000080">import</span></span> numpy as np
np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">seed</span></span><span style="color: #990000">(</span><span style="color: #993399">426</span><span style="color: #990000">)</span>

steps <span style="color: #990000">=</span> <span style="color: #993399">100</span>
threshold <span style="color: #990000">=</span> <span style="color: #993399">0.5</span>
lower_sum <span style="color: #990000">=</span> <span style="color: #993399">0</span>
higher_sum <span style="color: #990000">=</span> <span style="color: #993399">0</span>

<span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="color: #993399">0</span><span style="color: #990000">,</span> <span style="color: #993399">100</span><span style="color: #990000">):</span>
  value <span style="color: #990000">=</span> np<span style="color: #990000">.</span>random<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random</span></span><span style="color: #990000">()</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">if</span></span> value <span style="color: #990000">&lt;</span> threshold<span style="color: #990000">:</span>
    lower_sum <span style="color: #990000">+=</span> value <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">else</span></span><span style="color: #990000">:</span>
    higher_sum <span style="color: #990000">+=</span> value <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>

<span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">([</span>lower_sum<span style="color: #990000">/</span>steps<span style="color: #990000">,</span> higher_sum<span style="color: #990000">/</span>steps<span style="color: #990000">])</span></tt></pre></div></div>
<div class="paragraph"><p>This is really a contrived example:</p></div>
<div class="colist arabic"><ol>
<li>
<p>
We throw a random die to uniformly produce a number in [0,1].
</p>
</li>
<li>
<p>
When the value is less than <strong>0.5</strong>, we add it to <span class="monospaced">lower_sum</span>.
</p>
</li>
<li>
<p>
If it is equal or larger, then we add it to <span class="monospaced">higher_sum</span>.
</p>
</li>
</ol></div>
<div class="paragraph"><p>Let us look at how to implement this in tensorflow. Naturally, we need to
convert all expressions to tensorflow statements. The <span class="monospaced">if</span> condition is
replaced by <span class="monospaced">tf.cond()</span> which takes three arguments: the condition statement,
a function for the true case, a function for the false case. Let us say that
again: You need to give function handlers as second and third argument, <em>not</em>
tensorflow nodes! However, you can wrap your node in a dummy function that
returns it. This is the first peculiarity but there is another. The first is not
too bad because tensorflow will throw an error informing you that it needs a
function in place of a node. The second one is more subtle to debug.</p></div>
<div class="paragraph"><p>Let us first take a look at <strong>how not to do it</strong>, i.e. let us deliberately
stumble over the second peculiarity.</p></div>
<div class="listingblock">
<div class="title">Naive branching implementation</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>steps<span style="color: #990000">=</span><span style="color: #993399">100</span>
threshold <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">constant</span></span><span style="color: #990000">(</span><span style="color: #993399">0.5</span><span style="color: #990000">,</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
lower_sum <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variable</span></span><span style="color: #990000">(</span><span style="color: #993399">0</span><span style="color: #990000">.,</span> trainable<span style="color: #990000">=</span>False<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
higher_sum <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Variable</span></span><span style="color: #990000">(</span><span style="color: #993399">0</span><span style="color: #990000">.,</span> trainable<span style="color: #990000">=</span>False<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>
random_t <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">random_uniform</span></span><span style="color: #990000">(</span>min<span style="color: #990000">=</span><span style="color: #993399">0</span><span style="color: #990000">.,</span> max<span style="color: #990000">=</span><span style="color: #993399">1</span><span style="color: #990000">.,</span> dtype<span style="color: #990000">=</span>tf<span style="color: #990000">.</span>float32<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span>

<span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_low</span></span><span style="color: #990000">():</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> lower_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>lower_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)</span>

<span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_high</span></span><span style="color: #990000">():</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;3&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> higher_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>higher_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)</span>

branching_t <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">cond</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">less</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">,</span> threshold<span style="color: #990000">),</span>
  accept_low<span style="color: #990000">,</span> accept_high<span style="color: #990000">)</span>  <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;4&gt;</b></span></span>

with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Session</span></span><span style="color: #990000">()</span> as sess<span style="color: #990000">:</span>
  <span style="font-weight: bold"><span style="color: #0000FF">for</span></span> i <span style="font-weight: bold"><span style="color: #0000FF">in</span></span> <span style="font-weight: bold"><span style="color: #000000">range</span></span><span style="color: #990000">(</span><span style="color: #993399">0</span><span style="color: #990000">,</span>steps<span style="color: #990000">):</span>
    sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">(</span>branching_t<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;5&gt;</b></span></span>
  <span style="font-weight: bold"><span style="color: #0000FF">print</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">run</span></span><span style="color: #990000">([</span>lower_sum<span style="color: #990000">/</span>steps<span style="color: #990000">,</span> higher_sum<span style="color: #990000">/</span>steps<span style="color: #990000">]))</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;6&gt;</b></span></span></tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
First, we create several variables in the same way as we instantiated python
variables before.
</p>
</li>
<li>
<p>
The first difference is that we have to create a source of random numbers.
However, this should be quite familiar as we had to do the same thing for the
sampler implementation above.
</p>
</li>
<li>
<p>
Next, circumventing pecularity number one we define two functions that sum
onto the <span class="monospaced">lower_sum</span> and <span class="monospaced">higher_sum</span> variables through assignment.
</p>
</li>
<li>
<p>
Then we come to the branching statement. Using <span class="monospaced">tf.less()</span> we make a
comparison node between the current random number in <span class="monospaced">random_t</span> and the
constant <span class="monospaced">threshold</span>. <span class="monospaced">tf.cond()</span> then should execute either <span class="monospaced">accept_low</span> or
<span class="monospaced">accept_high</span>.
</p>
</li>
<li>
<p>
We evaluate the branching node for 100 steps.
</p>
</li>
<li>
<p>
And finally we print the two resulting averages for either sum.
</p>
</li>
</ol></div>
<div class="paragraph"><p>And the result is: something close to <strong>0.5, 0.5</strong>?! We had expected this to
be <strong>0.25, 0.75</strong>.</p></div>
<div class="paragraph"><p>What has happened?
When tensorflow evaluates the <span class="monospaced">tf.cond()</span> statement it needs to look at <em>both</em>
the possible true and false branches in deciding which nodes they depend on.
Somehow this also seems to make it necessary to evaluate them. In other words,
in the naive way above both branches are executed and we obtain two equivalent
sums.</p></div>
<div class="paragraph"><p>The workaround is to hide away all <em>side effects</em>, i.e. changing the state,
of your branches inside a <span class="monospaced">tf.control_dependencies()</span> statement and only return
a dummy node, e.g., using <span class="monospaced">tf.identity()</span>.</p></div>
<div class="listingblock">
<div class="title">Changes for a working branching implementation</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">...</span>
<span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_low</span></span><span style="color: #990000">():</span>
  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">control_dependencies</span></span><span style="color: #990000">([</span>lower_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>lower_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)]):</span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">identity</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">)</span>

<span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_high</span></span><span style="color: #990000">():</span>
  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">control_dependencies</span></span><span style="color: #990000">([</span>higher_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>higher_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)]):</span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">identity</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">)</span>
<span style="color: #990000">...</span></tt></pre></div></div>
<div class="paragraph"><p>This now returns the expected output.</p></div>
<div class="paragraph"><p>In our corrections we have circumvented another peculiarity: Inside the control
dependency there <em>must not appear nodes</em>. Tensorflow will not throw an error
but these will not be triggered. Depenedencies must be always given as
statements. After all, this is why this workaround is working at all.</p></div>
<div class="paragraph"><p>To make this concrete, <em>the following does nothing</em>.</p></div>
<div class="listingblock">
<div class="title">Node should not be used in control dependencies when branching</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="color: #990000">...</span>
accept_low_t <span style="color: #990000">=</span> lower_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>lower_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)</span>
<span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_low</span></span><span style="color: #990000">():</span>
  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">control_dependencies</span></span><span style="color: #990000">([</span>accept_low_t<span style="color: #990000">]):</span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">identity</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">)</span>
<span style="color: #990000">...</span></tt></pre></div></div>
</div>
<div class="sect3">
<h4 id="extensions.samplers.other_flow_control">3.1.5. More flow control</h4>
<div class="paragraph"><p>Note that tensorflow basically has support for any kind of flow control. Using
the branching statement one could easily implement a loop. There is also a
<span class="monospaced">tf.while_loop</span> statement right away. See
<a href="https://www.tensorflow.org/api_guides/python/control_flow_ops#Control_Flow_Operations">Control Flow Operations</a>
for a complete list of all control flow operatipons that tensorflow supports.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Not every one of them is useful. In our case of the sampler it is
perfectly acceptable to have a hybrid python/tensorflow implementation as each
train step depends on the former and therefore no parallelization is possible
there.</td>
</tr></table>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="debugging">4. Debugging tensorflow code</h2>
<div class="sectionbody">
<div class="paragraph"><p>Here, we show tips and tricks on how to debug tensorflow code.</p></div>
<div class="paragraph"><p>Tensorflow code relies on the concept of a computational graph which is often
harder to grasp intuitively than the typical programming paradigm such as found
when writing python programs.</p></div>
<div class="paragraph"><p>Typical debugging makes use of print statements that show the control flow and
internal states of the program on the console for the user to follow.
Tensorflow has similar statements but they work in the particular manner of the
computational graph.</p></div>
<div class="sect2">
<h3 id="debugging.tips">4.1. Tips</h3>
<div class="paragraph"><p>The tips and guidelines revolve around the following concepts:</p></div>
<div class="ulist"><ul>
<li>
<p>
print statements,
</p>
</li>
<li>
<p>
debugger,
</p>
</li>
<li>
<p>
toy models and minimum working examples.
</p>
</li>
</ul></div>
<div class="sect3">
<h4 id="debugging.tips.print">4.1.1. Print statement</h4>
<div class="paragraph"><p>Our first advice is to make use of <span class="monospaced">tf.Print()</span> when something does not look
right. It serves a similar purpose as would a <span class="monospaced">print()</span> statement do in normal
python code: We want to inspect the value of a certain variable or we want to
see when the program reaches a certain point in its control flow.</p></div>
<div class="paragraph"><p>Because of the nature of the computational graph, we cannot simply add a
node using <span class="monospaced">tf.Print()</span> and be done. We have to make sure that the node is
actually executed in the evaluation of the graph. In other words, it needs to
be inserted in the proper place.</p></div>
<div class="paragraph"><p>To enable placement in arbitrary positions, <span class="monospaced">tf.Print()</span> works the same way as
<span class="monospaced">tf.identity()</span> does, i.e. it is a pass-thru node that simply passes the value
of its argument on, see Figure <a href="#debugging.tips.print.pass_thru">Pass-thru node</a>.
However, it has the side-effect of printing a <span class="monospaced">message</span> together with <span class="monospaced">data</span>.</p></div>
<div class="imageblock" id="debugging.tips.print.pass_thru" style="text-align:center;">
<div class="content">
<img src="/home/heber/workspace_Python/ThermodynamicAnalyticsToolkit/doc/userguide/pictures/pass-thru_node.png" alt="pictures/pass-thru_node.png" width="350">
</div>
<div class="title">Figure 2. Pass-thur node: <span class="monospaced">tf.Print()</span> simply passes thru input to output</div>
</div>
<div class="paragraph"><p>Imagine we wanted to inspect whether the random number tensor in our
<a href="#extensions.samplers.branching">Branching</a>
example before would not actually be evaluated two times. Or even three times
in the naive example: Once for evaluating <span class="monospaced">branching_t</span> and two more times when
adding onto either sum.</p></div>
<div class="paragraph"><p>Does this value coincide each time? Or has a new random number been triggered?
We can deduce the behavior from the result <strong>0.25, 0.75</strong>.
Nonetheless, let us do this properly.</p></div>
<div class="paragraph"><p>Therefore, let us modify the <span class="monospaced">branching_t</span> statement and
the function definitions as follows:</p></div>
<div class="listingblock">
<div class="title">Print debugging in the tensorflow lingo</div>
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt><span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_low</span></span><span style="color: #990000">():</span>
  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">control_dependencies</span></span><span style="color: #990000">([</span>lower_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>lower_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)]):</span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Print</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">,</span> <span style="color: #990000">[</span>random_t<span style="color: #990000">],</span> message<span style="color: #990000">=</span><span style="color: #FF0000">"random number low"</span><span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>

<span style="font-weight: bold"><span style="color: #0000FF">def</span></span> <span style="font-weight: bold"><span style="color: #000000">accept_high</span></span><span style="color: #990000">():</span>
  with tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">control_dependencies</span></span><span style="color: #990000">([</span>higher_sum<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">assign</span></span><span style="color: #990000">(</span>higher_sum <span style="color: #990000">+</span> random_t<span style="color: #990000">)]):</span>
  <span style="font-weight: bold"><span style="color: #0000FF">return</span></span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Print</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">,</span> <span style="color: #990000">[</span>random_t<span style="color: #990000">],</span> message<span style="color: #990000">=</span><span style="color: #FF0000">"random number high"</span><span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;1&gt;</b></span></span>

branching_t <span style="color: #990000">=</span> tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">cond</span></span><span style="color: #990000">(</span>tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">less</span></span><span style="color: #990000">(</span>
    tf<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">Print</span></span><span style="color: #990000">(</span>random_t<span style="color: #990000">,</span> <span style="color: #990000">[</span>random_t<span style="color: #990000">],</span> message<span style="color: #990000">=</span><span style="color: #FF0000">"random number"</span><span style="color: #990000">),</span> threshold<span style="color: #990000">),</span>
  accept_low<span style="color: #990000">,</span> accept_high<span style="color: #990000">)</span> <span style="font-style: italic"><span style="color: #9A1900"># <b>&lt;2&gt;</b></span></span></tt></pre></div></div>
<div class="colist arabic"><ol>
<li>
<p>
Here, we have simply replaced the identity by the print statement.
</p>
</li>
<li>
<p>
In the branching statement we do not use <span class="monospaced">random_t</span> directly but let it
pass thru the print statement first.
</p>
</li>
</ol></div>
<div class="paragraph"><p>This way we obtain two messages per loop iteration and we can easily
compare the values of the message with "random number" to the one from
"random number low" or "random number high".</p></div>
</div>
<div class="sect3">
<h4 id="debugging.tips.debugger">4.1.2. Using the debugger</h4>
<div class="paragraph"><p><span class="monospaced">tfdbg</span> is a specialized <a href="https://www.tensorflow.org/guide/debugger">debugger</a>
for inspecting tensorflow&#8217;s computational graphs.</p></div>
<div class="paragraph"><p>It is added when the <span class="monospaced">Session</span> object is modified in the following way:</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>sess <span style="color: #990000">=</span> tf_debug<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">LocalCLIDebugWrapperSession</span></span><span style="color: #990000">(</span>sess<span style="color: #990000">,</span> ui_type<span style="color: #990000">=</span>FLAGS<span style="color: #990000">.</span>ui_type<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>The debugger is more powerful when inspecting multiple values. Execute the
program through <span class="monospaced">run</span>. This will stop once per encountered <span class="monospaced">session.run()</span>
statement. <span class="monospaced">lt</span> lists all tensors. <span class="monospaced">pt</span> prints the value of a particular
tensor. We refer to above linked tutorial on how to use the debugger.</p></div>
<div class="paragraph"><p>Debugging is also possible through TensorBoard which is a webserver specialized
on displaying inputs, outputs and states of the tensorflow graph.</p></div>
<div class="paragraph"><p>It is added when the <span class="monospaced">Session</span> is modified like this.</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>sess <span style="color: #990000">=</span> tf_debug<span style="color: #990000">.</span><span style="font-weight: bold"><span style="color: #000000">TensorBoardDebugWrapperSession</span></span><span style="color: #990000">(</span> sess<span style="color: #990000">,</span>
    FLAGS<span style="color: #990000">.</span>tensorboard_debug_address<span style="color: #990000">)</span></tt></pre></div></div>
<div class="paragraph"><p>This debugger is essentially a graphical interface with many of the features
of the command-line (CLI) debugger. It requires a running tensorboard session.
Again, we refer the reader to the tutorial.</p></div>
</div>
<div class="sect3">
<h4 id="debugging.tips.toy_models">4.1.3. Understanding through toy models</h4>
<div class="paragraph"><p>One last way of debugging a program is in making a <em>minimum working example</em>.</p></div>
<div class="paragraph"><p>This could for example be a toy model such as simple harmonic oscillator
where analytical properties such as the
mean loss are known that we may compute and compare against.</p></div>
<div class="paragraph"><p>In general, when something is not working right, then try to make the code
that is not working as small as possible. Throw away any other code that has
nothing to do with the problem.</p></div>
<div class="paragraph"><p>As a start, you can use the code examples in this guide to build a light-weight
GLA2 sampler on a single-layer perceptron. By feeding a dataset consisting of
a single <strong>0.</strong> as feature and a single <strong>0.</strong> as its label and using the
<span class="monospaced">mean_squared</span> loss, you obtain a harmonic oscillator in the single bias
variable.</p></div>
<div class="paragraph"><p>But a toy model may also be much simpler, consisting only of two variables and
a statement combining the two. It may help understanding how a tensorflow
statement works, see <a href="#concepts.tensorflow.graph_construction">Constructing a graph</a>.</p></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="tati">5. Setup of TATi</h2>
<div class="sectionbody">
<div class="paragraph"><p>In this chapter we want to explain the program structure of TATi, namely its</p></div>
<div class="ulist"><ul>
<li>
<p>
directory and python module structure
</p>
</li>
<li>
<p>
build system
</p>
</li>
<li>
<p>
version control system (git)
</p>
</li>
</ul></div>
<div class="sect2">
<h3 id="tati.structure">5.1. Structure</h3>
<div class="paragraph"><p>Understanding the structure of TATi&#8217;s directories and python modules/files
is essential when contributing.</p></div>
<div class="sect3">
<h4 id="tati.structure.general">5.1.1. Directories</h4>
<div class="paragraph"><p>Let us explain each folder in turn:</p></div>
<div class="ulist"><ul>
<li>
<p>
doc
</p>
<div class="paragraph"><p>Contains documentation such as the userguide and also configuration files for
<a href="http://www.stack.nl/~dimitri/doxygen/">doxygen</a> to produce the reference documentation.</p></div>
</li>
<li>
<p>
examples
</p>
<div class="paragraph"><p>This folder is for IPython/Jupyter notebooks on tutorials or short examples on
using TATi.</p></div>
</li>
<li>
<p>
src
</p>
<div class="paragraph"><p>Contains the source tree of TATi, i.e. all python modules that are installed
are located in this folder.</p></div>
</li>
<li>
<p>
tests
</p>
<div class="paragraph"><p>Contains all tests, i.e. everything needed to execute <span class="monospaced">make check</span>.</p></div>
</li>
<li>
<p>
util
</p>
<div class="paragraph"><p>This folder contains helper python scripts that are not general enough to be
placed into the <span class="monospaced">src</span> folder but still serve a purpose close to TATi&#8217;s nature.</p></div>
</li>
</ul></div>
<div class="paragraph"><p>There are probably several other folders: <span class="monospaced">autom4te.cache</span>, <span class="monospaced">build-aux</span>, and
<span class="monospaced">m4</span>. These belong to the autoconf build system are explained in
<a href="#tati.buildsystem">[tati.buildsystem]</a>.</p></div>
<div class="paragraph"><p>In the following we look more closely at each folder. Each of them is connected
to a different toolchain as well.</p></div>
</div>
<div class="sect3">
<h4 id="tati.structure.documentation">5.1.2. Documentation</h4>
<div class="paragraph"><p>TATi&#8217;s <em>userguide and programmer&#8217;s guide</em> in <span class="monospaced">doc/userguide</span> consist of simple
text files that use a special "markdown" notation in the
<a href="http://www.methods.co.nz/asciidoc/">asciidoc</a> format.
See the userguide there for a full reference and plenty of examples.</p></div>
<div class="paragraph"><p>All <em>example scripts</em> are full python scripts that are tested in the folder
<span class="monospaced">tests/userguide</span>. They reside in subfolders <span class="monospaced">cmdline</span>, <span class="monospaced">python</span>, and
<span class="monospaced">simulation</span> for the command-line interface, for the general python interface
and for the <span class="monospaced">simulation</span> interface. This is to make sure that all examples
are always up-to-date and working.</p></div>
<div class="paragraph"><p>All <em>images</em> in the guides reside in the subfolder <span class="monospaced">pictures</span> and typically
consist of <strong>png</strong> files. If they have been created using <a href="www.texample.net/tikz/examples/">tikz</a>
or <a href="http://pgfplots.sourceforge.net">pgfplots</a>, then the source <span class="monospaced">.tex</span>
files have been placed in there as well.</p></div>
<div class="paragraph"><p>The program packages <span class="monospaced">asciidoc</span> and <span class="monospaced">dblatex</span> are used to convert the text
files first fto html and xml and the latter is finally converted one more time
to a pdf file. Instructions are found <span class="monospaced">doc/userguide/Makefile.am</span>.</p></div>
<div class="paragraph"><p>Moreover, TATi&#8217;s <em>reference documentation</em> is automatically created from the
source files located in folder <span class="monospaced">src</span> using <a href="http://www.stack.nl/~dimitri/doxygen/">doxygen</a>.
It pulls in additional files that explain the source code in general from
<span class="monospaced">src/documentation</span>. The configuration files for doxygen are located in
<span class="monospaced">doc/Doxyfile</span>.</p></div>
</div>
<div class="sect3">
<h4 id="tati.structure.examples">5.1.3. Examples</h4>
<div class="paragraph"><p>Examples as Jupyter notebooks reside in the <span class="monospaced">examples</span> folder. They are
currently still untested, i.e. there are not checked to still comply with
the current API of TATi.</p></div>
<div class="paragraph"><p>In principle it is possible to extract the pure python code from notebooks,
see this <a href="https://stackoverflow.com/a/30776383/1967646">stackoverflow question</a>.
However, a few issues are difficult to overcome:</p></div>
<div class="ulist"><ul>
<li>
<p>
comments in cells cannot be extracted without jupyter.
</p>
</li>
<li>
<p>
jupyter notebooks lack proper syspath in general.
</p>
</li>
<li>
<p>
plot commands (<span class="monospaced">plt.show()</span>, <span class="monospaced">ax.scatter</span>) are difficult to prevent in this
  non-interactive environment.
</p>
</li>
</ul></div>
<div class="paragraph"><p>To this end, they remain still untested until these can be resolved.</p></div>
</div>
<div class="sect3">
<h4 id="tati.structure.modules">5.1.4. Python modules</h4>
<div class="paragraph"><p>The folder <span class="monospaced">src</span> contains two folders: <span class="monospaced">TATi</span> and <span class="monospaced">documentation</span>.</p></div>
<div class="paragraph"><p>The latter simply contains additional <a href="http://www.stack.nl/~dimitri/doxygen/">doxygen</a>
files to explain general parts and concepts of the source code.</p></div>
<div class="paragraph"><p>The folder <span class="monospaced">TATi</span> consists of all python modules for the <span class="monospaced">TATi</span> module. The
structure in the source tree is exactly the same as it is installed, i.e.
a module <span class="monospaced">TATi.models.model</span> can be found in the folder <span class="monospaced">TATi/models/model.py</span>.</p></div>
<div class="paragraph"><p>The folder itself contains several subfolders which we briefly enumerate and
explain.</p></div>
<div class="ulist"><ul>
<li>
<p>
datasets
</p>
<div class="paragraph"><p>This contains generators for toy example datasets.</p></div>
</li>
<li>
<p>
diffusion_maps
</p>
<div class="paragraph"><p>Here, we have code for performing diffusion map analysis. Similar functionality
also exists in the package <span class="monospaced">pydiffmap</span>, see
 <a href="#quickstart.simulation.analysis">[quickstart.simulation.analysis]</a>. </p></div>
</li>
<li>
<p>
exploration
</p>
<div class="paragraph"><p>In exploration we code for obtained several sampling and training trajectories
and analysing them. To this end, there is a "queue" implementation there and
each part (e.g., sampling trajectory) is encoded as a "job". To allow for
parallel runs, there is also an extension where each job is run as an
independent "process" if it relies in tensorflow.</p></div>
</li>
<li>
<p>
models
</p>
<div class="paragraph"><p>Models contains the general python interface class <span class="monospaced">model.py</span> and helper
classes to construct the actual "model", i.e. the input pipeline in subfolder
<span class="monospaced">input</span> and the neural network.</p></div>
</li>
<li>
<p>
runtime
</p>
<div class="paragraph"><p>This folder contains a helper class to measure the runtime of events and is
used in the command-line tools.</p></div>
</li>
<li>
<p>
samplers
</p>
<div class="paragraph"><p>Here, all sampler implementations are gathered. They all derive from
<span class="monospaced">walkerensembleoptimizer.py</span> which contains functionality to have multiple
copies of the neural network, one for each walker.</p></div>
</li>
<li>
<p>
tools
</p>
<div class="paragraph"><p>The tools folder contains the python script that form the toolkit part of
TATi. They are small command-line programs that accept arguments using the
<span class="monospaced">argparse</span> package and each perform a very specific task, e.g., executing
obtaining a single sampling trajectory.</p></div>
</li>
</ul></div>
<div class="paragraph"><p>Moreover, the folder <span class="monospaced">TATi</span> itself contains a few special files. There is
<span class="monospaced">common.py</span> which is a general module where small helper functions are placed
that have not found a place anywhere else, i.e. in any proper class.</p></div>
<div class="paragraph"><p>There is <span class="monospaced">TrajectoryAnalyser.py</span> which contains functionality for analysing
trajectories.</p></div>
<div class="paragraph"><p>Last but not least, there is <span class="monospaced">version.py</span> which contains helper functions to
give the name version and build hash of TATi. Similarly, <span class="monospaced">TATi.__version__</span> is
contained in  <span class="monospaced"><em>init</em>.py</span> which is produced from requesting the current
version from the git repository using <span class="monospaced">git describe</span>.</p></div>
</div>
</div>
<div class="sect2">
<h3 id="tati.buildsystem">5.2. Build system</h3>
<div class="paragraph"><p>The build systems used the <a href="https://www.gnu.org/software/automake/manual/html_node/Autotools-Introduction.html"><strong>autootols</strong></a>
suite for dependency generation, testing and packaging.
It is the standard build system for all <a href="link:https://www.gnu.org/software">GNU software</a>
and has a very long history.</p></div>
<div class="paragraph"><p>It based on the scripting language <em>m4</em> that is a bit outdated from a modern
viewpoint.</p></div>
<div class="paragraph"><p>Note that there are other build systems such as <a href="https://cmake.org">cmake</a>.
However, the principal author of TATi at its creation felt that autotools has
the superior test system and TATi would be solidly based in the UNIX world
and less in Windows environments that are scarcely found in scientific
working environments. Cmake needs to make some compromises to fully support
the Windows platform, while autotools basically admits the full set of GNU
utilities which makes testing especially powerful.</p></div>
<div class="sect3">
<h4 id="tati.buildsystem.general">5.2.1. General concept</h4>
<div class="paragraph"><p>The build system has the following responsibilities:</p></div>
<div class="ulist"><ul>
<li>
<p>
installation into a target folder (<span class="monospaced">make install</span>)
</p>
</li>
<li>
<p>
packaging of releases as so-called tarballs (<span class="monospaced">make dist</span>)
</p>
</li>
<li>
<p>
testing of the software package (<span class="monospaced">make check</span>)
</p>
</li>
<li>
<p>
creation of guides and API documentation (<span class="monospaced">make doc</span>)
</p>
</li>
</ul></div>
<div class="paragraph"><p>As you notice, the central tool is (GNU) <span class="monospaced">make</span>. The central concept of make
are rules. Each rule revolves around a <em>target</em>, <em>dependencies</em> for the target
and a prescription how to use the dependencies to produce the target. All in
all this creates a dependency graph through which make automatically recreates
targets even when "distant" dependencies have changed.</p></div>
<div class="paragraph"><p>The autotools suite is then mainly responsible for creating a set of
+Makefile+s containing all necessary targets that tell make how to
build/test/deploy the package.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/note.png" alt="Note">
</td>
<td class="content">Automake focuses on creating packages from C/C++ code. It can be easily
extended to be useful for other languages, too. This has been done for Python.
However, C needs to be compiled which Python as an interpreted language does
not. Therefore, there are some steps that seem superfluous for Python.</td>
</tr></table>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.general.automake">Automake</h5>
<div class="paragraph"><p>Writing <span class="monospaced">Makefile</span> s typically can be quite cryptic. It becomes much simpler by
using <span class="monospaced">automake</span>. In automake one defines sets of specific variables in a file
<em>Makefile.am</em> from which it then creates a fully fletched <span class="monospaced">Makefile</span>.</p></div>
<div class="paragraph"><p>Let us alook at an example, namely <span class="monospaced">src/TATi/runtime/Makefile.am</span>.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>TATI_runtime_PYTHON = \
        runtime.py \
        __init__.py

TATI_runtimedir = $(pythondir)/TATi/runtime</pre>
</div></div>
<div class="paragraph"><p>You notice that we simply define two variables, <strong>TATI_runtime_PYTHON</strong> and
<strong>TATI_runtimedir</strong>. The first tells automake that it deals with files written
in the python language as the variables are suffixed with <strong>PYTHON</strong>. The
naming <strong>TATI_runtime</strong> is arbitrary but we want to reflect the module structure
as this module would be imported by <span class="monospaced">import TATI.runtime</span>.
The variable ending in <strong>..dir</strong> tells automake the installation folder.</p></div>
<div class="paragraph"><p>Automake automatically notes that the files listed in the first variabe also
need to go into the tarball when doing a release.</p></div>
<div class="paragraph"><p>All +Makefile.am+s are written by simply defining specific variables. Most of
the problems revolve around finding the correct name.</p></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.general.autoconf">Autoconf</h5>
<div class="paragraph"><p>Not all Unix systems are alike. And even like systems such as Linux-based
variants tend to differ among each other, e.g., in the way the handle packages.</p></div>
<div class="paragraph"><p>To this end, autoconf is a tool for setting up variables such as <strong>$(PYTHON)</strong>
to contain the path to the python interpreter. Autoconf takes files with
suffix <span class="monospaced">.in</span> and replaces these variables by their contents.</p></div>
<div class="paragraph"><p>Let us take a look at <span class="monospaced">src/TATi/tools/Makefile.am</span>.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>do_substitution = sed -e 's,[@]pythondir[@],$(pythondir),g' \
        -e 's,[@]PACKAGE[@],$(PACKAGE),g' \
        -e 's,[@]PYTHON[@],$(PYTHON),g' \
        -e 's,[@]VERSION[@],$(VERSION),g'</pre>
</div></div>
<div class="paragraph"><p>There you see that we define a <em>macro</em> (i.e. a variable being a a function)
that will use the command-line program <span class="monospaced">sed</span> to replace the placeholder
<strong>@PYTHON@</strong> by <strong>$(PYTHON)</strong> which in turn is replaced by the python interpreter
that autoconf found.</p></div>
<div class="paragraph"><p>Autoconf parses the file <span class="monospaced">configure.ac</span> in the package&#8217;s root folder and
creates a script called <span class="monospaced">configure</span>.</p></div>
<div class="paragraph"><p>This script contains a bunch of tests that define all these variables. Their
contents can be inspected in <span class="monospaced">build64/config.log</span> if <span class="monospaced">build64</span> is the folder
for the out-of-source build, see
<a href="userguide.html#introduction.installation.procedure">Installation procedure</a>.</p></div>
<div class="paragraph"><p>These tests are instantiated by small functions written in the <strong>m4</strong> language
that can typically be found in a folder installed alongside with autoconf or
on the web.</p></div>
<div class="paragraph"><p>Let us take a look at <span class="monospaced">configure.ac</span> and the part where we check for the
python interpreter.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>AM_PATH_PYTHON([2.5])</pre>
</div></div>
<div class="paragraph"><p>This calls the macro <strong>AM_PATH_PYTHON</strong> to look for a python interpreter in
various folders encoded in the macro that is at least version <strong>2.5</strong>.
This macro is contained in the autoconf installation.</p></div>
<div class="paragraph"><p>The macro can be overridden by the environment variable <strong>PYTHON</strong> which is
why</p></div>
<div class="listingblock">
<div class="content"><!-- Generator: GNU source-highlight
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>configure -C <span style="color: #009900">PYTHON</span><span style="color: #990000">=</span>/usr/bin/python<span style="color: #993399">3</span></tt></pre></div></div>
<div class="paragraph"><p>works. Note that these variables should go <em>after</em> configure as they are
cached (<strong>-C</strong>). Caching speeds up configure quite a lot.</p></div>
<div class="paragraph"><p>One macro is not contained, the one looking for <strong>doxygen</strong>, see
<span class="monospaced">m4/ac_doxygen.m4</span>. These need to be placed in the <strong>m4</strong> folder.</p></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.general.autotest">Autotest</h5>
<div class="paragraph"><p>When executing <span class="monospaced">make check</span>, then the autotest part of the autotools suite
is used. It is not a tool by itself but again a set of macros written in m4
which implement the test driver.</p></div>
<div class="paragraph"><p>We just would like to gather a few tricks here:</p></div>
<div class="ulist"><ul>
<li>
<p>
use <span class="monospaced">make -j4 check</span> to execute tests in parallel
</p>
</li>
<li>
<p>
you can execute tests individually: Look for the line above the tests
header. This is line is pre- and postfixed by lines looking like this
</p>
<div class="listingblock">
<div class="content monospaced">
<pre>## --------------------------------------------------------------- ##</pre>
</div></div>
<div class="paragraph"><p>The line contains a call to <span class="monospaced">testsuite</span> along with <strong>$nrjobs</strong>, followed
by defining the variable <strong>AUTOTEST_PATH</strong>. Copy that line and replace <strong>$nrjobs</strong>
by the following arguments: <span class="monospaced">-d -k sampler -j4</span>. This will execute all
tests matching the keyword <strong>sampler</strong> (see <strong>AT_KEYWORD</strong> in the test case
files) on four processes and leave all output (<strong>-d</strong>).</p></div>
</li>
</ul></div>
<div class="paragraph"><p>You can inspect the output of all failed tests by looking at the file
<span class="monospaced">testsuite.dir/01/testsuite.log</span> if <strong>01</strong> is the number of your test as
printed by the <span class="monospaced">make check</span> or <span class="monospaced">testsuite</span> run. In that folder also all
temporary files can be found.</p></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.general.others">Other tools</h5>
<div class="paragraph"><p>There are more tools like <span class="monospaced">aclocal</span> that need not be of interest here.</p></div>
</div>
</div>
<div class="sect3">
<h4 id="tati.buildsystem.adding">5.2.2. Adding new files</h4>
<div class="paragraph"><p>Clashes with the build system will occur when trying to add a new file.</p></div>
<div class="paragraph"><p>In this section we go through the few standard cases one by one.</p></div>
<div class="sect4">
<h5 id="tati.buildsystem.adding.source">Source file</h5>
<div class="paragraph"><p>You should know about the <a href="#tati.structure">Directory structure</a> by now.
Therefore you know where to put your file in the source tree.</p></div>
<div class="paragraph"><p>However, you also have to tell automake where it belongs.</p></div>
<div class="paragraph"><p>Look at the <span class="monospaced">Makefile.am</span> that should already by present in the folder. IN
there, there is a variable ending in <strong>.._PYTHON</strong>. Add it to the list and it
will get installed.</p></div>
<div class="paragraph"><p>If you have added a new folder with a file in it, then a few things need to
be done:</p></div>
<div class="ulist"><ul>
<li>
<p>
create the init file in the folder: <span class="monospaced">touch __init__.py</span>
</p>
</li>
<li>
<p>
create the <span class="monospaced">Makefile.am</span>, see <a href="#tati.buildsystem.general.automake">[tati.buildsystem.general.automake]</a> for an example
</p>
</li>
<li>
<p>
add the folder to the <strong>SUBDIRS</strong> of the <span class="monospaced">Makefile.am</span> in the folder below.
  Otherwise, make will not enter this directory.
</p>
</li>
</ul></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.adding.tool">Tool</h5>
<div class="paragraph"><p>Tools residing in the folder <strong>tools</strong> have the special ending <strong>.in</strong> because they
still need some variables replaced. Typically, it is <strong>@PYTHON@</strong> and
<strong>@pythondir@</strong> required when <span class="monospaced">tati</span> is installed at a non-default location.</p></div>
<div class="paragraph"><p>These files need to be added in <span class="monospaced">configure.ac</span>. Look close to the bottom for
statements <strong>AC_CONFIG_FILES</strong> that list the tools (without the *.in suffix).
The statement causes them to be marked as executable.
Furthermore, add the target statement in the <span class="monospaced">Makefile.am</span> in <span class="monospaced">src/TATi/tools</span>
in the same way as for the tools already present.</p></div>
<div class="paragraph"><p>In case you need to test the new tool, then you have to define a wrapper
script. They are located in <span class="monospaced">tests</span> and have the same name as the tool itself.
Simply look at one of those for an example and adapt the names.</p></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.adding.test">Test file</h5>
<div class="paragraph"><p>In <span class="monospaced">tests</span> there is the following distinction between the various tests:</p></div>
<div class="ulist"><ul>
<li>
<p>
integration
</p>
<div class="paragraph"><p>These tests combine several tools or parts of TATi. For example they might
sample a trajectory and feed it to the <span class="monospaced">LossFunctionSampler</span>. This ensures that
the tools understand the file formats among one another.</p></div>
</li>
<li>
<p>
regression
</p>
<div class="paragraph"><p>The regression tests either check for encountered bugs and make sure that
these do not occur again. Or they check specific capabilities of TATI, e.g.,
they make sure that every sampler behaves the same by checking its output
against a stored one. This is to ensure that changes in the code do not change
the behavior of the package.</p></div>
</li>
<li>
<p>
simulation
</p>
<div class="paragraph"><p>Here, we test specific parts of the <span class="monospaced">simulation</span> module with examples that will
not go in the userguide.</p></div>
</li>
<li>
<p>
userguide
</p>
<div class="paragraph"><p>All examples given in the userguide are tested in here one by one.</p></div>
</li>
<li>
<p>
tensorflow
</p>
<div class="paragraph"><p>Here, we check for certain tensorflow functionality or rather dysfunctiontality,
meaning that certain behavior of tensorflow is non-intuitive. These tests
whether the behavior changes with future tensorflow versions which would allow
to remove certain boilerplate code in overcoming the unexpected.</p></div>
</li>
</ul></div>
<div class="paragraph"><p>Each test cases resides in a unique file <span class="monospaced">testsuite-&#8230;.at* where the directory
structure also reflects in the name, e.g., +testsuite-userguide-simulation-complex.at</span>,</p></div>
<div class="paragraph"><p>These test cases are included in a tree-like structure up till the topmost
<span class="monospaced">testsuite.at</span> through <span class="monospaced">m4_include()</span> statements, see for example
<span class="monospaced">tests/userguide/testsuite.at</span>.</p></div>
<div class="paragraph"><p>A general introduction to this <em>autotest</em> part of autotools can be found
<a href="https://www.gnu.org/software/autoconf/manual/autoconf-2.68/html_node/Using-Autotest.html">here</a>.
Therefore, we do not want to cover its details here.</p></div>
<div class="paragraph"><p>Basically, the test cases consist of shell commands that wrapped into
<span class="monospaced">AT_CHECK()</span> statements that take an expected return code (0 - success, else -
failure) and whether to capture stdout and stderr (e.g., <strong>[stdout]</strong>) or to
ignre them (<strong>[ignore]</strong>).</p></div>
<div class="paragraph"><p>Typically, we use <span class="monospaced">grep</span> to make sure certain output was printed. We use
<span class="monospaced">diff</span> to compare files and the <a href="#tati.buildsystem.adding.tool">tool</a>
<span class="monospaced">NumericalDiff</span> for comparing files allowing for specific numerical variations.
Moreover, <span class="monospaced">sed</span> can be used to reformat output and <span class="monospaced">awk</span> to extract certain
elements based on delimiters (e.g., <strong>-F","</strong> for comma-based spitting).</p></div>
<div class="paragraph"><p>When adding a new test case, make sure to do the following:</p></div>
<div class="ulist"><ul>
<li>
<p>
write a new test case file
</p>
</li>
<li>
<p>
properly include the new <strong>testsuite-&#8230;at</strong> file in the upper level testsuite
  file
</p>
</li>
<li>
<p>
add the file to the <span class="monospaced">Makefile.am</span>, look for <strong>TESTSCRIPTS</strong>.
</p>
</li>
<li>
<p>
check that your tests is executed, see <a href="#tati.buildsystem.general.autotest">[tati.buildsystem.general.autotest]</a>
</p>
</li>
</ul></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.adding.documentation">Documentation</h5>
<div class="paragraph"><p>The documentation is contained <span class="monospaced">doc/userguide</span>. See all files ending <strong>.txt</strong> in
there.</p></div>
<div class="paragraph"><p>When adding new files make sure they are properly included in other asciidoc
files. If it is a new root file, add it to the <strong>all:</strong> target in +Makefile.am*.</p></div>
<div class="paragraph"><p>In any case, add it to the variable <strong>DEPENDENCIES</strong> in +Makefile.am*.</p></div>
<div class="paragraph"><p>If your new piece of documentation also has an example code piece, then put
the code into a distinct file and use the <span class="monospaced">include::..[]</span> statement of asciidoc.
Add the filename to the variable to the variable <strong>PYTHON_EXAMPLES</strong> in
+Makefile.am*.</p></div>
<div class="paragraph"><p>Then, you need to create a test case. Look at one of the already present
test cases in <span class="monospaced">tests/userguide</span>. These make use of a <span class="monospaced">run</span> script that invokes
python with the correct <strong>PYTHONPATH</strong> such that the module <span class="monospaced">tati</span> can be found
even when installed on non-default locations.</p></div>
</div>
<div class="sect4">
<h5 id="tati.buildsystem.adding.other">Other files</h5>
<div class="paragraph"><p>If the added file is none of the above but it should still go into the release
tarball, then add it to the <strong>EXTRA_DIST</strong> variable or add a statement such as
<span class="monospaced">EXTRA_DIST = foo.bar</span> to the folder&#8217;s <span class="monospaced">Makefile.am</span> if your file is called
<strong>foo.bar</strong>.</p></div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="tati.versioncontrol">5.3. Version control</h3>
<div class="paragraph"><p>TATi&#8217;s source code is maintained with <a href="https://git.org">git</a> as its
version control system. The "central" repository is located at
<a href="https://github.com/alan-turing-institute/ThermodynamicAnalyticsToolkit">GitHub</a>.</p></div>
<div class="paragraph"><p>Git records changes to the source code in <em>commits</em>. Each commit forms a node
in a graph. A commit is uniquely identified by its <em>hash</em> (a 7-digit hex code).</p></div>
<div class="paragraph"><p>Development should usually commence in a distinct <em>branch</em> that can be easily
created by <span class="monospaced">git checkout -b DevBranch</span> with any name in place for <strong>DevBranch</strong>.
The top of a branch is referred to as head and is the most current commit to
this branch.</p></div>
<div class="admonitionblock">
<table><tr>
<td class="icon">
<img src="./images/icons/important.png" alt="Important">
</td>
<td class="content">The branch <span class="monospaced">master</span> should only be modified by the package
maintainer and is basically only advanced onto the next release branch, see
below.</td>
</tr></table>
</div>
<div class="sect3">
<h4 id="tati.versioncontrol.release_policy">5.3.1. Release policy</h4>
<div class="paragraph"><p>Releases should be frequent and not too large.</p></div>
<div class="paragraph"><p>To this end, a branch <span class="monospaced">Candidate_vVERSION</span> is checked out from the current
<span class="monospaced">master</span> branch where <strong>VERSION</strong> is the upcoming version number, e.g., 0.9.
All development branches that are fit for release are merged using <span class="monospaced">git
merge --no-ff DevBranch</span> (if <strong>DevBranch</strong> is the branch&#8217;s name) such that
the original branch name is encoded in the merge commit message.</p></div>
<div class="paragraph"><p>Conflicts have to be resolved in the merge. Note that the
<a href="#tati.versioncontrol.test_policy">test policy</a> also and especially applies
to this candidate branch.</p></div>
<div class="paragraph"><p>At the very end, we need to test whether <span class="monospaced">make dist</span> produces a usable tarball,
i.e. whether unpacking, <span class="monospaced">configure</span>, <span class="monospaced">make &amp;&amp; make install &amp;&amp; make check</span> runs
flawlessly.</p></div>
<div class="paragraph"><p>Finally, the version number <strong>VERSION</strong> is placed into the <strong>AC_INIT</strong> statement in
<span class="monospaced">configure.ac</span> and this should always be done in a single commit not containing
any other changes. This last commit is tagged using <span class="monospaced">git tag -a vVERSION</span> and
the tag message should follow the example below.</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>Version VERSION contains the following branches:
HASH  Candidate_vVERSION
HASH1 BranchName1
HASH2 BranchName2</pre>
</div></div>
<div class="paragraph"><p>We put a list of all merged branches including the hash value of their
respective head. This way even when branches are deleted but have been
merged into <strong>master</strong>, then they can still be tracked.</p></div>
<div class="paragraph"><p>Subsequently, all development branches are deleted from the central repository
using <span class="monospaced">git branch -D DevBranch</span> (again assuming the branch&#8217;s name to be
<strong>DevBranch</strong>).</p></div>
</div>
<div class="sect3">
<h4 id="tati.versioncontrol.test_policy">5.3.2. Test policy</h4>
<div class="paragraph"><p>Using the version control system we retain a strict test policy in the
following way: <strong>For every commit <span class="monospaced">make &amp;&amp; make install &amp;&amp; make check</span> needs to
run flawlessly.</strong></p></div>
<div class="paragraph"><p>This ensures that the userguide compiles and TATi&#8217;s code complies with the
result expected in every test.</p></div>
<div class="paragraph"><p>If this should not be possible <strong>within development branches</strong>, then test cases
can be marked as <em>expected to fail (XFAIL)</em> <strong>temporarily</strong> by adding the
statement</p></div>
<div class="listingblock">
<div class="content monospaced">
<pre>AT_XFAIL_IF($BIN_TRUE)</pre>
</div></div>
<div class="paragraph"><p>just after the <strong>AT_KEYWORDS</strong> statement. By the very latest these issues must
be resolved till the next release.</p></div>
<div class="paragraph"><p>It is generally advised to modify commits such that adhere to the above test
policy. The commit history can be partially rewritten using <span class="monospaced">git rebase</span>.</p></div>
<div class="paragraph"><p>As an extreme measure, i.e. when too many commits would be affected, the tests
 can be deactivated <strong>temporarily</strong> by removing the respective test folder from
 the <strong>SUBDIRS</strong> statements in <span class="monospaced">tests/Makefile.am</span>.</p></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_tensorflow_flaws">6. Tensorflow Flaws</h2>
<div class="sectionbody">
<div class="paragraph"><p>It may be thought a little too much to dedicate a whole chapter to the
potential flaws in a framework that forms the basis of this software.
However, this particular framework has given me so much hardship and
failed in such unexpected ways that I have to make a list.</p></div>
<div class="paragraph"><p>Note that this list is up-to-date with respect to the tests employed in
the code and is currently focused at tensorflow version 1.4</p></div>
<div class="paragraph"><p>Most of issues simply made it hard to have reproducible runs which made
it diffult to maintain my testsuite. Moreover, most of them are made on
purpose for the sake of speed over deterministic behavior.</p></div>
<div class="ulist"><ul>
<li>
<p>
Non-deterministic <span class="monospaced">reduce_sum</span>
</p>
<div class="paragraph"><p>See the <a href="https://github.com/tensorflow/tensorflow/issues/3103">issue</a> at
Github. Non-deterministic is obviously faster than deterministic, so
that&#8217;s what they are going for. Sadly, no determinstic alternative for
calculating norms of 1-dimensional tensors or scalar products is
offered. This is very hurtful for reproducibility. The current
workaround is to set <span class="monospaced">inter_op_thread</span> to one, eliminating any use of
multiple cores.</p></div>
</li>
<li>
<p>
<span class="monospaced">tf.set_global_seed</span> not useful
</p>
<div class="paragraph"><p>This is working as intended: it sets the global seed in such a way that
all operations requiring randomness derive their seed in a deterministic
fashion from it. And this is valid as long as it is <em>exactly</em> the same
graph. If just a single node is added that does not even need to be
relevant for the operation, all seeds will change because the derivation
of seeds probably depends on some random order of nodes and not on the
name of the node or any other unique property.</p></div>
</li>
<li>
<p>
<span class="monospaced">tf.float64</span> is flawed
</p>
<div class="paragraph"><p>I encountered issues with precision when ascertaining theoretical
properties of the samplers. One remedy I though might solve the issue
was to switch from tf.float32 to tf.float64, i.e. from 1e-8 to 1e-18
internal floating point precision.</p></div>
<div class="paragraph"><p>What I found was that suddenly I could not recover the theoretical
properties any more. Even simple sampling (i.e. central limit theorem
and expected convergence rates of $\frac 1 {sqrt(n)}$ would not
bring up slopes of -0.5 as expected in log-log plots but also -0.4.</p></div>
<div class="paragraph"><p>I went to great length to check that all values are tf.float64. If I had
forgotten one, either the internal type checker would admonish it, or
the precision should just be the one I had with tf.float32. However, the
quality of the values had changed. My only guess is that there must be
some weird bug hidden deep in the C parts of the tensorflow code.</p></div>
</li>
<li>
<p>
Parsing from CSV file despite caching tenfold slower
</p>
<div class="paragraph"><p>With tensorflow 1.4 the Dataset module arrived (no longer being
"contrib") and I happily switched to this as means of constructing my
input pipeline. So far, I had just been looking at small test datasets
which fit in memory without issues. As the datasets were so small, I did
not expect any much slowing down of my code switching to parsing CSV
files and feeding them.</p></div>
<div class="paragraph"><p>However, both the old "queues" input pipeline and the new "Dataset"
pipeline (the latter even with caches) experienced a tenfold decrease of
runtime with respect to in/memory.</p></div>
<div class="paragraph"><p>I must admit though that the Dataset module at least made it possible to
let the user decide between in-memory storing and file parsing.</p></div>
</li>
<li>
<p>
<span class="monospaced">tf.if</span> conditional working in funny way
</p>
<div class="paragraph"><p>For the Hamiltonian Monte Carlo sampler an "if" block is required
inside the gradient evaluation that decides on whether the current short
trajectory run using Hamiltonian Dynamics is accepted or rejected. When
I tried make this work, I failed utterly, until I hit this
<a href="https://stackoverflow.com/a/37064128">answer</a> on stackoverflow. In a
comment even one from the tensorflow team admits that he finds this
behavior confusing.</p></div>
</li>
<li>
<p>
Shuffling (in queues) shuffles over all repeated datasets causing
duplicate items.
</p>
<div class="paragraph"><p>I gues this is for speed reasons as well but it is really a pain in the
arse. I guess the reason is a reshuffled dataset in every epoch, hence
the reshuffle over all repeated sets instead of reshuffling at the start
of the epoch. Probably it is simpler to implement with really large
datasets in multiple files.</p></div>
<div class="paragraph"><p>However, for small datasets suddenly your gradients change (not using
mini-batches) because one element is missing as another is in the set
twice. Again, bad for reproducibility.</p></div>
</li>
<li>
<p>
<span class="monospaced">tf.concat</span> dropping variable character
</p>
<div class="paragraph"><p>This is more of a nuisance but a painful one that has quite strong effects on
the efficiency of the <strong>simulations</strong> interface part: You cannot simply
concatenate four variable tensors and then set them all using a single
placeholder of the right dimension as the "variable" character is lost in
the concatenation. It can only be read, see <a href="https://stackoverflow.com/questions/47699569">stackoverflow</a>.
See also this related <a href="https://github.com/tensorflow/tensorflow/issues/1723">issue</a>.</p></div>
</li>
</ul></div>
</div>
</div>
<div class="sect1">
<h2 id="glossary">Glossary</h2>
<div class="sectionbody">
<div class="ulist"><ul>
<li>
<p>
<a id="BAOAB"></a> <strong>BAOAB</strong>
</p>
<div class="paragraph"><p>BAOAB is the short-form for the order of the exact solution steps in the
splitting of the Langevin Dynamics SDE: B means momentum update, A is the
position update, and O is the random noise update. It has 2nd order convergence
properties, showing even 4th order super-convergence in the context of
high friction, see <a href="#Leimkuhler2012">[Leimkuhler2012]</a>.</p></div>
</li>
<li>
<p>
<a id="CCAdL"></a> <strong>Covariance Controlled Adaptive Langevin</strong> (CCAdL)
</p>
<div class="paragraph"><p>This is an extension of <a href="#SGD">[SGD]</a> that uses a thermostat to dissipate the
extra noise through approximate gradients from the system.</p></div>
</li>
<li>
<p>
<a id="GD"></a> <strong>Gradient Descent</strong> (GD)
</p>
<div class="paragraph"><p>An iterative, first-order optimization that use the negative gradient
times a step width to converge towards the minimum.</p></div>
</li>
<li>
<p>
<a id="GLA"></a> <strong>Geometric Langevin Algorithm</strong> (GLA)
</p>
<div class="paragraph"><p>This family of samplers results from a first-order splitting between the
Hamiltonian and the Ornstein-Uhlenbeck parts. It provides up to
second-order accuracy. In the package we have implemented both the 1st
and 2nd order variant. GLA2nd is among the most accurate samplers,
especially when it comes to accuracy of momenta. It is surpassed by
BAOAB, particularly for positions.</p></div>
</li>
<li>
<p>
<a id="HMC"></a> <strong>Hamiltonian Monte Carlo</strong> (HMC)
</p>
<div class="paragraph"><p>Instead of Langevin Dynamics this sampler relies on Hamiltonian
Dynamics. After a specific number of trajectory steps an acceptance
criterion is evaluated. Afterwards momenta are drawn randomly. Hence,
here noise comes into play at distinct intervals while for the other
samplers noise enters gradually in every step.</p></div>
</li>
<li>
<p>
<a id="SGD"></a> <strong>Stochastic Gradient Descent</strong> (SGD)
</p>
<div class="paragraph"><p>A variant of <a href="#GD">[GD]</a> where not the whole dataset is used for the gradient
computation but only a smaller part. This lightens the computational
complexity and adds some noise to the iteration as gradients are only
approximate. However, given redundancy in the dataset this noise is
often welcome and helps in overcoming barriers in the non-convex
minimization problem.</p></div>
<div class="paragraph"><p>See also <a href="#GD">[GD]</a>.</p></div>
</li>
<li>
<p>
<a id="SGLD"></a> <strong>Stochastic Gradient Langevin Dynamics</strong> (SGLD)
</p>
<div class="paragraph"><p>A variant of SGD where the approximate gradients are not only source of
noise but an additional noise term is added whose magnitude controls the
noise from the gradients.</p></div>
<div class="paragraph"><p>See also <a href="#SGD">[SGD]</a>.</p></div>
</li>
</ul></div>
</div>
</div>
<div class="sect1">
<h2 id="bibliography">Literature</h2>
<div class="sectionbody">
<div class="ulist"><ul>
<li>
<p>
<a id="Coifman2006"></a> Coifman, R. R., &amp; Lafon, S. (2006).
Diffusion maps.
Applied and Computational Harmonic Analysis, 21(1), 530.
<a href="https://doi.org/10.1016/j.acha.2006.04.006">https://doi.org/10.1016/j.acha.2006.04.006</a>
</p>
</li>
<li>
<p>
<a id="Duane1987"></a> Duane, S., Kennedy, A. D., Pendleton, B. J., &amp; Roweth, D. (1987).
Hybrid Monte Carlo.
Physics Letters B, 195(2), 216222.
<a href="http://doi.org/10.1016/0370-2693(87)91197-X">http://doi.org/10.1016/0370-2693(87)91197-X</a>
</p>
</li>
<li>
<p>
<a id="Leimkuhler2012"></a> Leimkuhler, B., &amp; Matthews, C. (2012).
Rational Construction of Stochastic Numerical Methods for Molecular Sampling.
Applied Mathematics Research EXpress, 2013(1), 3456.
<a href="http://doi.org/10.1093/amrx/abs010">http://doi.org/10.1093/amrx/abs010</a>
</p>
</li>
<li>
<p>
<a id="Leimkuhler2015"></a> Leimkuhler, B., Matthews, C., &amp; Stoltz, G. (2015).
The computation of averages from equilibrium and nonequilibrium Langevin molecular dynamics.
IMA Journal of Numerical Analysis, 155.
<a href="http://doi.org/10.1093/imanum/dru056">http://doi.org/10.1093/imanum/dru056</a>
</p>
</li>
<li>
<p>
<a id="Matthews2018"></a> Matthews, C., Weare, J., &amp; Leimkuhler, B. (2018).
Ensemble preconditioning for Markov chain Monte Carlo simulation.
Statistics and Computing, 28(2), 277290.
<a href="http://doi.org/10.1007/s11222-017-9730-1">http://doi.org/10.1007/s11222-017-9730-1</a>
</p>
</li>
<li>
<p>
<a id="Neal2011"></a> Neal, R. M. (2011).
MCMC Using Hamiltonian Dynamics.
In Handbook of Markov Chain Monte Carlo (pp. 113162).
</p>
</li>
<li>
<p>
<a id="Shang2015"></a> Shang, X., Zhu, Z., Leimkuhler, B., &amp; Storkey, A. J. (2015).
Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling.
In C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett (Ed.),
Advances in Neural Information Processing Systems 28 (pp. 3745).
Curran Associates, Inc.
<a href="http://doi.org/10.1515/jip-2012-0071">http://doi.org/10.1515/jip-2012-0071</a>
</p>
</li>
<li>
<p>
<a id="Trstanova2016"></a> Trstanova, Z. (2016).
Mathematical and Algorithmic Analysis of Modified Langevin Dynamics.
</p>
</li>
<li>
<p>
<a id="Welling2011"></a> Welling, M., &amp; Teh, Y.-W. (2011).
Bayesian Learning via Stochastic Gradient Langevin Dynamics.
Proceedings of the 28th International Conference on Machine Learning, 681688.
<a href="http://doi.org/10.1515/jip-2012-0071">http://doi.org/10.1515/jip-2012-0071</a>
</p>
</li>
</ul></div>
</div>
</div>
</div>
<div id="footnotes"><hr></div>
<div id="footer">
<div id="footer-text">
Version v0.9.1-0-g994aa50<br>
Last updated
 2018-07-27 21:38:51 BST
</div>
</div>
</body>
</html>
