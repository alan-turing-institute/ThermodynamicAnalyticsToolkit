[[concepts.tensorflow]]
Tensorflow
~~~~~~~~~~

Historically, there were different approaches in setting up and using this
computational graph.
link:https://deeplearning.net/software/theano[theano]
required to actually compile code that would form the graph.
In link:https://www.tensorflow.org/[tensorflow] the graph needs to be static
throughout the program: first the graph is constructed, then the a session
object instantiated and nodes evaluated. This session object contains the
internal state of the graph and every of its nodes. Note that while the graph is
static, the session is not, i.e. the contents of  variables, may naturally
change. Only adding more nodes has a bad effect on performance.
link:https://pytorch.org[Pytorch] on the other hand strictly believes in a
dynamical graph, i.e. there is no session object containing temporary values
but the internal state is directly encoded with each node.

NOTE: From Tensorflow version 1.6 so-called "eager execution" was introduced
to allow for the same dynamical graph use as with PyTorch. However, at the time
of writing (tf1.9) the static graph is generally faster and generally adapts
better to different hardwares and GPU setups, see tensorflow's
link:https://www.tensorflow.org/community/roadmap[roadmap] on eager execution.
In general, static graphs will always execute faster than dynamic graphs.

[[concepts.tensorflow.graph_construction]]
Constructing a graph
^^^^^^^^^^^^^^^^^^^^

Let us directly see at how the above graph is constructed using tensorflow.

[source,python]
-----
import tensorflow as tf

a = tf.Constant(2.)
b = tf.Constant(3.)
sum = a + b   # alternatively: tf.add(a,b)
-----

We imported the tensorflow module and then created two nodes containing constant
values. Afterwards, we construct a sum node that depends on the two constant
nodes.

NOTE: The above does not perform any actual computation! All we do is construct
objects in memory.

In order to actually evaluate the sum in this case, we need to create a
`Session` object.

[source,python]
-----
sess = tf.Session()
print(sess.run(sum))
-----

This will print *5.0* as the result of the operation. The `Session` object
contains all the temporary memory required for containing information in
nodes.

Tensorflow internally has a default graph to which all create nodes are
associated and the `Session` takes hold of it. The graph can be resetted by
calling `tf.reset_default_graph()`.

Tensorflow has a whole assortment of the usual functions. Let us enumerate a few
of them: `tf.multiply`, `tf.exp`, `tf.sin`, .... For standard algebraic
operations the operators have been overloaded such that the node references
such as `a` and `b` in our example can be combined to a sum node by `a + b` or
similarly `a * b` for their product. Moreover, `2. * a - b` will automatically
add constant nodes containing here the additional value *2.*.

[[concepts.tensorflow.variables]]
Variables
^^^^^^^^^

Constants are given at the graphs construction at may not change. However,
there also variables whose contents may change. These are constructed by
giving an initial_value, a (derived) type and a name.

The shape is most important and it is derived from the _initial value_.

[source,python]
-----
a = tf.zeros((784))
b = tf.random_uniform((784,10), minval=-0.5, maxval=0.5)
v = tf.Variable(a, name="zero_vector")
W = tf.Variable(b, name="random_matrix")
-----

As you notice immediately, tensorflow has functions in likeness very similar
to numpy. First, `a` is a vector with 784 zero components. Then, a random
(constant)  matrix `b` is constructed. Finally, both are used as initial values
for variables. Of course, even higher  order tensor can be constructed.

NOTE: As these tensors "flow" through the computational graph, this gave rise to
the name "tensorflow".

A _type_ can be `tf.int`, `tf.float32`, `tf.float64`, and so on.

The _name_ identifies the node - in general, each node has a name and this eases
debugging and allows for readable errors messages.

CAUTION: In contrast to constants and the up-coming placeholders, variables
_used_ in evaluation need to be initialized once at the beginning of the
session. Use `session.run(tf.global_variables_initializer())`.

There are actually three different types of variables: `tf.constant`,
 `tf.Variable`, and `tf.placeholder`. The last of which we come to now.

The distinction between variables and constants is that variables allow
assignment and the latter do not. Placeholders, however, are more complicated.
They represent a value of predefined shape that is supplied by the user lateron.
In other words, while constants have to be given at "compile-time" (when
writing the python program), placeholder values are supplied at "run-time"
(when the program is executed).

[[concepts.tensorflow.feeding_values]]
Feeding values
^^^^^^^^^^^^^^

Placeholders allow for far more flexibility. For example, one could extend the
above example to a small program that would sum two arbitrary values given by
the user in the following way.

[source,python]
-----
import tensorflow as tf
import sys

a = tf.placeholder(shape=(), dtype=tf.float32)
b = tf.placeholder(shape=(), dtype=tf.float32)
sum = a + b
sess = tf.Session()
print(sess.run(sum, feed_dict={a: float(sys.argv[1]), b: float(sys.argv[2])}))
-----

Here, we use `sys.argv[..]` to read the first and second command-line
argument if you would call this script in a file `sum.py` as `python3 sum.py 2. 3.`.
For simplicity of the example we do not catch any errors such as missing
arguments.

Of course, this is a silly example! However, it serves a point. Notice the
`feed_dict` argument to the `Session.run()` statement? This is the way _all
placeholder values_ are "fed" into the computational graph from the outside.

This `feed_dict` is simply a python `dict` with keys and values. The keys are
are simply the node references themselves and the values are whatever the user
decides to feed in.

NOTE: Not all values for each placeholders need to be fed all the time, namely
only those the evaluated nodes depend on.

[[concepts.tensorflow.summary]]
Summary
^^^^^^^

This has been a very brief introduction to tensorflow. In case you need more
information, then head over to the tensorflow link:https://www.tensorflow.org/[website]
where there are plenty of well-written tutorials on light-weight examples such
as seen above. Moreover, there you find the Application Programmer's Interface
(API) documentation explaining each and every function.
