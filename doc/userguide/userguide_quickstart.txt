[[quickstart]]
Quickstart
----------

Before we come to actually using TATi, we explain what possible approaches
there are to sampling a high-dimensional function such as the loss manifold of
neural networks. To this end, we talk about grid-based sampling that however
suffers from the Curse of Dimensionality. Moreover, we will discuss Monte Carlo
and especially Markov Chain Monte Carlo methods. In the latter category we have
what we will call dynamics-based sampling. This approach does not suffer in
principle from the Curse of Dimensionality but moreover may have additional
savings by looking only at areas of the manifold that have small loss.
At the end, want to set the stage with a little example: We will look  at a
very simple classification task and see how it is solved using neural networks.

include::userguide_quickstart_sampling.txt[]

include::userguide_quickstart_simulation.txt[]

include::userguide_quickstart_cmdline.txt[]

[[quickstart.parallelization]]
A note on parallelization
~~~~~~~~~~~~~~~~~~~~~~~~~

Internally, Tensorflow uses a computational graph to represent all
operations. Nodes in the graph represent computations and their results
and edges represent dependencies between these values, i.e. some may act
as input to operations resulting in certain output.

Because of this internal representation Tensorflow has two kind of
parallelisms:

* inter ops
* intra ops

Each is connected to its its own thread pool. Both the command-line and
the Python interface let you pick the number of threads per pool. If 0
is stated (default), then the number of threads is picked automatically.

In general, 'inter_ops_threads' refers to multiple cores performing
matrix multiplication or reduction operations together.
'intra_ops_threads' seems to be connected to executing multiple nodes
in parallel that are independent of each other but this is guessing at
the moment.

[WARNING]
====
When setting 'inter_ops_threads' +unequal+ to 1, then subsequent runs
may produce different results, i.e. results are no longer strictly
reproducible. According to Tensorflow this is because reduction
operations such as `reduce_sum()` run non-deterministically on multiple
cores for sake of speed.
====

[[quickstart.conclusion]]
Conclusion
~~~~~~~~~~

This has been the quickstart introduction.

In the following reference section you may find the following pieces
interesting after having gone through this quickstart tutorial.

- <<reference.examples.harmonic_oscillator>> for a light-weight example
  probability distribution function whose properties are well understood.
- <<reference.implementing_sampler>> explaining how to implement your own
  sampler using the `Simulation` module as rapid-prototyping framework.
- <<reference.simulation>> giving detailed examples on each function in
  `Simulations`'s interface'.
