[[reference]]
The reference
-------------

[[reference.concepts]]
General concepts
~~~~~~~~~~~~~~~~

Here, we give definitions on a number of concepts that occur throughout this
userguide.

- Dataset
+
The dataset latexmath:[D = \{X,Y\}] contains a fixed number of datums of input
tuples or features latexmath:[X=\{x_d\}] and output tuples or labels
latexmath:[Y=\{y_d\}]. Basically, they are samples taken from
the unknown function which we wish to approximate using the neural
network. If the output tuples are binary in each component, the
approximation problem is called a _classification_ problem. Otherwise,
it is a _regression_ problem.

- Neural network
+
The neural network is a black-box representing a certain set of general
functions that are efficient in solving classification problems (among others).
They are parametrized explicitly using weights and biases
latexmath:[w=\{w_1, \ldots, w_M\}] and implicitly through the topology of the
network (connections of nodes residing in layers) and the activation functions
used. The network's output latexmath:[F_w(x_i)] depends explicitly on the
dataset's current datum (fed into the network) and implicitly on the parameters.

- Loss
+
The default is 'mean_squared'.
+
The loss function latexmath:[L_D(w)] determines for a given (labeled) dataset
what set of neural network's parameters are best. Different losses result in
different set of parameters. It is a high-dimensional manifold that we want to
learn and capture using the neural network. It implicitly depends on the given
dataset and explicitly on the parameters of the neural network, namely
weights and biases, latexmath:[w=\{w_1, \ldots, w_M\}].
+
Most important to understand about the loss is that it is a _non-convex_
function and therefore in general does not just have a single minimum.
This makes the task of finding a good set of parameters that (globally)
minimize the loss difficult as one would have to find each and every
minima in this high-dimensional manifold and check whether it is
actually the global one.

- Momenta and kinetic energy
+
Momenta latexmath:[p=\{p_1, \ldots, p_M\}] is a concept taken over from physics
where the parameters are considered as particles each in a one-dimensional
space where the loss is a potential function whose ( negative) gradient acts as
a force onto the particle driving them down-hill (towards the local minimum).
This force is integrated in a classical Newton's mechanic style, i.e.
Newton's equation of motion is discretized with small time steps
(similar to the learning rate in Gradient Descent). This gives first
rise to/velocity and second to momenta, i.e. second order ordinary
differential equation (ODE) split up into a system of two
one-dimensional ODEs. There are numerous stable time integrators, i.e.
velocity Verlet/Leapfrog, that are employed to propagate both particle
position (i.e. the parameter value) and its momentum through time. Note
that momentum and velocity are actually equivalent as usually the mass
is set to unity.
+
The kinetic energy latexmath:[0.5 \sum^M_i p^T_i p_i] is computed as sum over
kinetic energies of each parameter.

- Virial
+
Virial latexmath:[0.5 \sum^M_i w^T_i \nabla_{w_i} L_D(w)] is defined as one
half of the sum over the scalar product of gradients with parameters. Given
that the loss function is unbounded from above and raises more quickly than a
linear function, the asymptotic value of the virial is related to the average
kinetic energy through the virial theorem, see
https://en.wikipedia.org/wiki/Virial_theorem.

- Averages
+
Averages are meaningful when looking at the probability distribution function
instead of the loss directly, see Section <<quickstart.sampling.sequences>>.
There, we compute the integral latexmath:[\int A(w,p) \mu(w,p)] for in our case
the (canonical) Gibbs measure latexmath:[\mu(w,p) = \exp(-\beta L_D(w))] that
turns the loss into probability distribution function. Here, latexmath:[A(w,p)]
is an observable, i.e. a function depending on positions and momenta. Examples
of such observables are the kinetic energy or the virial, or maybe the
parameters themselves.

include::userguide_reference_examples.txt[]

include::userguide_reference_samplers.txt[]

include::userguide_reference_implementing_sampler.txt[]

include::userguide_reference_simulation.txt[]

[[reference.parallelization]]
A note on parallelization
~~~~~~~~~~~~~~~~~~~~~~~~~

Internally, Tensorflow uses a computational graph to represent all
operations. Nodes in the graph represent computations and their results
and edges represent dependencies between these values, i.e. some may act
as input to operations resulting in certain output.

Because of this internal representation Tensorflow has two kind of
parallelisms:

* inter ops
* intra ops

Each is connected to its its own thread pool. Both the command-line and
the Python interface let you pick the number of threads per pool. If 0
is stated (default), then the number of threads is picked automatically.

In general, 'inter_ops_threads' refers to multiple cores performing
matrix multiplication or reduction operations together.
'intra_ops_threads' seems to be connected to executing multiple nodes
in parallel that are independent of each other but this is guessing at
the moment.

[WARNING]
====
When setting 'inter_ops_threads' +unequal+ to 1, then subsequent runs
may produce different results, i.e. results are no longer strictly
reproducible. According to Tensorflow this is because reduction
operations such as `reduce_sum()` run non-deterministically on multiple
cores for sake of speed.
====

[[reference.reproducibility]]
A note on reproducibility
~~~~~~~~~~~~~~~~~~~~~~~~~

In many of the examples in the quickstart tutorials we have set a 'seed' value
to enforce reproducible runs.

We have gone through great lengths to make sure that runs using the same set
of options yield the same output on every evocation.

Tensorflow is not fully reproducible per se. Its internal random number seeds
change when the computational graph changes. Its reduction operations are
non-deterministic. The latter can be overcome by setting 'inter_ops_threads' to
_1_, which take away some of the parallelization for the sake of
reproducibility. The former is taken care of by TATi itself. We make sure to
set the random number seeds deterministically to ensure that values are
unchanged even if the graph is slightly changed.

If we find that this should not be the case, please file an issue, see
<<introduction.feedback>>.

[[reference.performance]]
Notes on Performance
~~~~~~~~~~~~~~~~~~~~

Performance is everything in the world of neural network training. Codes and
machines are measured by how fast they perform in images/second when training
AlexNet or other networks on the ImageNet dataset, see link:https://www.tensorflow.org/performance/benchmarks[Tensorflow Benchmarks].

We worked hard to ensure that whatever Tensorflow offers in performance is also
seen when using TATi. In order to guide the user in what to expect and what to
do when these expectations are not met, we invite to go through this section.

In general, performance hinges *critically* on the input pipeline. In other
words, it depends very much on how fast a specific machine setup can feed the
dataset into the input layer of the neural network.

NOTE: In our examples both datasets and networks are very small. This causes
the sequential parts of tensorflow to overwhelm any kind of parallel execution.

Typically, these datasets are stored as a set of files residing on disk. Note
that reading from disk is very slow compared to reading from memory. Hence, the
first step is to read the dataset from disk and this will completely dominate
the computational load at the beginning.

If the dataset is small enough to completely fit in memory, TATi will uses
Tensorflow's _caching_ to speed up the operations. This will become noticeable
after the first epoch, i.e. when all batches of the dataset have been processed
exactly once. Caching delivers at least a tenfold increase in learning speed,
depending on wer hard drive setup.

[NOTE]
.In memory pipeline
======
If the dataset fits in memory, it is advised to use the `InMemoryPipeline`
by setting the appropriate options in `tati` instantiation, see
<<quickstart.simulation>>. This is especially true for small batch sizes and
CPU-only hardware. On GPU-assisted systems the in-memory pipeline should not
be used.

[source, python]
----------------
nn = tati(
	# ...
	in_memory_pipeline = True,
	# ...
)
----------------

When using the command-line interface, add the respective option, see <<quickstart.cmdline>>.

[source, bash]
----------------
...
  --in_memory_pipeline 1 \
...
----------------

======

Furthermore, TATi uses Tensorflow's prefetching to interleave feeding and
training operations. This will take effect roughly after the second epoch.
Prefetching will show an increase by another factor of 2.

A typical runtime profile is given in Figure <<references.performance.runtime_comparison_cpu>>
where we show the time spent for every 10 steps over the whole history. This is
done by simply plotting the 'time_per_nth_step' column from the run file against
the 'step' column.
There, we have used the <<BAOAB>> sampler. Initially, there is a large peak
caused by the necessary parsing of the dataset from disk. This is followed by a
period where the caching is effective and runtime per nth step has dropped
dramatically. From this time on, Tensorflow will be able to make use of parallel
threads for training. Then, we see another drop when prefetching kicks in.

[[references.performance.runtime_comparison_cpu]]
.Runtime comparison, CPU: Core i7, network with a single hidden layer and various numbers of nodes on a random MNIST dataset
image::pictures/time_per_nth_step_hidden_dimension-hash_912b074-dimension_5000-batch_size_100-semilogy-2018-06-27.png[alt="runtime comparison",{basebackend@docbook:scaledwidth="60%":width=600}]

Note that Tensorflow has been designed to use GPU cards such as offered by
NVIDIA (and also Google's own domain-specific chip called Tensor Proccessing
Unit). If such a GPU card is employed, the actual linear algebra operations
necessary for the gradient calculation and weight and bias updates during
training will become negligible except for very large networks (1e6 dof and
beyond).

In <<references.performance.runtime_comparison_gpu>> we give the same runtime
profile as before. In contrast to before, the simulation is now done on a
system with 2 NVIDIA V100 cards. Comparing this to figure <<references.performance.runtime_comparison_cpu>>
we notice that now all curves associated to different number of nodes in the
hidden layer (*hidden_dimension*) basically lie on top of each other. In the
runtime profile on CPUs alone there is a clear trend for networks with more
degrees of freedom to significantly require more time per training step. We
conclude that with these networks (784 input nodes, 10 output nodes,
*hidden_dimension* hidden nodes, i.e. ~1e6 dof) the V100s do not see full load,
yet.

[[references.performance.runtime_comparison_gpu]]
.Runtime comparison, GPU: 2x V100 cards, network with a single hidden layer and various numbers of nodes on a random MNIST dataset
image::pictures/time_per_nth_step_hidden_dimension-dimension_16000-batch_size_1000-2018-06-28.png[alt="runtime comparison",{basebackend@docbook:scaledwidth="60%":width=600}]

[[reference.performance.hints]]
General advice
^^^^^^^^^^^^^^

We conclude this section with some general advice on resolving performance issues:

- trajectory files become very big with large networks. Do not write the
  trajectory if you do not really need it. If you do need it, then write only
	every tenth or better every hundredth step (option 'every_nth').
- summaries (option 'summaries_path') are very costly as well. Write them only
  for debugging purposes.
- if you can afford the stochastic noise, use a small batch size (option
  'batch_size').
- use hessians (option 'do_hessians') only for really small networks.
- covariance computations are latexmath:[{\cal O}(M^2)] in the number of
  parameters *M*. Rather use diffusion_map analysis which is squared only in the
	number of trajectory steps.
- measure the time needed for a few sampling/training steps, see the
  *time_per_nth_step* in the run info dataframes/files. A very good value
  is 300 steps per second. If you find you are much worse than this (on otherwise
	able hardware), then start tweaking parameters.
- Parallel load is not everything. The dataset pipeline will produce much more
  parallel load but is not necessarily faster than the in-memory pipeline. The
	essential measure is **steps/second**.

[[reference.miscellaneous]]
Miscellaneous
~~~~~~~~~~~~~

[[reference.miscellaneous.progress_bar]]
Displaying a progress bar
^^^^^^^^^^^^^^^^^^^^^^^^^

For longer simulation runs it is desirable to obtain an estimate after a
few steps of the time required for the entire run.

This is possible using the `progress` option. Specified to 1 or True it
will produce a progress bar showing the total number of steps, the
iterations per second, the elapsed time since start and the estimated
time till finish.

This features requires the link:https://github.com/tqdm/tqdm[tqdm] package.

[NOTE]
====
On the debug verbosity level per output step also an estimate of the
remaining run time is given.
====

[[reference.miscellaneous.summaries]]
Tensorflow summaries
^^^^^^^^^^^^^^^^^^^^

Tensorflow delivers a powerful instrument for inspecting the inner
workings of its computational graph: TensorBoard.

This tool allows also to inspect values such as the activation
histogram, the loss and accuracy and many other parameters and values
internal to TATi.

Supplying a path +/foo/bar+ present in the file system using the
`summaries_path` variable, summaries are automatically written to the
path and can be inspected with the following call to tensorboard.

[source,bash]
---------------
tensorboard --logdir /foo/bar
---------------

The tensorboard essentially comprises a web server for rendering the
nodes of the graph and figures of the inspected values inside a web page.
On execution it provides a URL that needs to be entered in any
web browser to access the web page.


[NOTE]
====
The accumulation and writing of the summaries has quite an impact on
TATi's overall performance and is therefore switched off by default.
====
