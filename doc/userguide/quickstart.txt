[[quickstart]]
Quickstart
----------

[[quickstart.sampling]]
Sampling in neural networks
~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[quickstart.dataset]]
.Dataset: "Two Clusters" dataset consisting of two normally distributed point clouds in two dimensions
image::pictures/dataset_two_clusters.png[{basebackend@docbook:scaledwidth="45%":width=400}]

Assume we are given a very simple data set as depicted in
link:#quickstart.dataset[Dataset]. The goal is to
classify all red and blue dots into two different classes. This problem
is quite simple to solve: a line in the two-dimensional space can easily
separate the two classes.

A very simple neural network, a perceptron, is all we need: it uses two
inputs nodes, namely each coordinate component, latexmath:[$x_{1}$] and
latexmath:[$x_{2}$], and a single output node with an activation
function latexmath:[$f$] whose sign gives the class the input item
belongs to. The network is given in
link:#quickstart.perceptron[Network].

[[quickstart.perceptron]]
.Network: Single-layer perceptron with weights and biases
image::pictures/simple_single_layer_perceptron.png[{basebackend@docbook:scaledwidth="25%":width=200}]

In the following we want to use the mean square loss, i.e. the euclidean
distance between the output from the network and the expected values per
item, as the network's loss function. The loss depends implicitly on the
dataset and explicitly on the weights and biases associated with the
network. In our case, we have two weights for the two edges between
input nodes, latexmath:[$w_{1}$] and latexmath:[$w_{2}$], and the output
node and a single bias attached to the output node latexmath:[$b$].

In sampling we look at a system of particles that have two internal
properties: location and momentum. The location is simply their current
value, that changes through its momentum over time. The momentum again
changes because the particle is inside a potential that drives it
towards the minimum. The system is described by a so-called Hamilton
operator that gives rise to its same named dynamics. If noise is
additionally taken into account, then we look at Langevin Dynamics.

Returning to the neural networks, the role of the particles is taken by
the degrees of freedom of the system: weights and biases. The loss
function is called the _potential_ and it is accompanied by a _kinetic
energy_ that is simply the sum of all squared momenta. Adding Momentum
to Optimizers in neural networks is a concept known already and inspired
by physics. The momentum helps in overcoming areas of the loss function
where it is essentially flat.

Sampling produces trajectories of particles moving along the manifold.
Integrals along these trajectories, if they are long enough, are
equivalent to integrating over the whole manifold, if the system is
ergodic.

By using sampling we mean to discover more of the loss manifold than
just the closest local minimum. The wording seems to indicate that we
would like to explore _all_ of the loss manifold. However, this is not
the case as there are regions we are not interested in, namely those
with large loss function values. In other words, we would like to sample
in such a way as only to stay in regions of the loss manifold associated
with small values. Generating trajectories by dynamics where the
negative of the gradient acts as a driving force onto each particle
automatically brings them into regions where the loss's value is small.
This is the principle behind Gradient Descent. However, in general all
possible minima locations will not form a connected region on the loss
manifold. These minima regions may be separated by barriers which are
needed to overcome. We distinguish two kinds,

* entropic barriers,
* enthalpic barriers.

Both of which are conceptually very simple. The enthalpic barrier is
simply a ridge that is very high where the particles need a large
momentum to overcome it. Entropic barriers on the other hand are
passages very small in volume that are simply very difficult to find. In
order to overcome barriers of the first kind, higher temperatures
suffice. For the second type of barrer, this is not so easy.
Metaphorically speaking, we are looking for possibly very small door.

This quick description of the problem of sampling in the context of
neural networks in data science should have prepare you now for the
following quickstart tutorial on how to actually use
ThermodynamicAnalyticsToolkit to perform sampling.

Let us have a closer look at a very simple loss landscape. In
link:#quickstart.landscape.neuralnetwork[Permutation symmetry] we
look at a very simple network of a single input node, with a single
hidden layer containing just one node and a single output layer.
Activation function is linear everywhere. We set the output node's and
hidden node's bias to zero. The dataset contains two cluster of points,
one (label -1) centered at -2, another (label +1) centered at 2 which we
do not depict here. Any product of the two degrees of freedom of the
network, namely its two weights, equal to unity will classify the data
well.

[[quickstart.network]]
.Permutation symmetry: Neural network with permutation symmetry to provoke multiple minima
image:pictures/neuralnetwork_permutation_symmetry.png[scaledwidth=45.0%]

In link:#quickstart.landscape.loss[Loss] we then
turn to the loss landscape depending on either weight. We see two minima
basins both of hyperbole or "banane" shape. There is a clear (enthalpic)
potential barrier in between. However, the minima basins themselves are
to some part entropic barriers as they are elongated and flat.

In the figure we also give a trajectory. Here, we have chosen such a
(inverse) temperature value such that it is able to pass the potential
barrier and reach the other minima basin.

[[quickstart.landscape.loss]]
.Loss landscape with an example trajectory
image:pictures/losslandscape_permutation_symmetry.png[scaledwidth=45.0%]

This is the goal of exploration: To find all (local) minima to allow to
pick the lowest, namely the global minimum.

[[quickstart.python]]
Using Python
~~~~~~~~~~~~

The package can be readily used inside any python3 shell or script.
However, this interface rather lends itself to quick testing than
rigorous experiments. You are still fine if you perform all your
scientific experiments inside python scripts kept safely inside a code
versioning system such as `git`.

If you have installed the package in the folder `/foo`, i.e. we have
folders `TATi/models` with a file `model.py` residing in there, then you
probably need to add it to the `PYTHONPATH` as follows

---------------
PYTHONPATH=/foo python3
---------------

In this shell, you may import the sampling part of the package as
follows

---------------
from TATi.models.model import model
---------------

This will import the abstract `model` class from the file mentioned
before. This class contains wrapper functions to setup the network with
either training and sampling in a few keystrokes.

In order to make you own python scripts executable and know about the
correct (non-standard) path to ThermodynamicAnalyticsToolkit, place the
following two lines at the veryg beginning of your script:

---------------
import sys
sys.path.insert(1,"<path_to_TATi>/lib/python3.5/site-packages/")
---------------

where ``<path_to_TATi>'' needs to be replaced by your specific
installation path and ``python3.5`` needs to be replaced if you are using a
different python version.

[[quickstart.python.simple_evaluation]]
Evaluating loss and gradients
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

ThermodynamicAnalyticsToolkit can also be used as a simplified interface
to access the loss and the gradients of the neural network. Then, it can
be treated as a simple high-dimensional function (the loss), whose
derivative (the gradients) is available as a numpy array. See the
following example which sets up a simple fully-connected hidden network
and evaluates loss and then loss and gradients combined.

[source,python]
---------------
include::python/eval_loss_gradient.py[]
---------------

[[quickstart.python.writing_data]]
Preparing a dataset
^^^^^^^^^^^^^^^^^^^

At the moment, datasets are parsed from Comma Separated Values (CSV)
files. In order for the following examples on optimization and sampling
to work, we need such a data file containing features and labels.

One option is to use the TATiDatasetWriter, see
link:#quickstart.cmdline.writing_dataset[Writing a dataset]. However, we can
do the same using python as well.

[source,python]
---------------
include::python/writing_data.py[]
---------------

WARNING:The labels need to be integer values. Importing will fail if they are not.

After importing some modules we first fix the numpy seed to 426 in order
to get the same items reproducibly. Then, we first create 100 items
using the hard-coded ClassificationDatasets class from the
*TWOCLUSTERS* dataset. We randomly perturb by a relative noise of 0.1.

Afterwards, these items are simply written to file using the csv module.

[NOTE]
The file `dataset-twoclusters.csv` is used in the following examples, so keep
it around.

[[quickstart.python.optimizing]]
Optimizing the network
^^^^^^^^^^^^^^^^^^^^^^

Let us first start with optimizing the network.

[source,python]
---------------
include::python/optimize.py[]
---------------

As you see, all options are set in a struct called `FLAGS` that controls
how the optimization is performed. There is a helper function in `model`
called `setup_parameters` that creates the FLAGS for you with some
default parameters.

Let us quickly go through each of the parameters:

* batch_size
+
sets the subset size of the data set looked at per training step, if
smaller than dimension, then we add stochasticity/noise to the training
but for the advantage of smaller runtime.
* max_steps
+
gives the amount of training steps to be performed.
* optimizer
+
defines the method to use for training. Here, we use Gradient Descent
(in case batch_size is smaller than dimension, then we actually have
Stochastic Gradient Descent).
* output_activation
+
defines the activation function of all output nodes, here it is linear.
Other choices are: tanh, relu, relu6.
* seed
+
sets the seed of the random number generator. We will still have full
randomness but in a deterministic manner, i.e. calling the same
procedure again will bring up the exactly same values.
* step_width
+
defines the scaling of the gradients in each training step, i.e. the
learning rate. Values too large may miss the minimum, values too small
need longer to reach it.

For small networks the option ``do_hessians'' might be useful which will
compute the hessian matrix at the end of the trajectory and use the
largest eigenvalue to compute the optimal step width. This will add
nodes to the underlying computational graph for computing the components
of the hessian matrix.

[NOTE]
====
The creation of the nodes (not speaking of their evaluation) is a O(N^2)
process in the number of parameters of the network N. Hence, this should
only be done for small networks and on purpose.
====

Moreover, we did not say anything about _sampler_ as this is covered in
the next section.

Afterwards, the network is initialized, then we call `train()` which
performs the training and returns runtime info, trajectory, and averages
as a pandas DataFrame.

At the end of this section on training, let us have a quick glance at
the decrease of the loss function over the steps by using `matplotlib`.

[source,python]
---------------
include::python/plot_optimize.py[]
---------------

The graph should look similar to the one obtained with pgfplots (see
https://sourceforge.net/pgfplots in
link:#quickstart.python.optimizing.plot[figure_title].

.Plot of the loss history for the optimization run
image::pictures/optimization-step_loss.png[{basebackend@docbook:scaledwidth="60%":width=500}]

Go and have a look at the other columns. Or try to visualize the change
in the parameters (weights and biases) in the trajectories dataframe.

[[quickstart.python.sampling]]
Sampling the network
^^^^^^^^^^^^^^^^^^^^

After optimization we may continue sampling the network.

[source,python]
---------------
include::python/sample.py[]
---------------

Here, the _sampler_ setting takes the place of the `optimizer` before as
it states which sampling scheme to use. At the moment the following are
available: SGLD GeometricLangevinAlgorithm_1st,
GeometricLangevinAlgorithm_2nd, BAOAB, and HamiltonianMonteCarlo. GLA
2nd or BAOAB are currently recommended to use as they are second order
schemes and provide higher accuracies and allow for larger step widths.

Again, we produce three output arrays: run info, trajectory, and
averages. Trajectories contains among others all parameter degrees of
freedom for each step (or ``every_nth'' step). Run info contains loss,
accuracy, norm of gradient, norm of noise and others. Finally, in
averages we compute averages over the trajectory such as average
(ensemble) loss, average kinetic energy, average virial. There it is
advisable to skip some initial ateps (``burn_in_steps'') to allow for
some burn in time, i.e. for kinetic energies to adjust from initially
zero momenta. Some columns depend on whether the sampler provides the
specific quantity, e.g. SGLD does not have momentum, hence there will be
no average kinetic energy.

[[quickstart.python.sampling.supply_dataset]]
Provide your own dataset
++++++++++++++++++++++++

Using the Python API you can directly supply your own dataset, e.g. from
a numpy array residing in memory. See the following example where we do
not generate the data but parse them from a CSV file instead using the
pandas module.

[source,python]
---------------
include::python/supply_dataset.py[]
---------------

The major difference is that `batch_data_files` is now empty and instead
we call the function `provide_data` in order to provide an in-memory
dataset. In this example, we have parsed the same file as the in the
previous section into a numpy array using the pandas module. Natually,
this is just one way of creating a suitable numpy array. Input and
output dimensions are directly deduced from the the tuple sizes.

Note that we have ignored the average output here, by default none of
the three output pandas dataframes are accumulated.

[[quickstart.python.sampling.priors]]
Using a prior
+++++++++++++

You may add a prior to the sampling. At the current state two kinds of
priors as follows are available. In ``FLAGS'' `prior_upper_boundary` and
`prior_lower_boundary` give the admitted interval per parameter. Within
a relative distance of 0.01 (with respect to length of domain and only
in that small region to the specified boundary) an additional force acts
upon the particles to drive them back into the desired domain. Its
magnitude increases with distance to the covered inside the boundary
region. The distance is taken to the power of `prior_power`. The force
is modified by `prior_factor`.

If upper and lower boundary coincide, then we have the case of
tethering, where all parameters are pulled inward to the same point.

At the moment prioring just a subset of particles is not supported.

[NOTE]
====
The prior force is acting directly on the variables. It does not modify
momentum. Moeover, it is a force! In other words, it depends on step
width. If the step width is too large and if the repelling force
increases too steeply close to the walls with respect to the normal
dynamics of the system, it may blow up.
====

[[quickstart.python.sampling.optimize_then_sample]]
First optimize, then sample
+++++++++++++++++++++++++++

We might also concatenate optimize and sample if, in between the two, we
adjust FLAGS as follows:

[source,python]
---------------
include::python/optimize_sample.py[]
---------------

The only thing we change in FLAGS is the number of steps and adding the
sampler. However, as the model simply stores a copy of the FLAGS, in
order to update the FLAGS in the model class as well, we need to reset
it. Afterwards, we again initialize the network which will add only the
sampling nodes and prepare output files differently. We have skipped the
following steps that are equivalent to
link:#quickstart.python.sampling.example[example_title]. Again, at the
very end we obtain pandas DataFrame containing runtime information,
trajectory, and averages.

[NOTE]
====
This is actually the _recommended_ way of doing sampling: First make sure that
the parameters start in a local minima and from there we explore the surrounding
manifold.
====

[[quickstart.python.analysis]]
Analysing trajectories
^^^^^^^^^^^^^^^^^^^^^^

Analysis is so for constrained to parsing in run and trajectory files
that you would write through optimization and sampling runs.

To this end, specify `FLAGS.run_file` and `FLAGS.trajectory_file` with
some valid file names.

Subsequently, these may be easily parsed as follows, see also
"TATiAnalyser.in".

[source,python]
---------------
include::python/analyse.py[]
---------------

This would give a plot of the running average for each parameter in the
trajectory file. In a similar, the run file can be loaded and its
average quantities such as loss or kinetic energy be analysed and
plotted.

[[quickstart.python.exploration]]
Exploring the loss manifold
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Exploration of the loss manifold is a bit more involved using the python
interface.

In general, the procedure has the following stages:

* We sample a start trajectory.
* For the current set of trajectory points we perform a diffusion map
analysis. Using the first eigenvector as the dominant diffusion mode, we
pick the first corner points at its maximal component.
* If more corner points are needed, then we look at the diffusion
distance with respect to already picked corner points over all
eigenvectors of the diffusion map and pick the next point always such
that it maximizes the diffusion distance to the present ones.
* Finally, we sample further trajectories, one starting at each of the
picked corner points.
* This is repeated (go to second step) for as many exploration steps as
we want to do.

Note that each single trajectory is sampled in a special way:

* First, three legs of sampling are performed
* Then, we analyse the resulting diffusion map.
* If the eigenvalues have not yet converged with respect to some
relative threshold, we continue for one more leg and analyse again after
that
* If they have converged, we stop.
* Finally, we look at the norm of the gradients along the trajectory. If
it is below a certain threshold, then within this section of the
trajectory (with gradient norms beneath the threshold) we pick the
smallest gradient value as the trajectory step being a possible minimum
candidate.
* For all minimum candidates (if any) we run additional optimization
trajectories, e.g. using GradientDescent, to find a local minima.

Have a look at the following example.

[source,python]
---------------
include::python/explore.py[]
---------------

This performs exactly the procedure described before using very, very
short trajectories (`max_steps`), only a few legs (`max_legs`) and only
a very limited number of exploration steps (`exploration_steps
        `). This is simple for the purpose of illustration. Naturally,
larger values for all these parameters are required in order to explore
complex manifolds and eventually find the global minima.

[[quickstart.cmdline]]
Using command-line interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All the tests use the command-line interface and for performing rigorous
scientific experiments, we recommend using this interface as well. Here,
it is to do parameter studies and have extensive runs using different
seeds.

[[quickstart.cmdline.writing_dataset]]
Creating the dataset
^^^^^^^^^^^^^^^^^^^^

As data is read from file, this file needs to be created beforehand.

For a certain set of simple classification problems, namely those that
can be found in the tensorflow playground, we have added a
TATiDatasetWriter` that spills out the dataset in CSV format.

[source,bash]
---------------
include::cmdline/write_dataset.sh[]
---------------

This will write 500 datums of the dataset type 2 ("two clusters") to a
file ``testset-twoclusters.csv'' using all of the points as we have set
the test/train ratio to 0. Note that we also perturb the points by 0.1
relative noise.

[[quickstart.cmdline.parsing_dataset]]
Parsing the dataset
^^^^^^^^^^^^^^^^^^^

Similarly, for testing the dataset can be parsed using the same
tensorflow machinery as is done for sampling and optimizing, using

[source,bash]
---------------
include::cmdline/parse_dataset.sh[]
---------------

where the _seed_ is used for shuffling the dataset.

[[quickstart.cmdline.optimizing]]
Optimizing the network
^^^^^^^^^^^^^^^^^^^^^^

As weights (and biases) are usually uniformly random initialized and the
potential may therefore start with large values, we first have to
optimize the network, using (Stochastic) Gradient Descent (GD).

[source,bash]
---------------
include::cmdline/optimize.sh[]
---------------

This call will parse the dataset from the file
"dataset-twoclusters.csv". It will then perform a (Stochastic) Gradient
Descent optimization in batches of 50 (10% of the dataset) of the
parameters of the network using a step width/learning rate of 0.01 and
do this for 1000 steps after which it stops and writes the resulting
neural network in a TensorFlow-specific format to a set of files, one of
which is called `model.ckpt.meta` (and the other filenames are derived
from this).

We have also created a file `run.csv` which contains among others the
loss at each (``every_nth'', respectively) step of the optimization run.
Plotting the loss over the step column from the run file will result in
a figure similar to in
link:#quickstart.python.optimizing.plot[figure_title].


[NOTE]
====
Since Tensorflow 1.4 an absolute path is required for the storing the
model. In the example we use the current directory returned by the unix
command `pwd`.
====

If you need to compute the optimal step width, which is possible for
smaller networks from the largest eigenvalue of the hessian matrix, then
use the option ``do_hessians 1'' to activate it.

[NOTE]
====
The creation of the nodes is costly, latexmath:[O(N^2)] in the number of
parameters of the network N. Hence, may not work for anything but small
networks and should be done on purpose.
====

In case you have read the quickstart tutorial on the Python interface
before, then the names of the command-line option will probably remind
you of the variables in the FLAGS structure.

[[quickstart.cmdline.sampling]]
Sampling trajectories on the loss manifold
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We continue from this optimized or equilibrated state with sampling. It
is called equilibrated as the network's parameter should now be close to
a (local) minimum of the potential function and hence in equilibrium.
This means that small changes to the parameters will result in gradients
that force it back into the minimum.

Let us call the sampler.

[source,bash]
---------------
include::cmdline/sample.sh[]
---------------

This will cause the sampler to parse the same dataset as before.
Moreover, the sampler will load the neural network from the model, i.e.
using the optimized parameters right from the start. Afterwards it will
use the GLA in 2nd order discetization using again step_width of 0.01
and running for 1000 steps in total. The GLA is a descretized variant of
Langevin Dynamics whose accuracy scales with the inverse square of the
step_width (hence, 2nd order).

The seed is needed as we sample using Langevin Dynamics where a noise
term is present. The term basically ascertains a specific temperature
which is proportional to the average momentum of each particle.

After it has finished, it will create three files; a run file
`run.csv` containing run time information such as the step, the
potential, kinetic and total energy at each step, a trajectory file
`trajectory.csv` with each parameter of the neural network at each step,
and an averages file `averages.csv` containing averages accumulated
along the trajectory such as average kinetic energy, average virial (
connected to the kinetic energy through the virial theorem, valid if a
prior keeps parameters bound to finite values), and the average
(ensemble) loss. Moreover, for the HMC sampler the average rejection
rate is stored there. The first two files we need in the next stage.

[[quickstart.cmdline.analysing]]
Analysing trajectories
^^^^^^^^^^^^^^^^^^^^^^

Eventually, we now perform the diffusion map analysis on the obtained
trajectories. The trajectory file written in the last step is simply a
matrix of dimension (number of parameters) times (number of trajectory
steps). The eigenvector to the largest (but one) eigenvalue will give
the dominant direction in which the trajectory is moving.

[NOTE]
====
The largest eigenvalue is usually unity and its eigenvector is constant.
Therefore, it is omitted. That's why indexing for the diffusion maps
eigenvectors starts at 1 (omitted the constant eigenvector 0).
====

The analysis can perform three different tasks:

* Calculating averages.
* Calculating the diffusion map's largest eigenvalues and eigenvectors.
* Calculating landmarks and level sets to obtain an approximation to the
free energy.

[[quickstart.cmdline.analysing.averages]]
Averages
++++++++

Averages are calculated by specifying two options as follows:

[source,bash]
---------------
include::cmdline/analyse_average.sh[]
---------------

This will load both the run file `run.csv` and the trajectory file
`trajectory.csv`and average over them using only every 10th data point
(_every_nth_) and also dropping the first steps below 100
(_drop_burnin_). It will produce then ten averages (_steps_) for each of
energies in the run file and each of the parameters in the trajectories
file (along with the variance) from the first non-dropped step till one
of the ten end steps. These end steps are obtained by equidistantly
splitting up the whole step interval.

Eventually, we have two output file. The averages over the run
information such as total, kinetic, and potential energy in
`average_run.csv`. Also, we have the averages over the degrees of
freedom in `average_trajectories.csv`.

[NOTE]
====
Averages depend crucially on the number of steps we average over. I.e.
the more points we throw away, the less accurate it becomes. In other
words, if large accuracy is required, the averages file (if it contains
the value of interest) is a better place to look for.
====

[[quickstart.cmdline.analysing.diffusion_map]]
Diffusion map
+++++++++++++

The eigenvalues and eigenvectors can be written as well to two output
files.

[source,bash]
---------------
include::cmdline/analyse_diffmap.sh[]
---------------

The files ending in `..values.csv` contains the eigenvalues in two
columns, the first is the eigenvalue index, the second is the
eigenvalue.

The other file ending in `..vectors.csv` is simply a matrix of the
eigenvector components in one direction and the trajectory steps in the
other. Additionally, it contains the parameters at the steps and also
the loss and the kernel matrix entry.

Note that again the all values up till step 100 are dropped and only
every 10th trajectory point is considered afterwards.

There are two methods available. Here, we have used the simpler (and
less accurate) (plain old) vanilla method. The other is called TMDMap.

If you have installed the _pydiffmap_ python package, this mal also be
specified as diffusion map method. It has the benefit of an interal
optimal parameter choice. Hence, it should behave more robustly than the
other two methods. TMDMap is different only in reweighting tre samples
according to the specific temperature.

[[quickstart.cmdline.analysing.free_energy]]
Free energy
+++++++++++

Last but not least, the free energy is calculated.

[source,bash]
---------------
include::cmdline/analyse_free_energy.sh[]
---------------

This will extract landmark points from the trajectory. Basically, the
loss manifold is discretized using these landmarks where all
configurations close to a landmark step are combined onto a so-called
level-set, i.e. all these configurations have a similar loss function
value. By knowing the number of configurations in each level set and
knowing the level sets loss value, an approximation of the free energy
is computed.

This is computed for every step of the trajectory and it is insightful
to look at the free energy over the course of the trajectory represented
by the first eigenvalue. If in this graph clear minima with maxima in
between can be seen, then there are enthalpic barriers between two local
minima. If on the other hand there are flat areas, then we found
entropic barriers.

Both these types of barriers obstruct trajectories and keep the
optimization trapped in so-called meta-stable states. Each type of
barrier requires a different type of remedy to overcome.

[[quickstart.cmdline.exploration]]
Exploring the loss manifold
+++++++++++++++++++++++++++

Eventually, we are not interested in obtaining trajectories on the loss
manifold. Instead we would like to find the global minima. Or at least
have a good idea about whether the minimas we have found so far are
reasonable.

To this end, a command-line tool called `TATiExplorer` is provided. The
idea is to make use of the diffusion map with its diffusion distance to
assess what part of the loss manifold has been explored already.
Moreover, we use multiple trajectories that are spawned from a specific
number of places that are maximally separate with respect to their
diffusion distance. This will ensure that we cover the most ground
possible.

In the end, the eigenvectors obtained through a run using the
`TATiExplorer` will return the dominant diffusion directions and
therefore those pointing in the direction along the minima, i.e. where
the sampling usually gets stuck and remains for a while, hence diffusion
is slow.

[source,bash]
---------------
include::cmdline/exploring.sh[]
---------------

In the example we call the explorer utility in much the same way as we
have called the sampler. There are some additional options that give the
number of eigenvalues to calculate and which diffusion map method to
use. Note that `max_steps` now gives the number of steps of a single
leg. Further down you find what a lag actually is.

Furthermore, there are two options unique to the explorer. This is
`max_legs` which gives the maximum number of legs to look at. Each leg
goes over max_steps. After that a diffusion map analysis is performed
that checks whether the eigenvalues have converged already. If yes, the
trajectory is ended, if not we continue with a new leg (of max_steps
steps). If no convergence should occur, max_legs gives the maximum
number of legs after which the trajectory is terminated regardlessly.

Finally, we run multiple trajectories in parallel from starting points
that are maximally apart from each other in the sense of the diffusion
distances. This is controlled by `number_of_parallel_trajectories`.

[[quickstart.parallelization]]
A note on parallelization
~~~~~~~~~~~~~~~~~~~~~~~~~

Internally, Tensorflow uses a computational graph to represent all
operations. Nodes in the graph represent computations and their results
and edges represent dependencies between these values, i.e. some may act
as input to operations resulting in certain output.

Because of this internal representation Tensorflow has two kind of
parallelisms:

* inter ops
* intra ops

Each is connected to its its own thread pool. Both the command-line and
the Python interface let you pick the number of threads per pool. If 0
is stated (default), then the number of threads is picked automatically.

In general, ``inter_ops_threads''refers to multiple cores performing
matrix multiplication or reduction operations together.
``intra_ops_threads'' seems to be connected to executing multiple nodes
in parallel that are independent of each other but this is guessing at
the moment.

[WARNING]
====
When setting `inter_ops_threads` _unequal_ to 1, then subsequent runs
may produce different results, i.e. results are no longer strictly
reproducible. According to Tensorflow this is because reduction
operations such as `reduce_sum` run non-deterministically on multiple
cores for sake of speed.
====

[[quickstart.conclusion]]
Conclusion
~~~~~~~~~~

This has been the very quick introduction into samping done on neural
network's loss function manifolds. You have to take it from here.
