[[quickstart]]
Quickstart
----------

[[quickstart.sampling]]
Sampling in neural networks
~~~~~~~~~~~~~~~~~~~~~~~~~~~

[[quickstart.dataset]]
.Dataset: "Two Clusters" dataset consisting of two normally distributed point clouds in two dimensions
image::pictures/dataset_two_clusters.png[align="center",{basebackend@docbook:scaledwidth="45%":width=400}]

Assume we are given a very simple data set as depicted in
link:#quickstart.dataset[Dataset]. The goal is to
classify all red and blue dots into two different classes. This problem
is quite simple to solve: a line in the two-dimensional space can easily
separate the two classes.

A very simple neural network, a perceptron, is all we need: it uses two
inputs nodes, namely each coordinate component, latexmath:[$x_{1}$] and
latexmath:[$x_{2}$], and a single output node with an activation
function latexmath:[$f$] whose sign gives the class the input item
belongs to. The network is given in
link:#quickstart.perceptron[Network].

[[quickstart.perceptron]]
.Network: Single-layer perceptron with weights and biases
image::pictures/simple_single_layer_perceptron.png[align="center",{basebackend@docbook:scaledwidth="25%":width=200}]

Traing a Neural Network
^^^^^^^^^^^^^^^^^^^^^^^

In the following we want to use the mean square loss, i.e. the euclidean
distance between the output from the network and the expected values per
item, as the network's loss function. The loss depends implicitly on the
dataset and explicitly on the weights and biases associated with the
network. In our case, we have two weights for the two edges between
input nodes, latexmath:[$w_{1}$] and latexmath:[$w_{2}$], and the output
node and a single bias attached to the output node latexmath:[$b$].

The general goal of training is to find the set of parameters that achieve the
lowest loss. To this end, we use the gradient that is readily obtained from the
neural netwotk through backpropagation of the analytically known derivatives of
the activation functions. The parameters are modified using Gradient Descent
until the gradient becomes zero (or we stop before that).

What is Sampling?
^^^^^^^^^^^^^^^^^

Sampling typically has quite a different perspective: There, we look at a
system of particles that have two internal properties: _location_ and
_momentum_. The location is simply their current value, that changes through
its momentum over time. The momentum again changes because the particle are
affected by a potential. The system is described by a so-called
Hamilton operator that gives rise to its same named dynamics. If noise is
additionally taken into account, then instead we look at Langevin Dynamics.
_Noise is essential_ here: It is connected to the concept of _temperature_ as
the average squared momenta of each particle. High temperature therefore means
large amounts of noise, while low temperature is associated with small amounts.

Returning to the neural networks, the role of the particles is taken by
the degrees of freedom of the system: weights and biases. The loss
function is called the _potential_ and it is accompanied by a _kinetic
energy_ that is simply the sum of all squared momenta. Adding Momentum
to Optimizers in neural networks is a concept known already and inspired
by physics. There, it counteracts areas of the loss function where it is
essentially flat and the gradient therefore close to zero.

Sampling produces trajectories of particles moving along the manifold.
Integrals along these trajectories, if they are long enough, are
equivalent to integrating over the whole manifold, if the system is
ergodic.

By using sampling we mean to discover more of the loss manifold than
just the closest local minimum. The wording seems to indicate that we
would like to explore _all_ of the loss manifold. However, this is not
the case as there are regions we are not interested in, namely those
with large loss function values. In other words, we would like to sample
in such a way as only to stay in regions of the loss manifold associated
with small values. Generating trajectories by dynamics where the
negative of the gradient acts as a driving force onto each particle
automatically brings them into regions where the loss's value is small.
This is the principle behind Gradient Descent. However, in general all
possible minima locations will not form a connected region on the loss
manifold. These minima regions may be separated by barriers which are
needed to overcome. We distinguish two kinds,

* entropic barriers,
* enthalpic barriers.

Both of which are conceptually very simple. The enthalpic barrier is
simply a ridge that is very high where the particles need a large
momentum to overcome it. Entropic barriers on the other hand are
passages very small in volume that are simply very difficult to find. In
order to overcome barriers of the first kind, higher temperatures
suffice. For the second type of barrer, this is not so easy.
Metaphorically speaking, we are looking for a possibly very small door like
Alice in Wonderland.

What Does the Landscape Look Like?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let us have a closer look at a very simple loss landscape. In
link:#quickstart.landscape.neuralnetwork[Permutation symmetry] we
look at a very simple network of a single input node, with a single
hidden layer containing just one node and a single output layer.
Activation function is linear everywhere. We set the output node's and
hidden node's bias to zero. The dataset contains two cluster of points,
one (label -1) centered at -2, another (label +1) centered at 2 which we
do not depict here. Any product of the two degrees of freedom of the
network, namely its two weights, equal to unity will classify the data
well.

[[quickstart.network]]
.Permutation symmetry: Neural network with permutation symmetry to provoke multiple minima
image:pictures/neuralnetwork_permutation_symmetry.png[align="center",{basebackend@docbook:scaledwidth="50%":width=500}]

In link:#quickstart.landscape.loss[Loss manifold] we then
turn to the loss landscape depending on either weight. We see two minima
basins both of hyperbole or "banane" shape. There is a clear (enthalpic)
potential barrier in between.

In the figure we also give a trajectory. Here, we have chosen such a
(inverse) temperature value such that it is able to pass the potential
barrier and reach the other minima basin. As we have mentioned before, higher
temperature helps to overcome enthalpic barriers.

[[quickstart.landscape.loss]]
.Loss manifold: Loss landscape with an example trajectory
image:pictures/losslandscape_permutation_symmetry.png[scaledwidth=45.0%]

This quick description of the problem of sampling in the context of
neural networks in data science should have acquainted you with some of the
physical concepts underlying the idea of sampling. It hopefully has prepared
you for the following quickstart tutorial on how to actually use
ThermodynamicAnalyticsToolkit to perform sampling.

[[quickstart.python]]
Using Python
~~~~~~~~~~~~

The package can be readily used inside any python3 shell or script.

[TIP]
The Python interface lends itself well as a module inside larger programs
or to quick testing rather than rigorous experiments. You are still fine if you
perform all your scientific experiments inside python scripts kept safely
inside a code versioning system such as `git`.

If you have installed the package in the folder `/foo`, i.e. we have
folders `TATi/models` with a file `model.py` residing in there, then you
probably need to add it to the `PYTHONPATH` as follows

---------------
PYTHONPATH=/foo python3
---------------

In this shell, you may import the sampling part of the package as
follows

---------------
import TATi.simulation as tati
---------------

This will import the `simulation` interface class from the file mentioned
before. This class contains a set of convenience functions that hides all the
complexity of setting up of input pipelines and networks. Accessing the loss
function, gradients and alike or training and sampling can be done in just
a few keystrokes.

In order to make you own python scripts executable and know about the
correct (possibly non-standard) path to ThermodynamicAnalyticsToolkit, place
the following two lines at the veryg beginning of your script:

---------------
import sys
sys.path.insert(1,"<path_to_TATi>/lib/python3.5/site-packages/")
---------------

where +<path_to_TATi>+ needs to be replaced by your specific
installation path and +python3.5+ needs to be replaced if you are using a
different python version.

In the following we will first be creating a dataset to work on. This example
code will be the most extensive one. All following ones are rather short and
straight-forward.

[[quickstart.python.writing_data]]
Preparing a dataset
^^^^^^^^^^^^^^^^^^^

Therefore, let us prepare the dataset, see the Figure link:#quickstart.dataset[Dataset],
for our following experiments.

At the moment, datasets are parsed from Comma Separated Values (CSV)
or Tensorflow's own TFRecord files or can be provided in-memory from numpy
arrays. In order for the following examples on optimization and sampling to
work, we need such a data file containing features and labels.

TATi provides a few simple dataset generators contained in the class
`ClassificationDatasets`.

One option therefore is to use the TATiDatasetWriter that provides access to
`ClassificationDatasets`, see link:#quickstart.cmdline.writing_dataset[Writing a dataset].
However, we can do the same using python as well. This should give you an idea
that you are not constrained to the `simulation` part of the Python interface,
see the reference on the general Python interface where we go through the
same examples without importing `simulation`.

[source,python]
---------------
include::python/writing_data.py[]
---------------

[WARNING]
The labels need to be integer values. Importing will fail if they are not.

After importing some modules we first fix the numpy seed to 426 in order
to get the same items reproducibly. Then, we first create 100 items
using the `ClassificationDatasets` class from the *TWOCLUSTERS* dataset with
a random perturbation of relative 0.1 magnitude. We
shuffle the dataset as the generators typically create first items of one
label class and then items of the other label class.

[NOTE]
The class `ClassificationDatasets` mimicks the dataset examples that can also
be found on the link:https://playground.tensorflow.org/[Tensorflow playground].

Afterwards, we write the dataset to a simple CSV file with columns "x1", "x2",
and "label".

[CAUTION]
The file `dataset-twoclusters.csv` is used in the following examples, so keep
it around.

This is the very simple dataset we want to learn, sample from and exlore in the
following.

[[quickstart.python.simple_evaluation]]
Evaluating loss and gradients
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

ThermodynamicAnalyticsToolkit can also be used as a simplified interface
to access the loss and the gradients of the neural network. Then, it can
be treated as a simple high-dimensional function (the loss), whose
derivative (the gradients) is available as a numpy array. See the
following example which sets up a simple fully-connected hidden network
and evaluates loss and then loss and gradients combined.

[source,python]
---------------
include::simulation/complex/eval_loss_gradient.py[]
---------------

All we need to do is set some parameters -- here, 'batch_data_files' sets the
dataset file to parse, batch size of 10 and we use a linear output activation
function -- assign all network parameters to zero and then evaluate first the
loss and then the gradients of the network.

As you see `tati` forms the general interface class that contains the network
along with the dataset and everything.

This is basically all the access you need in order to use your own optimization,
sampling, or exploration methods in the context of neural networks in a
high-level, abstract way.

[[quickstart.python.optimizing]]
Optimizing the network
^^^^^^^^^^^^^^^^^^^^^^

Let us then start with optimizing the network, i.e. learning the data.

[source,python]
---------------
include::simulation/complex/optimize.py[]
---------------

Again all options are set in the init call to the interface. These options
control how the optimization is performed, what kind of network is created,
how often values are stored. In other words everything.

TIP: In case you need to change these options elsewhere in your python code,
use `set_options()`.

Let us quickly go through each of the parameters:

* 'batch_size'
+
sets the subset size of the data set looked at per training step, if
smaller than dimension, then we add stochasticity/noise to the training
but for the advantage of smaller runtime.

* 'learning_rate'
+
defines the scaling of the gradients in each training step, i.e. the
learning rate. Values too large may miss the minimum, values too small
need longer to reach it.

* 'max_steps'
+
gives the amount of training steps to be performed.

* optimizer
+
defines the method to use for training. Here, we use Gradient Descent
(in case 'batch_size' is smaller than dimension, then we actually have
Stochastic Gradient Descent).

* 'output_activation'
+
defines the activation function of all output nodes, here it is linear.
Other choices are: tanh, relu, relu6.

* 'seed'
+
sets the seed of the random number generator. We will still have full
randomness but in a deterministic manner, i.e. calling the same
procedure again will bring up the exactly same values.

In our case, the default option values are such that the network we use
looks exactly as in the Figure link:#quickstart.perceptron[Network], namely
a single-layer perceptron whose number of input and output nodes are completely
fixed by the dataset.

For these small networks the option 'do_hessians' might be useful which will
compute the hessian matrix at the end of the trajectory and use the
largest eigenvalue to compute the optimal step width. This will add
nodes to the underlying computational graph for computing the components
of the hessian matrix. However, we will not do so here.

[CAUTION]
====
The creation of these hessian evaluation nodes (not speaking of their
evaluation) is a latexmath:[O(N^2)] process in the number of parameters of the
network N. Hence, this should only be done for small networks and on purpose.
====

After the options have been provided, the network is initialized internally
and automatically, we then call `fit()` which performs the training and returns
runtime info, trajectory, and averages as a pandas `DataFrame`.

TIP: You want more output of what is actually going on in each training step?
Set `verbose=1` or even `verbose=2` in the `init()` of the interface.

Let us have a quick glance at the decrease of the loss function over the steps
by using `matplotlib`. In other words, let us look how effective the training
has been.

[source,python]
---------------
include::python/plot_optimize.py[]
---------------

The graph should look similar to the one obtained with `pgfplots` (see
https://sourceforge.net/pgfplots in
link:#quickstart.python.optimizing.plot[figure_title].

.Plot of the loss history for the optimization run
image::pictures/optimization-step_loss.png[{basebackend@docbook:scaledwidth="60%":width=500}]

As you see the loss has decreased quite quickly down to 1e-3. Go and have a
look at the other columns such as accuracy. Or try to visualize the change
in the parameters (weights and biases) in the trajectories dataframe. See
link:https://pandas.org/dataframe[Dataframe help] if you are unfamiliar
with the `pandas` module, yet.

Obviously, we did not use a different dataset set for testing the effectiveness
of the training which should commonly be done. This way we cannot check whether
we have overfitted or not. However, our example is trivial by design and the
network too small to be prone to overfitting this dataset.

Nonetheless, we show how to supply a different dataset and evaluate loss and
accuracy on it.

[[quickstart.python.sampling.supply_dataset]]
Provide your own dataset
++++++++++++++++++++++++

You can directly supply your own dataset, e.g. ,from a numpy array residing in
memory. See the following example where we do not generate the data but parse
them from a CSV file instead using the pandas module.

[source,python]
---------------
include::simulation/complex/supply_dataset.py[]
---------------

The major difference is that `batch_data_files` is now empty and instead
we simply assign `dataset` the in-memory dataset to use. Note that we could
also have supplied it directly with the filename +dataset-twoclusters.csv+. In
this example we have parsed the same file as the in the previous section into
a numpy array using the pandas module. Natually, this is just one way of
creating a suitable numpy array.

NOTE: Input and output dimensions are directly deduced from the the tuple sizes.

In the following link:#quickstart.python.sampling[section] we will explain
what each of these three dataframes exactly contains.

[[quickstart.python.sampling]]
Sampling the network
^^^^^^^^^^^^^^^^^^^^

Optimization only steps down to the nearest local minimum from some initial
random starting position. Only through sampling do we actually uncover the shape
of the loss manifold and thereby are able to deduce whether our network is
efficient at doing its job.

Nonetheless, optimization is always the initial step to sampling as we need
our random starting configuration to touch base with the loss manifold. If we
start at a hill, then the sampler will accumulate a lot of momentum when going
downhill which will carry him on for a quite a while. In contrast, we want the
sampler to move around by thermal noise. Therefore, we want to start at a point
where gradients are small, best close to zero, and simply use random initial
momenta.

However, let us ignore this procedure for the moment and simply look at
sampling from a random initial place on the loss manifold.

[source,python]
---------------
include::simulation/complex/sample.py[]
---------------

Here, the 'sampler' setting takes the place of the 'optimizer' before as
it states which sampling scheme to use. See <<reference.samplers>> for a
complete list and their parameter names.

NOTE: In the context of sampling we use 'step_width' in place of 'learning_rate'.

Again, we produce three output arrays: run info, trajectory, and
averages. Trajectories contains among others all parameter degrees of
freedom for each step (or 'every_nth' step). Run info contains loss,
accuracy, norm of gradient, norm of noise and others, again for each step.
Finally, in averages we compute running averages over the trajectory such as
average (ensemble) loss, average kinetic energy, average virial. There it is
advisable to skip some initial ateps ('burn_in_steps') to allow for
some burn in time, i.e. for kinetic energies to adjust from initially
zero momenta. Some columns depend on whether the sampler provides the
specific quantity, e.g. link:#reference.samplers.sgld[SGLD] does not have
momentum, hence there will be
no average kinetic energy.

[[quickstart.python.sampling.priors]]
Using a prior
+++++++++++++

You may add a prior to the sampling. At the current state two kinds of
priors are available: wall-repelling and tethering.

The options 'prior_upper_boundary' and 'prior_lower_boundary' give the admitted
interval per parameter. Within a relative distance of 0.01 (with respect to
length of domain and only in that small region next to the specified boundary)
an additional force acts upon the particles to drive them back into the desired
domain. Its magnitude increases with distance to the covered inside the boundary
region. The distance is taken to the power of 'prior_power'. The force
is modified by 'prior_factor'.

If upper and lower boundary coincide, then we have the case of
tethering, where all parameters are pulled inward to the same point.

At the moment prioring just a subset of particles is not supported.

[NOTE]
====
The prior force is acting directly on the variables. It does not modify
momentum. Moreover, it is a force! In other words, it depends on step
width. If the step width is too large and if the repelling force
increases too steeply close to the walls with respect to the normal
dynamics of the system, it may blow up. On the ither hand, if it is too weak,
then particles may even escape.
====

[[quickstart.python.sampling.optimize_then_sample]]
First optimize, then sample
+++++++++++++++++++++++++++

As we have already alluded to before, optimizing before sampling is the
_recommended* procedure. In the following example, we concatenate the two.
To this end, we might need to modify some of the options in between. Let us have
a look, however with a slight twist.

The dataset shown in Figure link:#quickstart.dataset[Dataset] can be even
learned by a simpler network, namely only one of the input nodes is actually
needed because of the symmetry.

Hence, we look at such a network by using 'input_columns' to only use input
column "x1" although the dataset contains both "x1" and "x2".

Moreover, we will add a hidden layer with a single node and thus obtain a
network as depicted in Figure link:#quickstart.network[Permutation symmetry].
We add this node hidden node to make the loss manifold a little bit more
interesting.

[source,python]
---------------
include::simulation/complex/optimize_sample.py[]
---------------

We needed to change the number of steps, set a sampling step width and add the
sampler (which might depend on additional parameters, see <<reference.samplers>>
).
At the very end we again obtain `pandas` DataFrame containing runtime information,
trajectory, and averages.

[[quickstart.python.analysis]]
Analysing trajectories
^^^^^^^^^^^^^^^^^^^^^^

Analysis involves parsing in run and trajectory files that you would write
through optimization and sampling runs. Naturally, you could also perform this
on the `pandas` dataframes directly. However, for completeness we will read
from files in the examples of this section.

To this end, specify `FLAGS.run_file` and `FLAGS.trajectory_file` with
some valid file names.

[[quickstart.python.analysis.averages]]
Averages
++++++++

Subsequently, these may be easily parsed as follows, see also
+tools/TATiAnalyser.in+ in the repository.

[source,python]
---------------
include::python/analyse_averages.py[]
---------------

This would give a plot of the running average for each parameter in the
trajectory file. In a similar, the run file can be loaded and its
average quantities such as loss or kinetic energy be analysed and
plotted.

[[quickstart.python.analysis.diffusion_map]]
Diffusion Map
+++++++++++++

Further analysis can be deducted using diffusion maps. Diffusion maps can be
summarized as follows:

[quote, pydiffmap, https://pydiffmap.readthedocs.io/en/master/theory.html]
_____
Diffusion maps is a dimension reduction technique that can be used to discover
low dimensional structure in high dimensional data. It assumes that the data
points, which are given as points in a high dimensional metric space, actually
live on a lower dimensional structure. To uncover this structure, diffusion
maps builds a neighborhood graph on the data based on the distances between
nearby points. Then a graph Laplacian L is constructed on the neighborhood
graph. Many variants exist that approximate different differential operators.
_____

link:https://github.com/DiffusionMapsAcademics/pyDiffMap[pydiffmap] is an
excellent Python package that performs the analysis which consists of computing
the eigendecomposition of a sparse neighborhood graph where the Euclidean
metric is used as distance measure.

The important property is that one can prove point-wise convergence of the
discrete eigenvectors of the diffusion maps to the eigenfunctions of the
Fokker-Plank equation and thus to certain differential operators which are
directly connected to the Langevin dynamics that underlies out sampling
methods. In other words, diffusion map fits like a glove.

The eigenvectors parametrize each one dimension of the lower-dimensional
manifold on which the trajectory and therefore the sampled points live.
The degrees of freedom of this manifold are typically called _collective
variables_.

Diffusion maps is a technique to find these collective variables automatically.

NOTE: In the module `TrajectoryAnalyser` an early variant of pydiffmap can be
found. pydiffmap if installed using link:https://pypi.org/project/pydiffmap/[pip]
 is used in +TATiExplorer+ and +TATiAnalyser+ (see <<reference.cmdline>>) if
 the option 'diffusion_map_method' is set to *pydiffmap*.

Let us take a look at the eigenvectors of a very simple trajectory, sampled
at a very low temperature.

[source,python]
---------------
include::python/analyse_diffmap.py[]
---------------

You should then obtain the Figure link:#quickstart.python.analysis.diffusion_map.eigenvectors[Diffusion map analysis].

[[quickstart.python.analysis.diffusion_map.eigenvectors]]
.Diffusion map analysis: Plot of first against second eigenvector.
image::pictures/eigenvectors.png[align="center",{basebackend@docbook:scaledwidth="25%":width=200}]

NOTE: The true first eigenvector is constant and is therefore dropped in the
function `compute_diffusion_maps()`.

[[quickstart.python.exploration]]
Exploring the loss manifold
^^^^^^^^^^^^^^^^^^^^^^^^^^^

Exploration of the loss manifold is a bit more involved and hence uses a
different part of the Python interface. We do not use the simulation interface
anymore but the general Python interface as we require greater access to its
internals.

In general, the procedure has the following stages:

. [[step1]] We sample a start trajectory.
. [[step2]] For the current set of trajectory points we perform a diffusion map
analysis. Using the first eigenvector as the dominant diffusion mode, we
pick the first corner points at its maximal component.
. [[step3]] If more corner points are needed, then we look at the diffusion
distance with respect to already picked corner points over all
eigenvectors of the diffusion map and pick the next point always such
that it maximizes the diffusion distance to the present ones.
. [[step4]] Finally, we sample further trajectories, one starting at each of the
picked corner points.
. [[step5]] This is repeated (go to link:#step2[second step]) for as many
exploration steps as we want to do.

Note that each single trajectory is sampled in a special way:

* First, three legs of sampling are performed
* Then, we analyse the resulting diffusion map.
* If the eigenvalues have not yet converged with respect to some
relative threshold, we continue for one more leg and analyse again after
that
* If they have converged, we stop.
* Finally, we look at the norm of the gradients along the trajectory. If
it is below a certain threshold, then within this section of the
trajectory (with gradient norms beneath the threshold) we pick the
smallest gradient value as the trajectory step being a possible minimum
candidate.
* For all minimum candidates (if any) we run additional optimization
trajectories, e.g. using GradientDescent, to find a local minima.

Have a look at the following example.

[source,python]
---------------
include::python/explore.py[]
---------------

This performs exactly the procedure described before using very, very
short trajectories ('max_steps'), only a few legs ('max_legs') and only
a very limited number of exploration steps ('exploration_steps'). This is
simple for the purpose of illustration. Naturally, larger values for all these
parameters are required in order to explore complex manifolds and eventually
find the global minimum.

[[quickstart.cmdline]]
Using command-line interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

All the tests use the command-line interface and for performing rigorous
scientific experiments, we recommend using this interface as well. Here,
it is to do parameter studies and have extensive runs using different
seeds.

[[quickstart.cmdline.writing_dataset]]
Creating the dataset
^^^^^^^^^^^^^^^^^^^^

As data is read from file, this file needs to be created beforehand.

For a certain set of simple classification problems, namely those that
can be found in the tensorflow playground, we have added a
TATiDatasetWriter` that spills out the dataset in CSV format.

[source,bash]
---------------
include::cmdline/write_dataset.sh[]
---------------

This will write 500 datums of the dataset type 2 ("two clusters") to a
file ``testset-twoclusters.csv'' using all of the points as we have set
the test/train ratio to 0. Note that we also perturb the points by 0.1
relative noise.

[[quickstart.cmdline.parsing_dataset]]
Parsing the dataset
^^^^^^^^^^^^^^^^^^^

Similarly, for testing the dataset can be parsed using the same
tensorflow machinery as is done for sampling and optimizing, using

[source,bash]
---------------
include::cmdline/parse_dataset.sh[]
---------------

where the _seed_ is used for shuffling the dataset.

[[quickstart.cmdline.optimizing]]
Optimizing the network
^^^^^^^^^^^^^^^^^^^^^^

As weights (and biases) are usually uniformly random initialized and the
potential may therefore start with large values, we first have to
optimize the network, using (Stochastic) Gradient Descent (GD).

[source,bash]
---------------
include::cmdline/optimize.sh[]
---------------

This call will parse the dataset from the file
"dataset-twoclusters.csv". It will then perform a (Stochastic) Gradient
Descent optimization in batches of 50 (10% of the dataset) of the
parameters of the network using a step width/learning rate of 0.01 and
do this for 1000 steps after which it stops and writes the resulting
neural network in a TensorFlow-specific format to a set of files, one of
which is called `model.ckpt.meta` (and the other filenames are derived
from this).

We have also created a file `run.csv` which contains among others the
loss at each (``every_nth'', respectively) step of the optimization run.
Plotting the loss over the step column from the run file will result in
a figure similar to in
link:#quickstart.python.optimizing.plot[figure_title].


[NOTE]
====
Since Tensorflow 1.4 an absolute path is required for the storing the
model. In the example we use the current directory returned by the unix
command `pwd`.
====

If you need to compute the optimal step width, which is possible for
smaller networks from the largest eigenvalue of the hessian matrix, then
use the option ``do_hessians 1'' to activate it.

[NOTE]
====
The creation of the nodes is costly, latexmath:[O(N^2)] in the number of
parameters of the network N. Hence, may not work for anything but small
networks and should be done on purpose.
====

In case you have read the quickstart tutorial on the Python interface
before, then the names of the command-line option will probably remind
you of the variables in the FLAGS structure.

[[quickstart.cmdline.sampling]]
Sampling trajectories on the loss manifold
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We continue from this optimized or equilibrated state with sampling. It
is called equilibrated as the network's parameter should now be close to
a (local) minimum of the potential function and hence in equilibrium.
This means that small changes to the parameters will result in gradients
that force it back into the minimum.

Let us call the sampler.

[source,bash]
---------------
include::cmdline/sample.sh[]
---------------

This will cause the sampler to parse the same dataset as before.
Moreover, the sampler will load the neural network from the model, i.e.
using the optimized parameters right from the start. Afterwards it will
use the GLA in 2nd order discetization using again 'step_width' of 0.01
and running for 1000 steps in total. The GLA is a descretized variant of
Langevin Dynamics whose accuracy scales with the inverse square of the
'step_width' (hence, 2nd order).

The seed is needed as we sample using Langevin Dynamics where a noise
term is present. The term basically ascertains a specific temperature
which is proportional to the average momentum of each particle.

After it has finished, it will create three files; a run file
`run.csv` containing run time information such as the step, the
potential, kinetic and total energy at each step, a trajectory file
`trajectory.csv` with each parameter of the neural network at each step,
and an averages file `averages.csv` containing averages accumulated
along the trajectory such as average kinetic energy, average virial (
connected to the kinetic energy through the virial theorem, valid if a
prior keeps parameters bound to finite values), and the average
(ensemble) loss. Moreover, for the HMC sampler the average rejection
rate is stored there. The first two files we need in the next stage.

[[quickstart.cmdline.analysing]]
Analysing trajectories
^^^^^^^^^^^^^^^^^^^^^^

Eventually, we now perform the diffusion map analysis on the obtained
trajectories. The trajectory file written in the last step is simply a
matrix of dimension (number of parameters) times (number of trajectory
steps). The eigenvector to the largest (but one) eigenvalue will give
the dominant direction in which the trajectory is moving.

[NOTE]
====
The largest eigenvalue is usually unity and its eigenvector is constant.
Therefore, it is omitted. That's why indexing for the diffusion maps
eigenvectors starts at 1 (omitted the constant eigenvector 0).
====

The analysis can perform three different tasks:

* Calculating averages.
* Calculating the diffusion map's largest eigenvalues and eigenvectors.
* Calculating landmarks and level sets to obtain an approximation to the
free energy.

[[quickstart.cmdline.analysing.averages]]
Averages
++++++++

Averages are calculated by specifying two options as follows:

[source,bash]
---------------
include::cmdline/analyse_average.sh[]
---------------

This will load both the run file `run.csv` and the trajectory file
`trajectory.csv`and average over them using only every 10th data point
(_every_nth_) and also dropping the first steps below 100
(_drop_burnin_). It will produce then ten averages (_steps_) for each of
energies in the run file and each of the parameters in the trajectories
file (along with the variance) from the first non-dropped step till one
of the ten end steps. These end steps are obtained by equidistantly
splitting up the whole step interval.

Eventually, we have two output file. The averages over the run
information such as total, kinetic, and potential energy in
`average_run.csv`. Also, we have the averages over the degrees of
freedom in `average_trajectories.csv`.

[NOTE]
====
Averages depend crucially on the number of steps we average over. I.e.
the more points we throw away, the less accurate it becomes. In other
words, if large accuracy is required, the averages file (if it contains
the value of interest) is a better place to look for.
====

[[quickstart.cmdline.analysing.diffusion_map]]
Diffusion map
+++++++++++++

The eigenvalues and eigenvectors can be written as well to two output
files.

[source,bash]
---------------
include::cmdline/analyse_diffmap.sh[]
---------------

The files ending in `..values.csv` contains the eigenvalues in two
columns, the first is the eigenvalue index, the second is the
eigenvalue.

The other file ending in `..vectors.csv` is simply a matrix of the
eigenvector components in one direction and the trajectory steps in the
other. Additionally, it contains the parameters at the steps and also
the loss and the kernel matrix entry.

Note that again the all values up till step 100 are dropped and only
every 10th trajectory point is considered afterwards.

There are two methods available. Here, we have used the simpler (and
less accurate) (plain old) vanilla method. The other is called TMDMap.

If you have installed the _pydiffmap_ python package, this mal also be
specified as diffusion map method. It has the benefit of an interal
optimal parameter choice. Hence, it should behave more robustly than the
other two methods. TMDMap is different only in reweighting tre samples
according to the specific temperature.

[[quickstart.cmdline.analysing.free_energy]]
Free energy
+++++++++++

Last but not least, the free energy is calculated.

[source,bash]
---------------
include::cmdline/analyse_free_energy.sh[]
---------------

This will extract landmark points from the trajectory. Basically, the
loss manifold is discretized using these landmarks where all
configurations close to a landmark step are combined onto a so-called
level-set, i.e. all these configurations have a similar loss function
value. By knowing the number of configurations in each level set and
knowing the level sets loss value, an approximation of the free energy
is computed.

This is computed for every step of the trajectory and it is insightful
to look at the free energy over the course of the trajectory represented
by the first eigenvalue. If in this graph clear minima with maxima in
between can be seen, then there are enthalpic barriers between two local
minima. If on the other hand there are flat areas, then we found
entropic barriers.

Both these types of barriers obstruct trajectories and keep the
optimization trapped in so-called meta-stable states. Each type of
barrier requires a different type of remedy to overcome.

[[quickstart.cmdline.exploration]]
Exploring the loss manifold
+++++++++++++++++++++++++++

Eventually, we are not interested in obtaining trajectories on the loss
manifold. Instead we would like to find the global minima. Or at least
have a good idea about whether the minimas we have found so far are
reasonable.

To this end, a command-line tool called `TATiExplorer` is provided. The
idea is to make use of the diffusion map with its diffusion distance to
assess what part of the loss manifold has been explored already.
Moreover, we use multiple trajectories that are spawned from a specific
number of places that are maximally separate with respect to their
diffusion distance. This will ensure that we cover the most ground
possible.

In the end, the eigenvectors obtained through a run using the
`TATiExplorer` will return the dominant diffusion directions and
therefore those pointing in the direction along the minima, i.e. where
the sampling usually gets stuck and remains for a while, hence diffusion
is slow.

[source,bash]
---------------
include::cmdline/exploring.sh[]
---------------

In the example we call the explorer utility in much the same way as we
have called the sampler. There are some additional options that give the
number of eigenvalues to calculate and which diffusion map method to
use. Note that `max_steps` now gives the number of steps of a single
leg. Further down you find what a lag actually is.

Furthermore, there are two options unique to the explorer. This is
`max_legs` which gives the maximum number of legs to look at. Each leg
goes over max_steps. After that a diffusion map analysis is performed
that checks whether the eigenvalues have converged already. If yes, the
trajectory is ended, if not we continue with a new leg (of max_steps
steps). If no convergence should occur, max_legs gives the maximum
number of legs after which the trajectory is terminated regardlessly.

Finally, we run multiple trajectories in parallel from starting points
that are maximally apart from each other in the sense of the diffusion
distances. This is controlled by `number_of_parallel_trajectories`.

[[quickstart.parallelization]]
A note on parallelization
~~~~~~~~~~~~~~~~~~~~~~~~~

Internally, Tensorflow uses a computational graph to represent all
operations. Nodes in the graph represent computations and their results
and edges represent dependencies between these values, i.e. some may act
as input to operations resulting in certain output.

Because of this internal representation Tensorflow has two kind of
parallelisms:

* inter ops
* intra ops

Each is connected to its its own thread pool. Both the command-line and
the Python interface let you pick the number of threads per pool. If 0
is stated (default), then the number of threads is picked automatically.

In general, ``inter_ops_threads''refers to multiple cores performing
matrix multiplication or reduction operations together.
``intra_ops_threads'' seems to be connected to executing multiple nodes
in parallel that are independent of each other but this is guessing at
the moment.

[WARNING]
====
When setting `inter_ops_threads` _unequal_ to 1, then subsequent runs
may produce different results, i.e. results are no longer strictly
reproducible. According to Tensorflow this is because reduction
operations such as `reduce_sum` run non-deterministically on multiple
cores for sake of speed.
====

[[quickstart.conclusion]]
Conclusion
~~~~~~~~~~

This has been the very quick introduction into samping done on neural
network's loss function manifolds. You have to take it from here.
