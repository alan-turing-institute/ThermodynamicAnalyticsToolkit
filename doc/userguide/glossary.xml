<glossary version="4.5" 
  xmlns="http://docbook.org/ns/docbook">
  
  <glossentry xml:id="CCAdL">
    <glossterm>Covariance Controlled Adaptive Langevin
    (<acronym>CCAdL</acronym>)</glossterm>
    <glossdef>
      <para>This is an extension of SGD that uses a thermostat to dissipate
      the extra noise through approximate gradients from the system.</para>
    </glossdef>
  </glossentry>
  
  <glossentry xml:id="GD">
    <glossterm>Gradient Descent
    (<acronym>GD</acronym>)</glossterm>
    <glossdef>
      <para>An iterative, first-order optimization that use the negative 
      gradient times a step width to converge towards the minimum.</para>
    </glossdef>
  </glossentry>
  
  <glossentry xml:id="GLA">
    <glossterm xml:id="GLA.full">Geometric Langevin Algorithm
    (<acronym xml:id="GLA.acronym">GLA</acronym>)</glossterm>
    <glossdef>
      <para>This family of samplers results from a first-order splitting between
      the Hamiltonian and the Ornstein-Uhlenbeck parts. It provides up to 
      second-order accuracy. In the package we have implemented both the 1st
      and 2nd order variant. GLA2nd is among the most accurate samplers,
      especially when it comes to accuracy of momenta. It is surpassed by
      BAOAB, particularly for positions.</para>
    </glossdef>
  </glossentry>
  
  <glossentry xml:id="HMC">
    <glossterm xml:id="HMC.full">Hamiltonian Monte Carlo
    (<acronym xml:id="HMC.acronym">HMC</acronym>)</glossterm>
    <glossdef>
      <para>Instead of Langevin Dynamics this sampler relies on Hamiltonian
      Dynamics. After a specific number of trajectory steps an acceptance
      criterion is evaluated. Afterwards momenta are drawn randomly. Hence,
      here noise comes into play at distinct intervals while for the other 
      samplers noise enters gradually in every step.</para>
    </glossdef>
  </glossentry>
  
  <glossentry xml:id="SGD">
    <glossterm xml:id="SGD.full">Stochastic Gradient Descent
    (<acronym xml:id="SGD.acronym">SGD</acronym>)</glossterm>
    <glossdef>
      <para>A variant of <acronym>GD</acronym> where not the whole dataset
      is used for the gradient computation but only a smaller part. This lightens
      the computational complexity and adds some noise to the iteration as
      gradients are only approximate. However, given redundancy in the dataset
      this noise is often welcome and helps in overcoming barriers in the 
      non-convex minimization problem.</para>
    </glossdef>
    <glossseealso>Gradient Descent</glossseealso>
  </glossentry>
  
  <glossentry xml:id="SGLD">
    <glossterm xml:id="SGLD.full">Stochastic Gradient Langevin Dynamics
    (<acronym xml:id="SGLD.acronym">SGLD</acronym>)</glossterm>
    <glossdef>
      <para>A variant of <acronym>SGD</acronym> where the approximate gradients
      are not only source of noise but an additional noise term is added whose 
      magnitude controls the noise from the gradients.</para>
    </glossdef>
    <glossseealso>Stochastic Gradient Descent</glossseealso>
  </glossentry>
  
</glossary>
