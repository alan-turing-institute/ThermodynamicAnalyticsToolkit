<glossary version="4.5" 
  xmlns="http://docbook.org/ns/docbook">
  
  <glossentry xml:id="GD">
    <glossterm>Gradient Descent
    (<acronym>GD</acronym>)</glossterm>
    <glossdef>
      <para>An iterative, first-order optimization that use the negative 
      gradient times a step width to converge towards the minimum.</para>
    </glossdef>
  </glossentry>
  
  <glossentry xml:id="SGD">
    <glossterm xml:id="SGD.full">Stochastic Gradient Descent
    (<acronym xml:id="SGD.acronym">SGD</acronym>)</glossterm>
    <glossdef>
      <para>A variant of <acronym>GD</acronym> where not the whole dataset
      is used for the gradient computation but only a smaller part. This lightens
      the computational complexity and adds some noise to the iteration as
      gradients are only approximate. However, given redundancy in the dataset
      this noise is often welcome and helps in overcoming barriers in the 
      non-convex minimization problem.</para>
    </glossdef>
    <glossseealso>Gradient Descent</glossseealso>
  </glossentry>
  
  <glossentry xml:id="SGLD">
    <glossterm xml:id="SGLD.full">Stochastic Gradient Langevin Dynamics
    (<acronym xml:id="SGLD.acronym">SGLD</acronym>)</glossterm>
    <glossdef>
      <para>A variant of <acronym>SGD</acronym> where the approximate gradients
      are not only source of noise but an additional noise term is added whose 
      magnitude controls the noise from the gradients.</para>
    </glossdef>
    <glossseealso>Stochastic Gradient Descent</glossseealso>
  </glossentry>
  
</glossary>