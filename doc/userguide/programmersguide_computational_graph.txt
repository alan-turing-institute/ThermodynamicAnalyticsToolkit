////
#
#    ThermodynamicAnalyticsToolkit - explore high-dimensional manifold of neural networks
#    Copyright (C) 2018 The University of Edinburgh
#    The TATi authors, see file AUTHORS, have asserted their moral rights.
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
////

[[concepts.computational_graph]]
Computational Graph
~~~~~~~~~~~~~~~~~~~

Underlying all of tensorflow and typically most of the Machine Learning (ML)
frameworks is the concept of a _computational graph_.

A graph consists of a set of vertices V and a set of edges E. Edges
latexmath:[$e_{ij} \in E$] connect two nodes latexmath:[$v_i, v_j$] and may be
directed.

In the context of a computational graph functions and variables represent the
vertices and directed edges represent dependencies between. In Tensorflow's
documentation and tutorials the vertices are referred to as _nodes_. Hence, we
will use the term nodes in the following, too.

[[concepts.computational_graph.figure]]
.Computational Graph: Sum function node depending on the input of two variable nodes *a* and *b*.
image::./pictures/computational_graph.png[{basebackend@docbook:scaledwidth="25%":width=250}]

Let us have a concrete example and take a look at Figure
link:#concepts.computational_graph.figure[Computational Graph].
There, we have two variable nodes *a* and *b* and one summation node latexmath:[$\Sigma$]
that depends on the two.

Assume we want to evaluate the sum function. The function node can be imagined
as a callback whose parameters are supplied by *a* and *b*. Knowing the
dependencies encoded in the edges of the computational graph, we know how to
execute the callback and evaluate the sum.

NOTE: Graphs are standard concepts in computer science and enjoy a large
variety of algorithms that discover their properties such a shortest paths,
number of connected components, cliques, cycles, and so on. Standard algorithms
such  as Breadth-First Search (BFS) and Depth-First Search (DFS) allow to
explore and enumerate all dependencies. For details, see
link:https://www.springer.com/de/book/9783662536216[Graph Theory, Reinhard Diestel]
and other textbooks.

Naturally, the variables *a* may themselves be functions depending on other
variables. I.e. arbitrary function concatentations are possible. Moreover, even
operations such as assignments are admissible. When the assignment is triggered,
using another node as input, this value is written to the internal state of a
variable node. Finally, nodes can also be combined into groups such that the
execution of the group node triggers the execution of all contained nodes.
This allows to program whole algorithms within the framework of computational
graphs.

NOTE: The graph is usually never completely evaluated. When evaluating a certain
node, then only dependent nodes must be evaluated, too. All other nodes are
ignored.

Think of the computional graph of another way of writing a computer program.
The program consists of many very tiny functions (nodes) and the edges
encode which function relies/calls which other function. Using the program
means executing certain functions that in turn trigger the execution of
many other functions in order to deliver their output value.

This concept has even more advantages:

- _lazy evaluation_, i.e., only computes what is necessary and first when it is
 needed.
- the graph can be analysed for independent or only loosely dependent parts
which therefore can be trivially or at least easily parallelized.
- as nodes consist of tiny functions, their computational complexity can be
estimated depending on the size of their input arguments. This allows to
queue the evaluation of nodes very efficiently.

NOTE: Tensorflow uses the graph analysis to automatically decide whether CPU
or GPU will execute the necessary computations for evaluating a node. Although
this automatic association can be overriden, ther is usually no need to.
This is different in the dynamic graph setting where the user needs to decide.
