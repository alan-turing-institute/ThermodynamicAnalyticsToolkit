////
#
#    ThermodynamicAnalyticsToolkit - explore high-dimensional manifold of neural networks
#    Copyright (C) 2018 The University of Edinburgh
#    The TATi authors, see file AUTHORS, have asserted their moral rights.
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
////

[[glossary]]
[glossary]
Glossary
--------

// please adhere to alphabetical ordering of acronyms!

- [[BAOAB]] *BAOAB*
+
BAOAB is the short-form for the order of the exact solution steps in the
splitting of the Langevin Dynamics SDE: B means momentum update, A is the
position update, and O is the random noise update. It has 2nd order convergence
properties, showing even 4th order super-convergence in the context of
high friction, see <<Leimkuhler2012>>.

- [[CCAdL]] *Covariance Controlled Adaptive Langevin* (CCAdL)
+
This is an extension of <<SGD>> that uses a thermostat to dissipate the
extra noise through approximate gradients from the system.

- [[GD]] *Gradient Descent* (GD)
+
An iterative, first-order optimization that use the negative gradient
times a step width to converge towards the minimum.

- [[GLA]] *Geometric Langevin Algorithm* (GLA)
+
This family of samplers results from a first-order splitting between the
Hamiltonian and the Ornstein-Uhlenbeck parts. It provides up to
second-order accuracy. In the package we have implemented both the 1st
and 2nd order variant. GLA2nd is among the most accurate samplers,
especially when it comes to accuracy of momenta. It is surpassed by
BAOAB, particularly for positions.

- [[HMC]] *Hamiltonian Monte Carlo* (HMC)
+
Instead of Langevin Dynamics this sampler relies on Hamiltonian
Dynamics. After a specific number of trajectory steps an acceptance
criterion is evaluated. Afterwards momenta are drawn randomly. Hence,
here noise comes into play at distinct intervals while for the other
samplers noise enters gradually in every step.

- [[SGD]] *Stochastic Gradient Descent* (SGD)
+
A variant of <<GD>> where not the whole dataset is used for the gradient
computation but only a smaller part. This lightens the computational
complexity and adds some noise to the iteration as gradients are only
approximate. However, given redundancy in the dataset this noise is
often welcome and helps in overcoming barriers in the non-convex
minimization problem.
+
See also <<GD>>.

- [[SGLD]] *Stochastic Gradient Langevin Dynamics* (SGLD)
+
A variant of SGD where the approximate gradients are not only source of
noise but an additional noise term is added whose magnitude controls the
noise from the gradients.
+
See also <<SGD>>.
