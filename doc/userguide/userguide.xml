<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
  <!ENTITY dataset_two_clusters SYSTEM "pictures/dataset_two_clusters.png" NDATA PNG>
  <!ENTITY simple_single_layer_perceptron SYSTEM "pictures/simple_single_layer_perceptron.png" NDATA PNG>
]>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:svg="http://www.w3.org/2000/svg" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:db="http://docbook.org/ns/docbook" version="5.0">
<info>
  <title>DataDrivenSampler</title>
  <subtitle>Manual</subtitle>
  <author>
      <personname>
        <firstname>Frederik</firstname>
        <surname>Heber</surname>
      </personname>
      <email>frederik.heber@gmail.com</email>
      <affiliation>
        <orgname>The University of Edinburgh</orgname>
        <address>School of Mathematics, 5613, JCMB, Peter Guthrie Tait Road, Edinburgh, EH9 3FD </address>
      </affiliation>
  </author>
  <copyright>
    <holder>The University of Edinburgh, all rights reserved</holder>
    <year>2017</year>
  </copyright>
   <cover>
        <para role="tagline">The Official Documentation for DataDrivenSampler</para>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="dataset_two_clusters"/>
          </imageobject>
        </mediaobject>
  </cover>
      <cover>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="dataset_two_clusters"/>
          </imageobject>
        </mediaobject>
        <para>DataDrivenSampler is a sampling-based approach to training 
        neural networks using TensorFlow </para>
        <para><citetitle>DataDrivenSampler: Manual</citetitle> is the…</para>
        <itemizedlist>
          <listitem>
    	      <para>A brief introduction to data-driven sampling in machine learning</para>
          </listitem>
          <listitem>
            <para>A guide to using DataDrivenSampler to perform sampling experiments</para>
          </listitem>
        </itemizedlist>
      </cover>
</info>

  <chapter xml:id="introduction">
    <title xml:id="introduction.title">Introduction</title>
    <section xml:id="introduction.whatis">
      <title xml:id="introduction.whatis.title">What is DataDrivenSampler?</title>
      <para>Typically, optimzation in neural network training employs methods 
      such as Gradient Descent, Stochastic Gradient Descent or derived methods
      using momentum such as ADAM and so on. The loss function itself may be
      convex, however, the loss manifold given a large dataset is not convex 
      in general. Hence, these methods will only find local minima. Therefore, 
      the eventual set of trained parameters of the neural network is not 
      optimal. Nonetheless, Neural networks, given enough data and processing 
      power, work marvelously and one may wonder why.
      </para>
      <para>The essential idea behind this program package is that we use
      sampling instead of optimization: we are not interested in the local 
      minimum closest to some random initial configuration and be done. 
      Instead we aim at finding all of the minima and all possible barriers 
      in between by treating the loss function as a high-dimensional potential
      and the weights and biases of the neural network as particles in a 
      stochastic differential equation, namely Langevin Dynamics.</para>
      <para>There is not need to panic: You do not need to know anything about 
      these kinds of equations when using the program. However, rest assured 
      that all statistical properties derived using trajectories obtained 
      through these equations are meaningful.</para>
      <para>The hope is to elucidate the marvel behind the wondrous 
      performance of neural networks, maybe to obtain even better 
      parametrizations or obtain the same in cheaper ways, and also to gather 
      means of optimizing the neural network's topology given a specific 
      dataset to train.</para>
      <para>In essence, this program suite provides samplers using 
      <link xlink:href="https://www.tensorflow.org/">TensorFlow</link> and
      analysis tools to extract specific statistical quantities from the 
      computed particle trajectories.</para>
      <para>It can be used both as python module and as stand-alone 
      command-line tools.</para>
    </section>
    <section xml:id="introduction.installation">
      <title xml:id="introduction.installation.title">Installation</title>
      <para>In the following we explain the installation procedure to get
      DataDrivenSampler up and running.</para>
      <section xml:id="introduction.installation.requirements">
        <title xml:id="introduction.installation.requirements.title">Installation requirements</title>
        <para>This program suite is implemented using python3 and the development
        mainly focused on Linux (development machine uses Ubuntu 16.04). At the
        moment other operation systems are not supported but may still work.</para>
        <para>It has the following non-trivial dependencies:</para>
        <itemizedlist>
          <listitem>TensorFlow: see below<uri>www.tensorflow.org</uri></listitem>
          <listitem>Numpy: see <uri>www.numpy.org</uri></listitem>
          <listitem>Pandas: see <uri>pandas.pydata.org</uri></listitem>
          <listitem>sklearn: see <uri>scikit-learn.org</uri></listitem>
        </itemizedlist>
        Note that most of these packages can be easily installed using either
        the repository tool (using some linux derivate such as Ubuntu), e.g.
        <programlisting>sudo apt install python3-numpy</programlisting>or via
        <command>pip3</command>, i.e.
        <programlisting>pip3 install numpy</programlisting>
        <para>In our setting what typically worked best was to use 
        <productname>anaconda</productname> in the following manner:</para>
        <programlisting>conda create -n tensorflow python=3.5 -y
  conda install -n tensorflow -y \
     tensorflow numpy scipy pandas scikit-learn
        </programlisting>
        <para>In case your machine has GPU hardware for tensorflow, replace
        <quote>tensorflow</quote> by <quote>tensorflow-gpu</quote>.</para>
        <note>Note that on systems with typical core i7 architecture 
        recompiling tensorflow from source provided only very small runtime
        gains in our tests which in most cases do not support the extra effort.
        However, by default the provided package did not seem to support
        multi-threading. Hence, in this scenario compiling from source is 
        required.</note>
        <para>Henceforth, we assume that there is a working tensorflow on 
        your system, i.e. inside the python3 shell</para>
        <programlisting>import tensorflow as tf</programlisting>
        <para>does <emphasis>not</emphasis> throw an error.</para>
        <para>Moreover,</para>
        <programlisting>a=tf.constant("Hello world")
  sess=tf.Session()
  sess.run(a)</programlisting>
        <para>should print <quote>Hello world</quote> or something similar.
        </para>
      </section>
      <section xml:id="introduction.installation.procedure">
        <title xml:id="introduction.installation.procedure.title">Installation procedure</title>
        <para>This package is distributed via autotools, "compiled" and installed 
        via automake. If you are familiar with this set of tools, there should be
        no problem. If not, please refer to the text INSTALL file that is included
        in this distributable archive. The installation, very briefly, goes like this:</para>
        <programlisting>
./bootstrap.sh
mkdir build64
cd build64
../configure --prefix="somepath" -C PYTHON="path to python3"
        make
        make install
        </programlisting>
        <para>where the first step is only required if you have obtained the 
        package through cloning the github repository. If you extracted it from
        a distributable tarball, then this is not necssary.</para>
        <para>Here, "compilation" is done in an extra folder 
        <quote>build64</quote>, i.e. it is an out-of-source build, that 
        prevents cluttering of the source folder. Naturally, you may pick any
        name (and actually any location on your computer) as you see fit.
        </para>
        <para>More importantly, please replace <quote>somepath</quote> and 
        <quote>path to python3 </quote> by the desired installation path and 
        the full path to the <command>python3</command> executable on your 
        system.</para>
        <note>In case of having used <productname>anaconda</productname>
        for the installation of required packages, then you need to look in
        <programlisting>$HOME/.conda/envs/tensorflow/bin/python3</programlisting>
        for the respective command, where <quote>$HOME</quote> is your home
        folder. This assumes that your anaconda environment is named
        <quote>tensorflow</quote> as in the example installation steps above.
        </note>
        <note>We recommend executing
          <programlisting>make check</programlisting>
          additionally. This will execute every test on the extensive testsuite
          and report any errors. None should fail. If all fail, a possible cause 
          might be a not working tensorflow installation. If some fail, please
          contact the author, see <xref linkend="introduction.feedback"/>.
          As always with GNU make you may use <command>make -j4 check</command>
          to execute four processes in parallel performing the checks which
          should give a significant speed up.
        </note>
      </section>
    </section>
    <section xml:id="introduction.license">
      <title xml:id="introduction.license.title">License</title>
      <para>As long as no other license statement is given, DataDrivenSampler is
      free for user under the GNU Public License (GPL) Version 3 (see
      <uri>www.gnu.de/documents/gpl-3.0.de.html</uri>).</para>
    </section>
    <section xml:id="introduction.disclaimer">
      <title xml:id="introduction.disclaimer.title">Disclaimer</title>
      <para>We quote section 11 from the GPLv3 license:</para>
      <remark>Because the program is licensed free of charge, there is
not warranty for the program, to the extent permitted by applicable law.
Except when otherwise stated in writing in the copyright holders and/or
other parties provide the program &quot;as is&quot; without warranty of
any kind, either expressed or implied. Including, but not limited to,
the implied warranties of merchantability and fitness for a particular
purpose. The entire risk as to the quality and performance of the
program is with you. Should the program prove defective, you assume the
cost of all necessary servicing, repair, or correction.</remark>
    </section>
    <section xml:id="introduction.feedback">
      <title xml:id="introduction.feedback.title">Feedback</title>
      <para>If you encounter any bugs, errors, or would like to submit
      feature request, please use the email address provided at the very
      beginning of this user guide. The author is especially thankful for
      any description of all related events prior to occurrence of the
      error, saved &quot;session scripts&quot; (see below) and auxiliary files.
      Please mind sensible space restrictions of email attachments.</para>
    </section>
  </chapter>
  
  <chapter xml.id="quickstart">
    <title  xml.id="quickstart.title">Quickstart</title>
    <section xml.id="quickstart.introduction">
      <title  xml.id="quickstart.introduction.title">Sampling in neural networks</title>
      <para>Assume we are given a very simple data set as depicted in 
      <xref xrefstyle="template:Figure %n" linkend="quickstart.introduction.dataset"/>. 
      <figure xml:id="quickstart.introduction.dataset">
        <title>Dataset: two gaussian distributed point clouds</title>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="dataset_two_clusters"/>
          </imageobject>
        </mediaobject>
      </figure>
      The goal is to classify all red and blue dots into two different classes. This
      problem  is quite simple to solve: a line in the two-dimensional space can 
      easily separate the two classes.</para>
      <para>A very simple neural network, a perceptron, is all we need:  it uses 
      two inputs nodes, namely each coordinate component, 
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>x</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>
      and
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>x</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>,
      and a single output 
      node with an activation function
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
              <mml:mi>f</mml:mi>
          </mml:mrow>
        </mml:math>
      </inlineequation>
      whose sign gives the class the input item belongs to. The network is 
      given in 
      <xref xrefstyle="template:Figure %n" linkend="quickstart.introduction.perceptron"/>.
      </para>
    <figure xml:id="quickstart.introduction.perceptron">
        <title>Simple single-layer perceptron with weights and biases</title>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="simple_single_layer_perceptron"/>
          </imageobject>
        </mediaobject>
      </figure>
      <para>In the following we want to use the mean square loss, i.e. the 
      euclidian distance between the output from the network and the expected
      values per item, as the network's loss function. The loss depends 
      implicitly on the dataset and explicitly on the weights and biases 
      associated with the network. In our case, we have two weights for the two
      edges between input nodes, 
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>w</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>
      and
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>w</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>,
      and the output node and a single bias attached
      to the output node
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
              <mml:mi>b</mml:mi>
          </mml:mrow>
        </mml:math>
      </inlineequation>.
      </para>
      <para>In sampling we look at a system of particles that have two internal
      properties: location and momentum. The location is simply their current 
      value, that changes through its momentum over time. The momentum again 
      changes because the particle is inside a potential that drives it towards
      the minimum. The system is described by a so-called Hamilton operator
      that gives rise to its Dynamics. If noise is additionally taken into 
      account, then we look at Langevin Dynamics.</para>
      <para>Returning to the neural networks, the role of the particles is taken
      by the degrees of freedom of the system, weights and biases. The loss
      function is called the <emphasis>potential</emphasis> and it is accompanied 
      by a <emphasis>kinetic energy</emphasis> that is simply the sum of all
      squared momenta. Adding Momentum to Optimizers in neural networks is a
      concept known already and inspired by physics. The momentum helps in
      overcoming areas of the loss function where it is essentially flat.</para>
      <para>Sampling produces trajectories of particles moving along the
      manifold. Integrals along these trajectories, if they are long enough, 
      are equivalent to integrating over the whole manifold, the system is
      ergodic.</para>
      <para>By using sampling we mean to discover more of the loss manifold
      than just the closest local minimum. To this end, barriers need to be
      overcome of which there two kind,</para>
      <itemizedlist>
        <listitem>entropic barriers,</listitem>
        <listitem>enthalpic barriers.</listitem>
      </itemizedlist>
      <para>Both of which are conceptually very simple. The enthalpic barrier
      is simply a ridge that is very high where the particles need a large 
      momentum to overcome it. Entropic barriers on the other hand are passages
      very small in volume that are simply very difficult to find. In order to
      overcome barriers of the first kind, higher temperatures suffice.
      For the second type of barrer, this is not so easy.</para>
      <para>This quick description of the problem of sampling in the context
      of neural networks in data science should have prepare you now for the
      following quickstart tutorial on how to actually use DataDrivenSampler 
      to perform sampling.</para>
    </section>
    <section xml:id="quickstart.python">
      <title  xml:id="quickstart.python.title">Using Python</title>
      <para>
      The package can be readily used inside any python3 shell or script.
      However, this interface rather lends itself to quick testing than rigorous
      experiments. You are still fine if you perform all your scientific 
      experiments inside python scripts kept safely inside a code versioning 
      system such as <command>git</command>.</para>
      <para>
      If you have installed the package in the folder "/foo", i.e. we have a 
      folder "DataDrivenSampler" with a file 
      <command>DataDrivenSampler.py</command> residing in there, then you 
      probably need to add it to the 
      <command>PYTHONPATH</command> as follows</para>
      <programlisting>PYTHONPATH=/foo python3</programlisting>
      <para>In this shell, you may import the sampling part of the package as 
      follows 
      <programlisting>from DataDrivenSampler.models.model import model</programlisting>
      This will import the abstract <command>model</command> class from the
      file mentioned before. This class contains wrapper functions to setup 
      the network with either training and sampling in a few keystrokes.</para>
      <section xml:id="quickstart.python.optimizing">
        <title  xml:id="quickstart.python.optimizing.title">Optimizing the network</title>
        <para>Let us first start with optimizing the network.</para>
        <example xml:id="quickstart.python.optimizing.example">
          <title>Optimizing network for two clusters dataset</title>
          <programlisting><xi:include  href="python/optimize.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>
        As you see, all options are set in a struct called <command>FLAGS</command> 
        that control how the optimization is performed. 
        There is a helper function in <command>model</command> called 
        <command>create_mock_flags</command>
        that creates the FLAGS for you with some default parameters. </para>
        <para>Let us quickly go through each of the parameters: 
        <itemizedlist>
          <listitem>
          <emphasis>data_type</emphasis> is a number between 0 and 3 that defines
          one of four hard-coded datasets. See the classification problems at
          <uri>playground.tensorflow.org</uri> to have an idea.
          </listitem><listitem>
          <emphasis>dimension</emphasis> defines the number of data points.
          </listitem><listitem>
          <emphasis>batch_size</emphasis> sets the subset size of the data set
          looked at per training step, if smaller than dimension, then we add
          stochasticity/noise to the training but for the advantage of smaller
          runtime.
          </listitem><listitem>
          <emphasis>max_steps</emphasis> gives the amount of training steps to
          be performed.
          </listitem><listitem>
          <emphasis>noise</emphasis> gives the relative perturbation for each
          of the dataset items. This is specific to the type of dataset.
          </listitem><listitem>
          <emphasis>optimizer</emphasis> defines the method to use for training.
          Here, we use Gradient Descent (in case batch_size is smaller than 
          dimension, then we actually have Stochastic Gradient Descent).
          </listitem><listitem>
          <emphasis>output_activation</emphasis> defines the activation function
          of all output nodes, here it is linear. Other choices are: tanh, relu, relu6.
          </listitem><listitem>
          <emphasis>seed</emphasis> sets the seed of the random number generator.
          We will still have full randomness but in a deterministic manner, i.e. 
          calling the same procedure again will bring up the exactly same values.
          </listitem><listitem>
          <emphasis>step_width</emphasis> defines the scaling of the gradients
          in each training step, i.e. the learning rate. Values too large may miss the
          minimum, values too small need longer to reach it.
          </listitem>
        </itemizedlist>
         </para>
         <para>We did not say anything about <emphasis>sampler</emphasis>
         as this is convered in the next section.</para>
        <para>Afterwards, the network is initialized, then we call <command>train()</command>
        which performs the training and returns runtime info and trajectory
        as a pandas DataFrame.</para>
        <para>At the end of this section on training, let us have a quick 
        glance at the decrease of the loss function over the steps by using
        <command>matplotlib</command>.</para>
        <example xml:id="quickstart.python.optimizing.example2">
          <title>Plotting the loss and other properties over steps</title>
          <programlisting><xi:include  href="python/plot_optimize.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
      <para>Go and have a look athe other columns. Or try to visualize the 
      change in the parameters (weights and biases) in the trajectories
      dataframe.</para>
      </section>
      <section xml:id="quickstart.python.sampling">
        <title  xml:id="quickstart.python.sampling.title">Sampling the network</title>
        <para>After optimization we may continue sampling the network.</para>
        <example xml:id="quickstart.python.sampling.example">
          <title>Plotting the loss and other properties over steps</title>
          <programlisting><xi:include  href="python/sample.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>Here, the <emphasis>sampler</emphasis> setting mentioned before
        becomes important as it states which sampling scheme to use. At the
        moment the following are available: sgld, 
        GeometricLangevinAlgorithm_1st, GeometricLangevinAlgorithm_2nd, where
        the firstrtefers to StochasticGradientLangevinDynamics
         [Welling, Teh, 2011]. GLA 2nd is currently recommended to use.</para>
        <para>We might also concatenate optimize and sample if in between the
        two we adjust FLAGS as follows:
        <programlisting>
...
FLAGS.max_steps = 10000
FLAGS.sampler = "GeometricLangevinAlgorithm_2ndOrder"
nn.reset_flags(FLAGS)
nn.init_network(None, setup="sample")
...
        </programlisting>
        The only thing we change in FLAGS is the number of steps and adding the
        sampler. However, as the model simply stores a copy of the FLAGS, in 
        order to update the FLAGS in the model class as well, we need to reset 
        it. Afterwards, we again  initialize the network which will add only the 
        sampling nodes and prepare output files differently.
        We have skipped the following steps that are equivalent to
        <xref xrefstyle="template:Example %n" linkend="quickstart.python.sampling.example"/>.
        Again, at the very end we obtain pandas DataFrame containing runtime 
        information and trajectory.
        </para>
      </section>
      <section xml:id="quickstart.python.analysis">
        <title  xml:id="quickstart.python.analysis.title">Analysing trajectories</title>
        <para>Analysis is so for constrained to parsing in run and trajectory
        files that you would write through optimization and sampling runs.</para>
        <para>To this end, specify <command>FLAGS.run_file</command> and
        <command>FLAGS.trajectory_file</command> with some valid file names.</para>
        <para>Subsequently, these may be easily parsed as follows, see also
         "DDSAnalyser.in".</para>
        <example xml:id="quickstart.python.analysis.example">
          <title>Plotting the averages of the parameters over steps</title>
          <programlisting><xi:include  href="python/sample.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <programlisting>
        </programlisting>
        <para>This would give a plot of the running average for each parameter
        in the trajectory file. In a similar, the run file can be loaded and its
        average quantities such as loss or kinetic energy be analysed and 
        plotted.</para>
      </section>
    </section>
    <section xml:id="quickstart.cmdline">
      <title  xml:id="quickstart.cmdline.title">Using command-line interface</title>
      All the tests use the command-line interface and for performing rigorous
      scientific experiments, we recommend using this interface as well. Here,
      it is to do parameter studies and have extensive runs using different
      seeds.
      <section xml:id="quickstart.cmdline.optimizing">
        <title  xml:id="quickstart.cmdline.optimizing.title">Optimizing the network</title>
        <para>As weights (and biases) are usually uniformly random initialized and
        the potential may therefore start with large values, we first have to 
        optimize the network, using (Stochastic) Gradient Descent (GD).</para>
        <programlisting>./DDSOptimizer \
    --batch_size 50 \
    --data_type 2 \
    --dimension 500 \
    --loss mean_squared \
    --max_steps 1000 \
    --noise 0.1 \
    --optimizer GradientDescent \
    --save_model model.ckpt.meta \
    --seed 426 \
    --step_width 1e-2
        </programlisting>
        <para>This call will create an internal dataset called "two clusters" with 500 
        points using random number seed 426 (to generate the dataset in a 
        deterministic fashion) using 10% of noise. 
        It will then perform a (Stochastic) Gradient  Descent optimization of the 
        parameters of the network using a step width/learning rate of 0.01 and
        do this for 1000 steps after which it stops and writes the resulting neural
        network in a TensorFlow-specific format to a set of files, one of which
        is called <command>model.cpkt.meta</command> (and the other filenames
        are derived from this).</para>
        <para>In case you have read the quickstart tutorial on the Python
        interface before, then the names of the command-line option will
        probably remind you of the variables in the FLAGS structure.</para>
      </section>  
      <section xml:id="quickstart.cmdline.sampling">
        <title  xml:id="quickstart.cmdline.sampling.title">Sampling trajectories on the loss manifold</title>
        <para>We continue from this optimized or equilibrated state with sampling.
        It is called equilibrated as the network's parameter should now be close to
        a (local) minimum of the potential function and hence in equilibrium. This
        means that small changes to the parameters will result in gradients that 
        force it back into the minimum.</para>
        <para>Let us call the sampler.</para>
        <programlisting>./DDSampler \
    --batch_size 50 \
    --data_type 2 \
    --dimension 500 \
    --friction_constant 10 \
    --inverse_temperature 10 \
    --loss mean_squared \
    --max_steps 100000 \
    --noise 0.1 \
    --sampler GeometricLangevinAlgorithm_2ndOrder \
    --restore_model model.ckpt \
    --run_file run.csv \
    --seed 426 \
    --step_width 1e-2 \
    --trajectory_file trajectory.csv
        </programlisting>
        <para>This will cause the sampler to prepare the same dataset as before, as
        we still are using the same seed, data_type, dimension, and noise.
        Moreover, the sampler will load the neural network from the model, i.e.
        using the optimized parameters right from the start.
        Afterwards it will use the Geometric Langevin Algorithm (GLA) in 2nd order
        discetization using again step_width of 0.01 and running for 100,000 steps
        in total. The GLA is a descretized variant of Langevin Dynamics whose 
        accuracy scales with the inverse square of the step_width (hence, 2nd 
        order).</para>
        <para>After it is finished, it will create two files; a run file 
        <command>run.csv</command>containing run time information such as the 
        step, the potential, kinetic and total energy at each step, and a 
        trajectory file <command>trajectory.csv</command>with each parameter 
        of the neural network at each step. These two files we need in the 
        next stage.</para>
      </section>
      <section xml:id="quickstart.cmdline.analysing">
        <title  xml:id="quickstart.cmdline.analysing.title">Analysing trajectories</title>
        <para>Eventually, we now perform the diffusion map analysis on the obtained
        trajectories. The trajectory file written in the last step is simply a matrix
        of dimension (number of parameters) times (number of trajectory steps).
        The eigenvector to the largest (but one) eigenvalue will give the dominant 
        direction in which the trajectory is moving.</para>
        <note>The largest eigenvalue is usually unity and its eigenvector is 
        constant.</note>
        <para>The analysis can perform three different tasks:</para>
        <itemizedlist>
          <listitem>Calculating averages.</listitem>
          <listitem>Calculating the diffusion map's largest  eigenvalues and 
          eigenvectors.</listitem>
          <listitem>Calculating landmarks and level sets to obtain an approximation
          to the free energy.</listitem>
        </itemizedlist>
        <section xml:id="quickstart.cmdline.analysing.averages">
          <title  xml:id="quickstart.cmdline.analysing.averages.title">Averages</title>
          <para>Averages are calculated by specifying two options as follows:</para>
          <programlisting>./DDSAnalyser \
    --average_run_file average_run.csv \
    --average_trajectory_file average_trajectory.csv \
    --drop_burnin 100 \
    --every_nth 10 \
    --run_file run.csv \
    --steps 10 \
    --trajectory_file trajectory.csv
          </programlisting>
        <para>This will load both the run file <command>run.csv</command>
        and the trajectory file <command>trajectory.csv</command>and average
         over them using only every 10th data point 
         (<emphasis>every_nth</emphasis>) and also dropping the first steps 
         below 100 (<emphasis>drop_burnin</emphasis>). It will produce then
         ten averages (<emphasis>steps</emphasis>) for each of energies in the 
         run file and each of the parameters in the trajectories file (along 
         with the variance) from the first non-dropped step till one of the 
         ten end steps. These end steps are obtained by equidistantly 
         splitting up the whole step interval.</para>
         <para>Eventually, we have two output file. The averages over the run
         information such as total, kinetic, and potential energy in
         <command>average_run.csv</command>. Also, we have the averages
         over the degrees of freedom in 
         <command>average_trajectories.csv</command>.</para>
        </section>
        <section  xml:id="quickstart.cmdline.analysing.diffusion_map">
          <title  xml:id="quickstart.cmdline.analysing.diffusion_map.title">Diffusion map</title>
          <para>The eigenvalues and eigenvectors can be written as well to two 
          output files.</para>
            <programlisting>./DDSAnalyser \
    --diffusion_map_file diffusion_map_values.csv \
    --diffusion_mapmethod vanilla \
    --diffusion_mtrixp_file diffusion_map_vectors.csv \
    --drop_burnin 100 \
    --every_nth 10 \
    --steps 10 \
    --trajectory_file trajectory.csv
            </programlisting>
          <para>The files ending in <command>..values.csv</command> contains 
          the eigenvalues in two columns, the first is the eigenvalue index,
          the second is the eigenvalue.</para>
          <para>The other file ending in <command>..vectors.csv</command> is 
          simply a matrix of the eigenvector components in one direction and 
          the trajectory steps in the other. Additionally, it contains the 
          parameters at the steps and also the loss and the kernel matrix 
          entry.</para>
          <para>Note that again the all values up till step 100 are dropped 
          and only every 10th trajectory point is considered afterwards.</para>
          <para>There are two methods available. Here, we have used the
          simpler (and less accurate) (plain old) vanilla method. The other is
          called TMDMap.</para>
        </section>
        <section xml:id="quickstart.cmdline.analysing.free_energy">
          <title  xml:id="quickstart.cmdline.analysing.free_energy.title">Free energy</title>
          <para>Last but not least, the free energy is calculated.</para>
            <programlisting>./DDSAnalyser \
    --diffusion_mapmethod TMDMap \
    --drop_burnin 100 \
    --every_nth 10 \
    --inverse_temperature 10 \
    --landmarks 5 \
    --landmark_file landmarks-ev_1.csv \
    --number_of_eigenvectors 2 \
    --steps 10 \
    --trajectory_file trajectory.csv
            </programlisting>
          <para>This will extract landmark points from the trajectory. 
          Basically, the loss manifold is discretized using these landmarks
          where all configurations close to a landmark step are combined
          onto a so-called level-set, i.e. all these configurations have a
          similar loss function value. By knowing the number of configurations
          in each level set and knowing the level sets loss value, an 
          approximation of the free energy is computed.</para>
          <para>This is computed for every step of the trajectory and it is
          insightful to look at the free energy over the course of the
          trajectory represented by the first eigenvalue. If in this graph
          clear minima with maxima in between can be seen, then there
          are enthalpic barriers between two local minima. If on the other
          hand there are flat areas, then we found entropic barriers.</para>
          <para>Both these types of barriers obstruct trajectories and keep
          the optimization trapped in so-called meta-stable states. Each type
          of barrier requires a different type of remedy to overcome.</para>
        </section>
      </section>
    </section>
    <section xml:id="quickstart.conclusion">
      <title  xml:id="quickstart.conclusion.title">Conclusion</title>
      <para>This has been the very quick introduction into samping done
      on neural network's loss function manifolds. You have to take it
      from here.</para>
    </section>
  </chapter>
  <chapter xml:id="reference">
    <title xml:id="reference.title">The reference</title>
    <section xml:id="reference.concepts">
    <title xml:id="reference.concepts.title">General concepts</title>
    <para>Before we dive into the internals of this program suite, let us
    first introduce some general underlying concepts assuming that the
    reader is only roughly familiar with them. This is not meant as a 
    replacement for the study of more in-depth material but should rather
    be seen as a reminder of the terms and notation that will appear later 
    on.</para>
  </section>
  <section xml:id="reference.neural_networks">
    <title xml:id="reference.neural_networks.title">Neural Networks</title>
    <para>A neural network (NN) is a tool used in the context of machine 
    learning.
    Formally, it is a graph with nodes and edges, where nodes represent
    (simple) functions. The edges represent scalar values by which the
    output of one node is scaled as input to another node.  The scalar value
    is called <emphasis>weight</emphasis> and each node also has a
    constant value, the <emphasis>bias</emphasis>, that does not depend
    on the input of other nodes.
    Nodes are organised in layers and nodes are (mostly) only connected 
    between adjacent layer. 
    Special are the very first layer with input nodes that simply accept 
    input from the user and the very last layer whose output is eventually 
    all that matters.</para>
    <para>Typically, a NN might be used for the task of classification:
    Data is fed into the network's input layer and its output layer has nodes
    equal to the number of classes to be distinguished. This can for example
    be used for image classification.</para>
    <para>The essential task at hand is to determine a good set of 
    parameters, i.e. values for the weights and biases, such that the task
    is performed best with respect to some measure.</para>
    </section>
    <section xml:id="reference.loss">
    <title xml:id="reference.loss.title">The loss function</title>
    <para>still empty</para>
  </section>
  <section xml:id="reference.exploring">
    <title xml:id="reference.exploring.title">Exploring the manifold</title>
    <para>still empty</para>
  </section>
</chapter>
  <chapter>
    <title>Acknowledgements</title>
    <para>Thanks to all users of the code!</para>
  </chapter>
</book>
