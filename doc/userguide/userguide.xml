<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [
  <!ENTITY dataset_two_clusters SYSTEM "pictures/dataset_two_clusters.png" NDATA PNG>
  <!ENTITY simple_single_layer_perceptron SYSTEM "pictures/simple_single_layer_perceptron.png" NDATA PNG>
]>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:svg="http://www.w3.org/2000/svg" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:html="http://www.w3.org/1999/xhtml" xmlns:db="http://docbook.org/ns/docbook" version="5.0">
<info>
  <title>DataDrivenSampler</title>
  <subtitle>Manual</subtitle>
  <author>
      <personname>
        <firstname>Frederik</firstname>
        <surname>Heber</surname>
      </personname>
      <email>frederik.heber@gmail.com</email>
      <affiliation>
        <orgname>The University of Edinburgh</orgname>
        <address>School of Mathematics, 5613, JCMB, Peter Guthrie Tait Road, Edinburgh, EH9 3FD </address>
      </affiliation>
  </author>
  <copyright>
    <holder>The University of Edinburgh, all rights reserved</holder>
    <year>2017</year>
  </copyright>
   <cover>
        <para role="tagline">The Official Documentation for DataDrivenSampler</para>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="dataset_two_clusters"/>
          </imageobject>
        </mediaobject>
  </cover>
      <cover>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="dataset_two_clusters"/>
          </imageobject>
        </mediaobject>
        <para>DataDrivenSampler is a sampling-based approach to training 
        neural networks using TensorFlow </para>
        <para><citetitle>DataDrivenSampler: Manual</citetitle> is the…</para>
        <itemizedlist>
          <listitem>
    	      <para>A brief introduction to data-driven sampling in machine learning</para>
          </listitem>
          <listitem>
            <para>A guide to using DataDrivenSampler to perform sampling experiments</para>
          </listitem>
        </itemizedlist>
      </cover>
</info>

  <chapter xml:id="introduction">
    <title xml:id="introduction.title">Introduction</title>
    <section xml:id="introduction.whatis">
      <title xml:id="introduction.whatis.title">What is DataDrivenSampler?</title>
      <para>Typically, optimzation in neural network training employs methods 
      such as Gradient Descent, Stochastic Gradient Descent or derived methods
      using momentum such as ADAM and so on. The loss function itself may be
      convex, however, the loss manifold given a large dataset is not convex 
      in general. Hence, these methods will only find local minima. Therefore, 
      the eventual set of trained parameters of the neural network is not 
      optimal. Nonetheless, Neural networks, given enough data and processing 
      power, work marvelously and one may wonder why.
      </para>
      <para>The essential idea behind this program package is that we use
      sampling instead of optimization: we are not interested in the local 
      minimum closest to some random initial configuration and be done. 
      Instead we aim at finding all of the minima and all possible barriers 
      in between by treating the loss function as a high-dimensional potential
      and the weights and biases of the neural network as particles in a 
      stochastic differential equation, namely Langevin Dynamics.</para>
      <para>There is not need to panic: You do not need to know anything about 
      these kinds of equations when using the program. However, rest assured 
      that all statistical properties derived using trajectories obtained 
      through these equations are meaningful.</para>
      <para>The hope is to elucidate the marvel behind the wondrous 
      performance of neural networks, maybe to obtain even better 
      parametrizations or obtain the same in cheaper ways, and also to gather 
      means of optimizing the neural network's topology given a specific 
      dataset to train.</para>
      <para>In essence, this program suite provides samplers using 
      <link xlink:href="https://www.tensorflow.org/">TensorFlow</link> and
      analysis tools to extract specific statistical quantities from the 
      computed particle trajectories.</para>
      <para>It can be used both as python module and as stand-alone 
      command-line tools.</para>
    </section>
    <section xml:id="introduction.installation">
      <title xml:id="introduction.installation.title">Installation</title>
      <para>In the following we explain the installation procedure to get
      DataDrivenSampler up and running.</para>
      <section xml:id="introduction.installation.requirements">
        <title xml:id="introduction.installation.requirements.title">Installation requirements</title>
        <para>This program suite is implemented using python3 and the development
        mainly focused on Linux (development machine uses Ubuntu 16.04). At the
        moment other operation systems are not supported but may still work.</para>
        <para>It has the following non-trivial dependencies:</para>
        <itemizedlist>
          <listitem>TensorFlow: see below<uri>www.tensorflow.org</uri></listitem>
          <listitem>Numpy: see <uri>www.numpy.org</uri></listitem>
          <listitem>Pandas: see <uri>pandas.pydata.org</uri></listitem>
          <listitem>sklearn: see <uri>scikit-learn.org</uri></listitem>
        </itemizedlist>
        Note that most of these packages can be easily installed using either
        the repository tool (using some linux derivate such as Ubuntu), e.g.
        <programlisting>sudo apt install python3-numpy</programlisting>or via
        <command>pip3</command>, i.e.
        <programlisting>pip3 install numpy</programlisting>
        <para>In our setting what typically worked best was to use 
        <productname>anaconda</productname> in the following manner:</para>
        <programlisting>conda create -n tensorflow python=3.5 -y
  conda install -n tensorflow -y \
     tensorflow numpy scipy pandas scikit-learn
        </programlisting>
        <para>In case your machine has GPU hardware for tensorflow, replace
        <quote>tensorflow</quote> by <quote>tensorflow-gpu</quote>.</para>
        <note>Note that on systems with typical core i7 architecture 
        recompiling tensorflow from source provided only very small runtime
        gains in our tests which in most cases do not support the extra effort.
        However, by default the provided package did not seem to support
        multi-threading. Hence, in this scenario compiling from source is 
        required.</note>
        <para>Henceforth, we assume that there is a working tensorflow on 
        your system, i.e. inside the python3 shell</para>
        <programlisting>import tensorflow as tf</programlisting>
        <para>does <emphasis>not</emphasis> throw an error.</para>
        <para>Moreover,</para>
        <programlisting>a=tf.constant("Hello world")
  sess=tf.Session()
  sess.run(a)</programlisting>
        <para>should print <quote>Hello world</quote> or something similar.
        </para>
      </section>
      <section xml:id="introduction.installation.procedure">
        <title xml:id="introduction.installation.procedure.title">Installation procedure</title>
        <para>This package is distributed via autotools, "compiled" and installed 
        via automake. If you are familiar with this set of tools, there should be
        no problem. If not, please refer to the text INSTALL file that is included
        in this distributable archive. The installation, very briefly, goes like this:</para>
        <programlisting>
./bootstrap.sh
mkdir build64
cd build64
../configure --prefix="somepath" -C PYTHON="path to python3"
        make
        make install
        </programlisting>
        <para>where the first step is only required if you have obtained the 
        package through cloning the github repository. If you extracted it from
        a distributable tarball, then this is not necssary.</para>
        <para>Here, "compilation" is done in an extra folder 
        <quote>build64</quote>, i.e. it is an out-of-source build, that 
        prevents cluttering of the source folder. Naturally, you may pick any
        name (and actually any location on your computer) as you see fit.
        </para>
        <para>More importantly, please replace <quote>somepath</quote> and 
        <quote>path to python3 </quote> by the desired installation path and 
        the full path to the <command>python3</command> executable on your 
        system.</para>
        <note>In case of having used <productname>anaconda</productname>
        for the installation of required packages, then you need to look in
        <programlisting>$HOME/.conda/envs/tensorflow/bin/python3</programlisting>
        for the respective command, where <quote>$HOME</quote> is your home
        folder. This assumes that your anaconda environment is named
        <quote>tensorflow</quote> as in the example installation steps above.
        </note>
        <note>We recommend executing
          <programlisting>make check</programlisting>
          additionally. This will execute every test on the extensive testsuite
          and report any errors. None should fail. If all fail, a possible cause 
          might be a not working tensorflow installation. If some fail, please
          contact the author, see <xref linkend="introduction.feedback"/>.
          As always with GNU make you may use <command>make -j4 check</command>
          to execute four processes in parallel performing the checks which
          should give a significant speed up.
        </note>
      </section>
    </section>
    <section xml:id="introduction.license">
      <title xml:id="introduction.license.title">License</title>
      <para>As long as no other license statement is given, DataDrivenSampler is
      free for user under the GNU Public License (GPL) Version 3 (see
      <uri>www.gnu.de/documents/gpl-3.0.de.html</uri>).</para>
    </section>
    <section xml:id="introduction.disclaimer">
      <title xml:id="introduction.disclaimer.title">Disclaimer</title>
      <para>We quote section 11 from the GPLv3 license:</para>
      <remark>Because the program is licensed free of charge, there is
not warranty for the program, to the extent permitted by applicable law.
Except when otherwise stated in writing in the copyright holders and/or
other parties provide the program &quot;as is&quot; without warranty of
any kind, either expressed or implied. Including, but not limited to,
the implied warranties of merchantability and fitness for a particular
purpose. The entire risk as to the quality and performance of the
program is with you. Should the program prove defective, you assume the
cost of all necessary servicing, repair, or correction.</remark>
    </section>
    <section xml:id="introduction.feedback">
      <title xml:id="introduction.feedback.title">Feedback</title>
      <para>If you encounter any bugs, errors, or would like to submit
      feature request, please use the email address provided at the very
      beginning of this user guide. The author is especially thankful for
      any description of all related events prior to occurrence of the
      error, saved &quot;session scripts&quot; (see below) and auxiliary files.
      Please mind sensible space restrictions of email attachments.</para>
    </section>
  </chapter>
  
  <chapter xml.id="quickstart">
    <title  xml.id="quickstart.title">Quickstart</title>
    <section xml.id="quickstart.introduction">
      <title  xml.id="quickstart.introduction.title">Sampling in neural networks</title>
      <para>Assume we are given a very simple data set as depicted in 
      <xref xrefstyle="template:Figure %n" linkend="quickstart.introduction.dataset"/>. 
      <figure xml:id="quickstart.introduction.dataset">
        <title>Dataset: two gaussian distributed point clouds</title>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="dataset_two_clusters"/>
          </imageobject>
        </mediaobject>
      </figure>
      The goal is to classify all red and blue dots into two different classes. This
      problem  is quite simple to solve: a line in the two-dimensional space can 
      easily separate the two classes.</para>
      <para>A very simple neural network, a perceptron, is all we need:  it uses 
      two inputs nodes, namely each coordinate component, 
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>x</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>
      and
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>x</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>,
      and a single output 
      node with an activation function
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
              <mml:mi>f</mml:mi>
          </mml:mrow>
        </mml:math>
      </inlineequation>
      whose sign gives the class the input item belongs to. The network is 
      given in 
      <xref xrefstyle="template:Figure %n" linkend="quickstart.introduction.perceptron"/>.
      </para>
    <figure xml:id="quickstart.introduction.perceptron">
        <title>Simple single-layer perceptron with weights and biases</title>
        <mediaobject>
          <imageobject>
            <imagedata width="30%" scalefit="1." entityref="simple_single_layer_perceptron"/>
          </imageobject>
        </mediaobject>
      </figure>
      <para>In the following we want to use the mean square loss, i.e. the 
      euclidian distance between the output from the network and the expected
      values per item, as the network's loss function. The loss depends 
      implicitly on the dataset and explicitly on the weights and biases 
      associated with the network. In our case, we have two weights for the two
      edges between input nodes, 
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>w</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>
      and
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
            <mml:msub>
              <mml:mi>w</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msub>
          </mml:mrow>
        </mml:math>
      </inlineequation>,
      and the output node and a single bias attached
      to the output node
      <inlineequation>
        <mml:math display="inline">
          <mml:mrow>
              <mml:mi>b</mml:mi>
          </mml:mrow>
        </mml:math>
      </inlineequation>.
      </para>
      <para>In sampling we look at a system of particles that have two internal
      properties: location and momentum. The location is simply their current 
      value, that changes through its momentum over time. The momentum again 
      changes because the particle is inside a potential that drives it towards
      the minimum. The system is described by a so-called Hamilton operator
      that gives rise to its Dynamics. If noise is additionally taken into 
      account, then we look at Langevin Dynamics.</para>
      <para>Returning to the neural networks, the role of the particles is taken
      by the degrees of freedom of the system, weights and biases. The loss
      function is called the <emphasis>potential</emphasis> and it is accompanied 
      by a <emphasis>kinetic energy</emphasis> that is simply the sum of all
      squared momenta. Adding Momentum to Optimizers in neural networks is a
      concept known already and inspired by physics. The momentum helps in
      overcoming areas of the loss function where it is essentially flat.</para>
      <para>Sampling produces trajectories of particles moving along the
      manifold. Integrals along these trajectories, if they are long enough, 
      are equivalent to integrating over the whole manifold, the system is
      ergodic.</para>
      <para>By using sampling we mean to discover more of the loss manifold
      than just the closest local minimum. To this end, barriers need to be
      overcome of which there two kind,</para>
      <itemizedlist>
        <listitem>entropic barriers,</listitem>
        <listitem>enthalpic barriers.</listitem>
      </itemizedlist>
      <para>Both of which are conceptually very simple. The enthalpic barrier
      is simply a ridge that is very high where the particles need a large 
      momentum to overcome it. Entropic barriers on the other hand are passages
      very small in volume that are simply very difficult to find. In order to
      overcome barriers of the first kind, higher temperatures suffice.
      For the second type of barrer, this is not so easy.</para>
      <para>This quick description of the problem of sampling in the context
      of neural networks in data science should have prepare you now for the
      following quickstart tutorial on how to actually use DataDrivenSampler 
      to perform sampling.</para>
    </section>
    <section xml:id="quickstart.python">
      <title  xml:id="quickstart.python.title">Using Python</title>
      <para>
      The package can be readily used inside any python3 shell or script.
      However, this interface rather lends itself to quick testing than rigorous
      experiments. You are still fine if you perform all your scientific 
      experiments inside python scripts kept safely inside a code versioning 
      system such as <command>git</command>.</para>
      <para>
      If you have installed the package in the folder "/foo", i.e. we have a 
      folder "DataDrivenSampler" with a file 
      <command>DataDrivenSampler.py</command> residing in there, then you 
      probably need to add it to the 
      <command>PYTHONPATH</command> as follows</para>
      <programlisting>PYTHONPATH=/foo python3</programlisting>
      <para>In this shell, you may import the sampling part of the package as 
      follows 
      <programlisting>from DataDrivenSampler.models.model import model</programlisting>
      This will import the abstract <command>model</command> class from the
      file mentioned before. This class contains wrapper functions to setup 
      the network with either training and sampling in a few keystrokes.</para>
      <section xml:id="quickstart.python.optimizing">
        <title  xml:id="quickstart.python.optimizing.title">Optimizing the network</title>
        <para>Let us first start with optimizing the network.</para>
        <example xml:id="quickstart.python.optimizing.example">
          <title>Optimizing network for two clusters dataset</title>
          <programlisting><xi:include  href="python/optimize.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>
        As you see, all options are set in a struct called <command>FLAGS</command> 
        that control how the optimization is performed. 
        There is a helper function in <command>model</command> called 
        <command>create_mock_flags</command>
        that creates the FLAGS for you with some default parameters. </para>
        <para>Let us quickly go through each of the parameters: 
        <itemizedlist>
          <listitem>
          <emphasis>batch_size</emphasis> sets the subset size of the data set
          looked at per training step, if smaller than dimension, then we add
          stochasticity/noise to the training but for the advantage of smaller
          runtime.
          </listitem><listitem>
          <emphasis>max_steps</emphasis> gives the amount of training steps to
          be performed.
          </listitem><listitem>
          <emphasis>optimizer</emphasis> defines the method to use for training.
          Here, we use Gradient Descent (in case batch_size is smaller than 
          dimension, then we actually have Stochastic Gradient Descent).
          </listitem><listitem>
          <emphasis>output_activation</emphasis> defines the activation function
          of all output nodes, here it is linear. Other choices are: tanh, relu, relu6.
          </listitem><listitem>
          <emphasis>seed</emphasis> sets the seed of the random number generator.
          We will still have full randomness but in a deterministic manner, i.e. 
          calling the same procedure again will bring up the exactly same values.
          </listitem><listitem>
          <emphasis>step_width</emphasis> defines the scaling of the gradients
          in each training step, i.e. the learning rate. Values too large may miss the
          minimum, values too small need longer to reach it.
          </listitem>
        </itemizedlist>
         </para>
         <para>We did not say anything about <emphasis>sampler</emphasis>
         as this is convered in the next section.</para>
        <para>Afterwards, the network is initialized, then we call <command>train()</command>
        which performs the training and returns runtime info and trajectory
        as a pandas DataFrame.</para>
        <para>At the end of this section on training, let us have a quick 
        glance at the decrease of the loss function over the steps by using
        <command>matplotlib</command>.</para>
        <example xml:id="quickstart.python.optimizing.example2">
          <title>Plotting the loss and other properties over steps</title>
          <programlisting><xi:include  href="python/plot_optimize.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
      <para>Go and have a look athe other columns. Or try to visualize the 
      change in the parameters (weights and biases) in the trajectories
      dataframe.</para>
      </section>
      <section xml:id="quickstart.python.sampling">
        <title  xml:id="quickstart.python.sampling.title">Sampling the network</title>
        <para>After optimization we may continue sampling the network.</para>
        <example xml:id="quickstart.python.sampling.example">
          <title>Plotting the loss and other properties over steps</title>
          <programlisting><xi:include  href="python/sample.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>Here, the <emphasis>sampler</emphasis> setting mentioned before
        becomes important as it states which sampling scheme to use. At the
        moment the following are available: sgld, 
        GeometricLangevinAlgorithm_1st, GeometricLangevinAlgorithm_2nd, where
        the firstrtefers to StochasticGradientLangevinDynamics
         [Welling, Teh, 2011]. GLA 2nd is currently recommended to use.</para>
        <para>We might also concatenate optimize and sample if in between the
        two we adjust FLAGS as follows:
        <programlisting>
...
FLAGS.max_steps = 10000
FLAGS.sampler = "GeometricLangevinAlgorithm_2ndOrder"
nn.reset_flags(FLAGS)
nn.init_network(None, setup="sample")
...
        </programlisting>
        The only thing we change in FLAGS is the number of steps and adding the
        sampler. However, as the model simply stores a copy of the FLAGS, in 
        order to update the FLAGS in the model class as well, we need to reset 
        it. Afterwards, we again  initialize the network which will add only the 
        sampling nodes and prepare output files differently.
        We have skipped the following steps that are equivalent to
        <xref xrefstyle="template:Example %n" linkend="quickstart.python.sampling.example"/>.
        Again, at the very end we obtain pandas DataFrame containing runtime 
        information and trajectory.
        </para>
      </section>
      <section xml:id="quickstart.python.analysis">
        <title  xml:id="quickstart.python.analysis.title">Analysing trajectories</title>
        <para>Analysis is so for constrained to parsing in run and trajectory
        files that you would write through optimization and sampling runs.</para>
        <para>To this end, specify <command>FLAGS.run_file</command> and
        <command>FLAGS.trajectory_file</command> with some valid file names.</para>
        <para>Subsequently, these may be easily parsed as follows, see also
         "DDSAnalyser.in".</para>
        <example xml:id="quickstart.python.analysis.example">
          <title>Plotting the averages of the parameters over steps</title>
          <programlisting><xi:include  href="python/sample.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <programlisting>
        </programlisting>
        <para>This would give a plot of the running average for each parameter
        in the trajectory file. In a similar, the run file can be loaded and its
        average quantities such as loss or kinetic energy be analysed and 
        plotted.</para>
      </section>
    </section>
    <section xml:id="quickstart.cmdline">
      <title  xml:id="quickstart.cmdline.title">Using command-line interface</title>
      All the tests use the command-line interface and for performing rigorous
      scientific experiments, we recommend using this interface as well. Here,
      it is to do parameter studies and have extensive runs using different
      seeds.
      <section xml:id="quickstart.cmdline.writing_dataset">
        <title  xml:id="quickstart.cmdline.writing_dataset.title">Creating the dataset</title>
        <para>As data is read from file, this file needs to be created beforehand.</para>
        <para>For a certain set of simple classification problems, namely those 
        that can be found in the tensorflow playground, we have added a
        <quote>DatasetWriter</quote> that spills out the dataset in CSV format.
        </para>
        <example xml:id="quickstart.cmdline.writing_dataset.example">
          <title>Creating the "two clusters" dataset as CSV file</title>
          <programlisting><xi:include  href="cmdline/write_dataset.sh"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>This will write 500 datums of the dataset type 2 ("two clusters")
        to a file <quote>testset-twoclusters.csv</quote> using all of the points as we have
        set the test/train ratio to 0. Note that we also perturb the points by
        0.1 relative noise.</para>
      </section>
      <section xml:id="quickstart.cmdline.parsing_dataset">
        <title  xml:id="quickstart.cmdline.parsing_dataset.title">Creating the dataset</title>
        <para>Similarly, for testing the dataset can be parsed using the same
        tensorflow machinery as is done for sampling and optimizing, using</para>
        <example xml:id="quickstart.cmdline.parsing_dataset.example">
          <title>Parsing dataset from CSV file</title>
          <programlisting><xi:include  href="cmdline/parse_dataset.sh"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>where the <emphasis>seed</emphasis> is used for shuffling the
        dataset.</para>
      </section>
      <section xml:id="quickstart.cmdline.optimizing">
        <title  xml:id="quickstart.cmdline.optimizing.title">Optimizing the network</title>
        <para>As weights (and biases) are usually uniformly random initialized and
        the potential may therefore start with large values, we first have to 
        optimize the network, using (Stochastic) Gradient Descent (GD).</para>
        <example xml:id="quickstart.cmdline.optimizing.example">
          <title>Optimizing the network with Gradient Descent on Two Clusters dataset</title>
          <programlisting><xi:include  href="cmdline/optimize.sh"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>This call will create an internal dataset called "two clusters" with 500 
        points using random number seed 426 (to generate the dataset in a 
        deterministic fashion) using 10% of noise. 
        It will then perform a (Stochastic) Gradient  Descent optimization of the 
        parameters of the network using a step width/learning rate of 0.01 and
        do this for 1000 steps after which it stops and writes the resulting neural
        network in a TensorFlow-specific format to a set of files, one of which
        is called <command>model.cpkt.meta</command> (and the other filenames
        are derived from this).</para>
        <para>In case you have read the quickstart tutorial on the Python
        interface before, then the names of the command-line option will
        probably remind you of the variables in the FLAGS structure.</para>
      </section>  
      <section xml:id="quickstart.cmdline.sampling">
        <title  xml:id="quickstart.cmdline.sampling.title">Sampling trajectories on the loss manifold</title>
        <para>We continue from this optimized or equilibrated state with sampling.
        It is called equilibrated as the network's parameter should now be close to
        a (local) minimum of the potential function and hence in equilibrium. This
        means that small changes to the parameters will result in gradients that 
        force it back into the minimum.</para>
        <para>Let us call the sampler.</para>
        <example xml:id="quickstart.cmdline.sampling.example">
          <title>Sampling the loss manifold using on Two Clusters dataset</title>
          <programlisting><xi:include  href="cmdline/sample.sh"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
        <para>This will cause the sampler to prepare the same dataset as before, as
        we still are using the same seed, data_type, dimension, and noise.
        Moreover, the sampler will load the neural network from the model, i.e.
        using the optimized parameters right from the start.
        Afterwards it will use the Geometric Langevin Algorithm (GLA) in 2nd order
        discetization using again step_width of 0.01 and running for 100,000 steps
        in total. The GLA is a descretized variant of Langevin Dynamics whose 
        accuracy scales with the inverse square of the step_width (hence, 2nd 
        order).</para>
        <para>After it is finished, it will create two files; a run file 
        <command>run.csv</command>containing run time information such as the 
        step, the potential, kinetic and total energy at each step, and a 
        trajectory file <command>trajectory.csv</command>with each parameter 
        of the neural network at each step. These two files we need in the 
        next stage.</para>
      </section>
      <section xml:id="quickstart.cmdline.analysing">
        <title  xml:id="quickstart.cmdline.analysing.title">Analysing trajectories</title>
        <para>Eventually, we now perform the diffusion map analysis on the obtained
        trajectories. The trajectory file written in the last step is simply a matrix
        of dimension (number of parameters) times (number of trajectory steps).
        The eigenvector to the largest (but one) eigenvalue will give the dominant 
        direction in which the trajectory is moving.</para>
        <note>The largest eigenvalue is usually unity and its eigenvector is 
        constant.</note>
        <para>The analysis can perform three different tasks:</para>
        <itemizedlist>
          <listitem>Calculating averages.</listitem>
          <listitem>Calculating the diffusion map's largest  eigenvalues and 
          eigenvectors.</listitem>
          <listitem>Calculating landmarks and level sets to obtain an approximation
          to the free energy.</listitem>
        </itemizedlist>
        <section xml:id="quickstart.cmdline.analysing.averages">
          <title  xml:id="quickstart.cmdline.analysing.averages.title">Averages</title>
          <para>Averages are calculated by specifying two options as follows:</para>
          <example xml:id="quickstart.cmdline.analysing.averages.example">
            <title>Calculating averages over a sampled trajectory</title>
            <programlisting><xi:include  href="cmdline/analyse_average.sh"  parse="text"  
        xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
            <programlisting>
            </programlisting>
          </example>
          <para>This will load both the run file <command>run.csv</command>
          and the trajectory file <command>trajectory.csv</command>and average
           over them using only every 10th data point 
           (<emphasis>every_nth</emphasis>) and also dropping the first steps 
           below 100 (<emphasis>drop_burnin</emphasis>). It will produce then
           ten averages (<emphasis>steps</emphasis>) for each of energies in the 
           run file and each of the parameters in the trajectories file (along 
           with the variance) from the first non-dropped step till one of the 
           ten end steps. These end steps are obtained by equidistantly 
           splitting up the whole step interval.</para>
           <para>Eventually, we have two output file. The averages over the run
           information such as total, kinetic, and potential energy in
           <command>average_run.csv</command>. Also, we have the averages
           over the degrees of freedom in 
           <command>average_trajectories.csv</command>.</para>
        </section>
        <section  xml:id="quickstart.cmdline.analysing.diffusion_map">
          <title  xml:id="quickstart.cmdline.analysing.diffusion_map.title">Diffusion map</title>
          <para>The eigenvalues and eigenvectors can be written as well to two 
          output files.</para>
          <example xml:id="quickstart.cmdline.analysing.diffusion_map.example">
            <title>Calculating diffusion maps on a sampled trajectory</title>
            <programlisting><xi:include  href="cmdline/analyse_diffmap.sh"  parse="text"  
        xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
            <programlisting>
            </programlisting>
          </example>
          <para>The files ending in <command>..values.csv</command> contains 
          the eigenvalues in two columns, the first is the eigenvalue index,
          the second is the eigenvalue.</para>
          <para>The other file ending in <command>..vectors.csv</command> is 
          simply a matrix of the eigenvector components in one direction and 
          the trajectory steps in the other. Additionally, it contains the 
          parameters at the steps and also the loss and the kernel matrix 
          entry.</para>
          <para>Note that again the all values up till step 100 are dropped 
          and only every 10th trajectory point is considered afterwards.</para>
          <para>There are two methods available. Here, we have used the
          simpler (and less accurate) (plain old) vanilla method. The other is
          called TMDMap.</para>
        </section>
        <section xml:id="quickstart.cmdline.analysing.free_energy">
          <title  xml:id="quickstart.cmdline.analysing.free_energy.title">Free energy</title>
          <para>Last but not least, the free energy is calculated.</para>
          <example xml:id="quickstart.cmdline.analysing.free_energy.example">
            <title>Calculating free energy over profiles  over diffusion map eigenvectors on a sampled trajectory</title>
            <programlisting><xi:include  href="cmdline/analyse_free_energy.sh"  parse="text"  
        xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
            <programlisting>
            </programlisting>
          </example>
          <para>This will extract landmark points from the trajectory. 
          Basically, the loss manifold is discretized using these landmarks
          where all configurations close to a landmark step are combined
          onto a so-called level-set, i.e. all these configurations have a
          similar loss function value. By knowing the number of configurations
          in each level set and knowing the level sets loss value, an 
          approximation of the free energy is computed.</para>
          <para>This is computed for every step of the trajectory and it is
          insightful to look at the free energy over the course of the
          trajectory represented by the first eigenvalue. If in this graph
          clear minima with maxima in between can be seen, then there
          are enthalpic barriers between two local minima. If on the other
          hand there are flat areas, then we found entropic barriers.</para>
          <para>Both these types of barriers obstruct trajectories and keep
          the optimization trapped in so-called meta-stable states. Each type
          of barrier requires a different type of remedy to overcome.</para>
        </section>
      </section>
    </section>
    <section xml:id="quickstart.conclusion">
      <title  xml:id="quickstart.conclusion.title">Conclusion</title>
      <para>This has been the very quick introduction into samping done
      on neural network's loss function manifolds. You have to take it
      from here.</para>
    </section>
  </chapter>
  <chapter xml:id="reference">
    <title xml:id="reference.title">The reference</title>
    <section xml:id="reference.concepts">
    <title xml:id="reference.concepts.title">General concepts</title>
    <para>Before we dive into the internals of this program suite, let us
    first introduce some general underlying concepts assuming that the
    reader is only roughly familiar with them. This is not meant as a 
    replacement for the study of more in-depth material but should rather
    be seen as a reminder of the terms and notation that will appear later 
    on.</para>
    <itemizedlist>
      <listitem>
        <emphasis>Dataset</emphasis><para>The dataset contains a fixed number
        of datums of input tuples and output tuples. They are typically 
        referred to as <quote>features</quote> and <quote>labels</quote> in
        the machine learning community. Basically, they are samples taken
        from the unknown function which we wish to approximate using the
        neural network. If the output tuples are binary in each component,
        the approximation problem is called a <quote>classification</quote>
        problem. Otherwise, it is a <quote>regression</quote> problem.</para>
      </listitem>
      <listitem>
        <emphasis>Neural network</emphasis><para>The neural network is a black-box
        representing a certain set of general functions that are efficient in 
        solving classification problems (among others). They are parametrized
        explicitly using weights and biases and implicitly through the topoloy
        of the network (connections of nodes residing in layers) and the
        activation functions used. Moreover, the loss function determines the
        best set of parameters for a given task. </para>
      </listitem>
      <listitem>
        <emphasis>Loss</emphasis><para>The loss function determines for a given
        (labelled) dataset what set of neural network's parameters are best.
        Note that there are losses that do not require labels though.
        Different losses result in different set of parameters. It is a
        high-dimensional manifold that we want to learn and capture using the 
        neural network. It implicitly depends on the given dataset and 
        explicitly on the parameters of the neural network, namely weights and 
        biases. Dual to the loss function is the network's output that 
        explicitly depends on the dataset's current datum (fed into the 
        network) and implicitly on the parameters.</para>
        <para>Most important to understand about the loss is that it is a
        <emphasis>non-convex</emphasis> function and therefore in general does 
        not just have a single minimum. This makes the task of finding a good 
        set of parameters that (globally) minimize the loss difficult as one 
        would have to find each and every minima in this high-dimensional 
        manifold and check whether it is actually the global one.</para>
      </listitem>
      <listitem>
        <emphasis>Momenta and kinetic energy</emphasis><para>Momenta is a concept
        taken over from physics where the parameters are considered as particles
        each in a one-dimensional space where the loss is a potential function 
        whose ( negative) gradient acts as a force onto the particle driving them
        down-hill (towards the local minimum). This force is integrated in a 
        classical Newton's mechanic style, i.e. Newton's equation of motion 
        is discretized with small time steps (similar to the learning rate in 
        Gradient Descent). This gives first rise to/velocity and second to 
        momenta, i.e. second order ordinary differential equation (ODE) 
        split up into a system of two one-dimensional ODEs. There are numerous
        stable time integrators, i.e. velocity Verlet/leapfrog, that are employed
        to propagate both particle position (i.e. the parameter value) and
        its momentum through time. Note that momentum and velocity are actually
        equivalent as usually the mass is set to unity.</para>
      </listitem>
      <listitem>
        <emphasis>Optimizers</emphasis><para>Optimizers are used to drive the
        parameters to the local minimum from a given (random) starting 
        position. GradientDescent (GD) is best known, but there are more
        elaborate Optimizers that use the concept of momentum as well. This
        helps in overcoming flat parts of the manifold where the gradient
        is effectively zero but momentum still drives the particles towards
        the minimum.</para>
      </listitem>
      <listitem>
        <emphasis>Samplers</emphasis><para>The goal of samplers is different than
        the goal of optimizers. Samplers aim at discovering a great deal of the
        manifold, not constraint to the local minimum. Usually, they are 
        started from the local minimum and drive the particles further and
        further out until new minima are found between which potential
        barriers had to be overcome.</para>
      </listitem>
    </itemizedlist>
  </section>
  <section xml:id="reference.neural_networks">
    <title xml:id="reference.neural_networks.title">Neural Networks</title>
    <para>A neural network (NN) is a tool used in the context of machine 
    learning.
    Formally, it is a graph with nodes and edges, where nodes represent
    (simple) functions. The edges represent scalar values by which the
    output of one node is scaled as input to another node.  The scalar value
    is called <emphasis>weight</emphasis> and each node also has a
    constant value, the <emphasis>bias</emphasis>, that does not depend
    on the input of other nodes.
    Nodes are organised in layers and nodes are (mostly) only connected 
    between adjacent layer. 
    Special are the very first layer with input nodes that simply accept 
    input from the user and the very last layer whose output is eventually 
    all that matters.</para>
    <para>Typically, a NN might be used for the task of classification:
    Data is fed into the network's input layer and its output layer has nodes
    equal to the number of classes to be distinguished. This can for example
    be used for image classification.</para>
    <para>The essential task at hand is to determine a good set of 
    parameters, i.e. values for the weights and biases, such that the task
    is performed best with respect to some measure.</para>
    </section>
    <section xml:id="reference.loss">
    <title xml:id="reference.loss.title">The loss function</title>
    <para>At the moment, there are two little utility programs that help in 
    evaluating the loss function given a certain dataset, namely the
    <quote>LossFunctionSampler</quote>. Let us give an example
    call right away.</para>
    <example xml:id="reference.loss.example.trajectory">
      <title>Using LossFunctionSampler on a given trajectory</title>
      <programlisting><xi:include  href="cmdline/lossfunctionsampler-trajectory.sh"  parse="text"  
  xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
      <programlisting>
      </programlisting>
    </example>
    <para>It takes as input the dataset file <quote>dataset-twoclusters.csv</quote>
    and either a trajectory file <quote>trajectory.csv</quote>. This will
    cause the program the re-evaluate the loss function at the trajectory
    points which should hopefully give the same values as already stored in the
    trajectory file itself.</para>
    <para>More interesting is the second case, where instead of giving
    a trajectory file, we sample the parameter space equidistantly as
    follows:</para>
    <example xml:id="reference.loss.example.grid">
      <title>Using LossFunctionSampler with an equidistant grid</title>
      <programlisting><xi:include  href="cmdline/lossfunctionsampler-grid.sh"  parse="text"  
  xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
      <programlisting>
      </programlisting>
    </example>
    <para>Here, sample for each weight in the interval [-5,5] at 11 points 
    (10 + endpoint), and similarly for the weights in the interval [-1,1] at
    5 points.</para>
    <note>For anything but trivial networks the computational cost quickly 
    becomes prohibitively large. However, you may use <quote>fix_parameter</quote>
    to lower the computational cost by choosing a certain subsets of weights
    and biases to sample.</note>
    <example xml:id="reference.loss.example.fix_parameter">
      <title>Using LossFunctionSampler with an equidistant grid and fixing parameters</title>
      <programlisting><xi:include  href="cmdline/lossfunctionsampler-fix_parameter.sh"  parse="text"  
  xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
      <programlisting>
      </programlisting>
    </example>
  </section>
  <section xml:id="reference.network">
    <title xml:id="reference.network.title">The learned function</title>
    <para>The second little utility programs does not evaluate the loss function
    itself but the unknown function learned by the neural network depending
    on the loss function, called the <quote>InputSpaceSampler</quote>. 
    In other words, it gives the classification result for data point sampled
    from an equidistant grid. Let us give an example call right away.</para>
    <example xml:id="reference.network.example">
      <title>Using InputSpaceSampler with an equidistant grid</title>
      <programlisting><xi:include  href="cmdline/inputspacesampler.sh"  parse="text"  
  xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
      <programlisting>
      </programlisting>
    </example>
    <para>Here, <quote>batch_data_files</quote> is an input file but it does
    not need to be present. (Sorry about that abuse of the parameter as
    usually <quote>batch_data_files</quote> is read-only. Here, it is 
    overwritten!). Namely, it is generated by the utility in that it
    equidistantly samples the input space, using the interval [-4,4] for each
    input dimension and 10+1 samples (points on -4 and 4 included). The
    trajectory file <quote>trajectory.csv</quote> now contains the values
    of the parameters (weights and biases) to use on which the learned
    function depends or by, in other words, by which it is parametrized. As
    the trajectory contains a whole flock of these, the <quote>steps</quote>
    parameter tells it which steps to use for evaluating each point on the
    equidistant input space grid, simply referring to rows in said file.</para>
    <note>For anything but trivial input spaces the computational cost quickly 
    becomes prohibitively large.</note>
  </section>
  <section xml:id="reference.exploring">
    <title xml:id="reference.exploring.title">Exploring the manifold</title>
    <para>still empty</para>
  </section>
  <section xml:id="reference.miscellaneous">
    <title xml:id="reference.miscellaneous.title">Miscellaneous</title>
    <section xml:id="reference.miscellaneous.parameter_freeze">
      <title>Freezing parameters</title>
      <para>Sometimes it might be desirable to freeze parameters during
      training or sampling. This can be done as follows:</para>
        <example xml:id="reference.miscellaneous.parameter_freeze.example">
          <title>Fix a parameter using the python interface</title>
          <programlisting><xi:include  href="python/fix_parameter.py"  parse="text"  
      xmlns:xi="http://www.w3.org/2001/XInclude"/></programlisting>
          <programlisting>
          </programlisting>
        </example>
      <para>Note that you need to initialize the network without adding training
      or sampling methods, i.e. <quote>setup</quote> is None. Then, we fix the
      parameter where we give its name in full tensorflow parlance. Afterwards,
      we may add sample or training nodes and start training/sampling.</para>
      <note>Single values cannot be frozen but only entire weight matrices or 
      bias vectors per layer at the moment.</note>
    </section>
    <para>still empty</para>
  </section>
  <section xml:id="reference.limitations">
    <title xml:id="reference.limitations.title">Current limitations</title>
    <para>We would like to list the current limitations of the implementation.
    Note that in general this code is heavily based on tensorflow, hence all its
    limitations also apply to us.</para>
    <itemizedlist>
      <listitem> 
        <para>At the moment only exactly two-dimensional inputs are supported.</para>
      </listitem>
    </itemizedlist>
  </section>
</chapter>
  <chapter>
    <title>Acknowledgements</title>
    <para>Thanks to all users of the code!</para>
  </chapter>
</book>
