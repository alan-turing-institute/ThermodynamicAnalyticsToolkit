{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the loss manifold\n",
    "\n",
    "Let us take a closer look at the network from the previous training.\n",
    "\n",
    "#### Using the Model class\n",
    "\n",
    "There are two changes: \n",
    "\n",
    "First, we will be using the `Model` class. Here, we use functionality that has not yet been streamlined into the `simulation` interface. This will give us the opportunity to glimpse at the more complex `Model` class and how to work with the neural networks using this interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TATi.model import Model as tati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we will fix the bias degree of freedom of the neural network. This makes it a *constant* parameter that is left unmodified during training or sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the `simulation` class so does the `Model` class rely on a set of options.\n",
    "\n",
    "First, we create the `options` instance of class `PythonOptions` which contains all the options that control the network and the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TATi.options.pythonoptions import PythonOptions\n",
    "\n",
    "options = PythonOptions()\n",
    "options.set_options(\n",
    "    batch_data_files=[\"dataset-twoclusters.csv\"],\n",
    "    fix_parameters=\"output/biases/Variable:0=-0.045677684\",\n",
    "    output_activation=\"linear\",\n",
    "    learning_rate=0.1,\n",
    "    loss=\"mean_squared\",\n",
    "    max_steps=1,\n",
    "    parse_parameters_file=\"training.csv\",\n",
    "    parse_steps=[100],\n",
    "    seed=426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass this `options` to the actual `Model` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tati(options)\n",
    "nn.init_input_pipeline()\n",
    "nn.init_network(None, setup=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *input pipeline* which feeds the dataset into the network actually must be created before the network. That's why the network was not created before when we did not specify a dataset.\n",
    "\n",
    "After having initialized the input pipeline using `init_input_pipeline()`, we need to initialize the network by calling `init_network()`.\n",
    "\n",
    "Let us check that the *network's parameters* have indeed been parsed from the training file. `Model` does not have a `parameters` data descriptior, but it has `weights` and `biases`. So, let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.weights, nn.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wonder: what does this mean?\n",
    "\n",
    "All weights and biases are actually nodes inside `tensorflow`'s computational graph. Moreover, there's one weight matrix and one bias vector per layer in the network. Inspecting nodes can only be done by evaluating them. This requires a `tensorflow.Session` object (a temporary workspace).\n",
    " \n",
    "The class `neuralnet_parameters` hides all this complexity and represents all weights together as a single vector. Similary, all biases are represented as a single vector.\n",
    "\n",
    "> Note that `weights` and `biases` are still lists: There is one per \"walker\". A walker is a single instantiation of the current network. There may be multiple copies of the graph, each with a different parameter set. See the option `number_walkers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.weights[0].evaluate(nn.sess), nn.biases[0].evaluate(nn.sess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, these are the correct values. Note that the list of biases is empty because the single bias is fixed.\n",
    "\n",
    "We needed to provide a `Session` object (from `Model.sess()`) to the `evaluate()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling with an equidistant grid\n",
    "\n",
    "Now, we would like to look at the region around the found minimum during our training run before.\n",
    "\n",
    "For this we will use an equidistant grid and evaluate the loss function at each grid point.\n",
    "\n",
    "We have ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.get_total_weight_dof())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... weights and ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.get_total_bias_dof())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... biases, i.e. 2 parameters in total. Hence, a two-dimensional grid which is easy to plot (that's why we fixed the bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate the loss on a grid we instantiate a specific `NaiveGridSampler`.\n",
    "\n",
    "We do not have to do this directly but there's a helper class called `SamplingModes` that will construct the instance for us (*factory pattern*) given some options.\n",
    "\n",
    "These options are \"non-standard\" options. However, the `Options` class can easily be extended by new option names using `add()` and `set()` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TATi.samplers.grid.samplingmodes import SamplingModes\n",
    "\n",
    "more_options = {\n",
    "    \"exclude_parameters\": [],\n",
    "    \"samples_weights\": 10,\n",
    "    \"samples_biases\": 1,\n",
    "    \"interval_weights\": [-1,1],\n",
    "    \"interval_biases\": [-1,1],\n",
    "}\n",
    "for k,v in more_options.items():\n",
    "    if not k in options:\n",
    "        options.add(k)\n",
    "    options.set(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use `create()` to instantiate our grid sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a \"naive_grid\" sampler class\n",
    "grid_sampler = SamplingModes.create(\"naive_grid\", nn, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grid_sampler` is prepared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to set max_steps to the number of grid points\n",
    "options.max_steps=grid_sampler.get_max_steps()\n",
    "nn.reset_parameters(options)\n",
    "\n",
    "# and re-create the input pipeline such that enough batches are present\n",
    "nn.init_input_pipeline()\n",
    "nn.reset_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option `max_steps` is important for the input pipeline as `tensorflow` will prepare as many batches as there are steps. As the number of steps depends on the sampler (and the options we pass), we query it for `get_max_steps()`. Next, we tell our network model `nn` about the change in options and reinitialize the input pipeline.\n",
    "\n",
    "Now, we use a for loop over the number of steps which equals the number of gridpoints. In the loop body we request the current grid point evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_grid(grid_sampler): \n",
    "    # set the sampler to the start of the grid\n",
    "    grid_sampler.goto_start()\n",
    "\n",
    "    # iterate over every grid point\n",
    "    coords = []\n",
    "    losses = []\n",
    "    for i in range(options.max_steps):\n",
    "        coords_eval = np.asarray(grid_sampler.set_step())\n",
    "        loss_eval, acc_eval = grid_sampler.evaluate_loss()\n",
    "        coords.append(coords_eval)\n",
    "        losses.append(loss_eval)\n",
    "        grid_sampler.goto_next_step()\n",
    "    \n",
    "    samples = np.zeros((len(coords),coords[0].shape[0]+1))\n",
    "    samples[:,:-1] = np.asarray(coords)\n",
    "    samples[:,-1:] = np.asarray(losses)\n",
    "    return np.asarray(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the loop is done, we construct a single numpy array as return value.\n",
    "\n",
    "Then, we call the above defined function `sample_grid()` to perform the grid evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_grid(grid_sampler)\n",
    "print(samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples[-12:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the sampled grid using `matplotlib` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "plt.scatter(samples[:,0], samples[:,1], c=samples[:,2], s=samples[:,2]*4, cmap=cm.coolwarm)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.text(0.14637233, 0.32722256, r'x')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save the sampled grid values for later use as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"loss-grid.csv\", samples, delimiter=\",\", header=\",\".join([\"x1\",\"x2\",\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- how to use and extend `PythonOptions`\n",
    "- how to generally use the `Model`\n",
    "- how to instantiate a specific `SamplingMode` for grid-based sampling (can also resample trajectories)\n",
    "- how to plot the loss the on the grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
