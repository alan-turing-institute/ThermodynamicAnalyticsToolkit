{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network\n",
    "\n",
    "Training in supervised learning is about finding a parameter set (weights and biases) for the neural network such that at best all labels of a labeled dataset are reproduced.\n",
    "\n",
    "Having the network set up and provided a dataset to use for *supervised learning*, we are now in the position to train our network.\n",
    "\n",
    "The training method is controlled by the option key `optimizer`. By default it's set to *GradientDescent*.\n",
    "\n",
    "Moreover, the following options control the training:\n",
    "\n",
    "- `batch_size` : if smaller than the dataset dimension, then we get Stochastic Gradient Descent (SGD)\n",
    "- `learning_rate` : learning rate to use for training, i.e. scaling the gradient per step\n",
    "- `loss` : type of training function to use\n",
    "- `max_steps` : fixed amount of training steps to execute\n",
    "- `seed` : influencing the random starting parameter choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TATi.simulation as tati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available loss functions\n",
    "\n",
    "Loss functions available in tensorflow are also available in `tati`. We can get a list as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tati.get_losses())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use simple *mean_squared* for the moment to train a network on the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn = tati(batch_data_files=[\"dataset-twoclusters.csv\"],\n",
    "          output_activation=\"linear\",\n",
    "          learning_rate=0.1,\n",
    "          loss=\"mean_squared\",\n",
    "          max_steps=100,\n",
    "          seed=426,\n",
    "          trajectory_file=\"training.csv\")\n",
    "training_data = nn.fit()\n",
    "print(nn.loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function returned a object `training_data`, we'll come to that in a moment.\n",
    "\n",
    "Let us first take a look at the minimum's parameters: two weights, one bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `training_data` object contains three dataframes: `run_info`, `trajectory`, and `averages`.\n",
    "\n",
    "- *run_info* contains information per step such as loss, accuracy, time spent, norm of gradient, ...\n",
    "- *trajectory* contains the parameter in each step\n",
    "- *averages* contains running averages for kinetic energy, for virials, for the ensemble loss, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run information\n",
    "\n",
    "First, we inspect how the training actually proceeded by looking at the loss value per step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "run = training_data.run_info[['step','loss']].values\n",
    "\n",
    "plt.plot(run[:,0], run[:,1])\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss has decreased well and is close to 0.\n",
    "\n",
    "We can also look at other values. For pandas `DataFrame`'s, `columns` contains the name of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.run_info.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Different optimizer and sampler methods produce different columns in *run_info* depending on their respective properties.\n",
    "\n",
    "Let us inspect the norm of the gradient, the *scaled_gradient*. It is scaled by the `learning_rate`.\n",
    "\n",
    "Generally, all columns in run info are scaled such that they are comparable among another, based on their effect on the parameters during the update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = training_data.run_info[['step','scaled_gradient']].values\n",
    "\n",
    "plt.semilogy(run[:,0], run[:,1])\n",
    "plt.xlabel(\"Training step\")\n",
    "plt.ylabel(\"Scale of gradient\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory\n",
    "\n",
    "Let us also look at the training trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.trajectory.columns)\n",
    "trajectory = training_data.trajectory[['weight0','weight1']].values\n",
    "\n",
    "plt.plot(trajectory[:,0], trajectory[:,1])\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"w2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict labels of unknown data\n",
    "\n",
    "Having trained a network, we would like to make predictions for new data that the network has not seen, yet.\n",
    "\n",
    "To this end, we use the `predict()` function which needs to be supplied with the unknown features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_data = np.array([[0,0], [1,1], [-1,1], [1,-1], [-1,-1]])\n",
    "print(np.sign(nn.predict(unknown_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a list of labels, one per item, in return. We have used `numpy.sign` to turn this into a lists with entries in {-1,1}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- how to train a network\n",
    "- how to visualize the run information and trajectory\n",
    "- how to predict using a pre-trained network on unknown data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
