{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at a tensorflow sampler implementation\n",
    "\n",
    "This will be probably be a bit too far out. However, the goal is more to show you what is possible rather than showing you how to fully implement with tensorflow. For that, we refer to the *programmer's guide* as a first step that comes with TATi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is very similar to training. And training or optimization is done in classes derived from `Optimizer` in Tensorflow. Hence, if we aim to implement a new sampler, then we need to override this class and add our own functionality.\n",
    "\n",
    "As a start, let us take a look what we have to do when overriding `GradientDescentOptimizer` in `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(tf.train.GradientDescentOptimizer):\n",
    "    def __init__(self, learning_rate, use_locking=False, name='GradientDescent'):\n",
    "        self._learning_rate = learning_rate\n",
    "        super(GradientDescent, self).__init__(learning_rate, use_locking, name)\n",
    "        \n",
    "    def _prepare(self):\n",
    "        self._learning_rate_t = ops.convert_to_tensor(self._learning_rate, name=\"learning_rate\")\n",
    "        super(GradientDescent, self)._prepare()\n",
    "        \n",
    "    def _apply_dense(self, grad, var):\n",
    "        lr_t = math_ops.cast(self._learning_rate_t, var.dtype.base_dtype)\n",
    "        scaled_gradient = lr_t * grad\n",
    "        var_update = state_ops.assign_sub(var, scaled_gradient)\n",
    "        return control_flow_ops.group(*[var_update])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we step on to the implementation of a sampler, here SGLD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientLangevinDynamicsSampler(tf.train.GradientDescentOptimizer)\n",
    "    def __init__(self, step_width, inverse_temperature,\n",
    "                 seed=None, use_locking=False, name='SGLD'):\n",
    "        super(StochasticGradientLangevinDynamicsSampler, self).__init__(use_locking, name)\n",
    "        self._step_width = step_width\n",
    "        self._seed = seed\n",
    "        self._inverse_temperature = inverse_temperature\n",
    "\n",
    "    def _prepare(self):\n",
    "        super(StochasticGradientLangevinDynamicsSampler, self)._prepare()\n",
    "        self._step_width_t = ops.convert_to_tensor(self._step_width, name=\"step_width\")\n",
    "        self._inverse_temperature_t = ops.convert_to_tensor(self._inverse_temperature, name=\"inverse_temperature\")\n",
    "\n",
    "        \n",
    "    def _prepare_dense(self, grad, var):\n",
    "        step_width_t = math_ops.cast(self._step_width_t, var.dtype.base_dtype)\n",
    "        inverse_temperature_t = math_ops.cast(self._inverse_temperature_t, var.dtype.base_dtype)\n",
    "        if self._seed is None:\n",
    "            random_noise_t = tf.random_normal(grad.get_shape(), mean=0.,stddev=1., dtype=dds_basetype)\n",
    "        else:\n",
    "            # increment such that we use different seed for each random tensor\n",
    "            self._seed += 1\n",
    "            random_noise_t = tf.random_normal(grad.get_shape(), mean=0., stddev=1., dtype=dds_basetype, seed=self._seed)\n",
    "        return step_width_t, inverse_temperature_t, random_noise_t\n",
    "\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        step_width_t, inverse_temperature_t, random_noise_t = self._prepare_dense(grad, var)\n",
    "\n",
    "        scaled_gradient = step_width_t * grad\n",
    "\n",
    "        scaled_noise = tf.sqrt(2.*step_width_t/inverse_temperature_t) * random_noise_t\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, scaled_gradient + scaled_noise)\n",
    "\n",
    "        return control_flow_ops.group(*[var_update])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
