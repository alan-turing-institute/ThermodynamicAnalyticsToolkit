{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the loss manifold\n",
    "\n",
    "Sampling on an equidistant grid may work for smaller networks. Subspace sampling can be used with one or two random directions as suggested by [Goodfellow, 2014] may be useful, too.\n",
    "\n",
    "However, the loss manifold is typically much larger with degrees of freedom easily in the range of $10^6$.\n",
    "\n",
    "The approach proposed by `tati` is to use dynamics-based sampling using Langevin dynamics in order to gain more insight. \n",
    "\n",
    "The general idea is to not only look at randomly selected local minima but at the more overall shape and statistical features of the loss manifold. Finally, we are not sampling the loss $L$ directly but the canonial Gibbs measure depending on the loss: $exp(-\\beta L)$.\n",
    "\n",
    "This said, sampling itself using `simulation` is quite similar to training.\n",
    "\n",
    "It depends principally on the following parameters:\n",
    "\n",
    "- `batch_size` : if the batch size is chosen smaller than the dataset dimension, then (additional) stochastic noise is added to the gradients.\n",
    "- `friction_constant` : friction value for Langevin-based samples\n",
    "- `hamiltonian_dynamics_time` : time passing between Metropolis-Hastings evaluation for HMC\n",
    "- `inverse_temperature` : inverse temperature value $\\beta$ giving an energy scale\n",
    "- `loss` : the loss function defining the loss manifold to sample from\n",
    "- `max_steps` : limits the number of points to sample\n",
    "- `sampler` : name of sampler function\n",
    "- `seed` : random number seed for added noise (Langevin) or randomly selected momenta (HMC)\n",
    "- `step_width` : this is like the `learning_rate` in training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TATi.simulation as tati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at sampling run using the BAOAB sampler for the dataset we have already looked at during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tati(batch_data_files=[\"dataset-twoclusters.csv\"],\n",
    "          friction_constant=1.,\n",
    "          inverse_temperature=10.,\n",
    "          loss=\"mean_squared\",\n",
    "          max_steps=100,\n",
    "          output_activation=\"linear\",\n",
    "          sampler=\"BAOAB\",\n",
    "          seed=426,\n",
    "          step_width=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start at the position of the minimum found during training. However, we will *not* use `parse_parameters_file` but instead set the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nn.parameters = np.array([0.14637233, 0.32722256, -0.045677684])\n",
    "print(nn.parameters, nn.loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `sample()` to initiate the sampling run of 100 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampling_data = nn.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get a instance of type `TrajectoryData` in return.\n",
    "\n",
    "Let us first look at the run information in `run_info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(sampling_data.run_info.columns)\n",
    "run = sampling_data.run_info[['step','loss']].values\n",
    "\n",
    "plt.plot(run[:,0], run[:,1])\n",
    "plt.xlabel(\"Sampling step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling properties\n",
    "\n",
    "With the above sampling, we can look at more properties though.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampling_data.averages.columns)\n",
    "run = sampling_data.averages[['step','average_inertia']].values\n",
    "\n",
    "plt.plot(run[:,0], run[:,1])\n",
    "plt.xlabel(\"Sampling step\")\n",
    "plt.ylabel(\"Average inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's look at the sampling trajectory.\n",
    "\n",
    "Moreover, let us load the sampled loss grid from before to get some \"background\" on where our trajectory is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_grid = np.loadtxt(\"loss-grid.csv\", delimiter=\",\", skiprows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "print(sampling_data.trajectory.columns)\n",
    "trajectory = sampling_data.trajectory[['weight0','weight1']].values\n",
    "\n",
    "plt.scatter(sampled_grid[:,0], sampled_grid[:,1], c=sampled_grid[:,2], s=sampled_grid[:,2]*4, cmap=cm.coolwarm)\n",
    "plt.plot(trajectory[:,0], trajectory[:,1], 'b')\n",
    "plt.xlabel(\"w1\")\n",
    "plt.ylabel(\"w2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller we pick `inverse_temperature`, the higher the walker in this loss valley climbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- import options affecting sampling\n",
    "- how to sample\n",
    "- how to inspect averages obtained from sampling\n",
    "- how to visualize sampling trajectories"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
