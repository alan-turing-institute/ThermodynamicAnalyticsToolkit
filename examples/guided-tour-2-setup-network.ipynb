{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a perceptrons as the neural network\n",
    "\n",
    "In this part we are going to set up a network and look in detail at its loss function, its activation function and how to control its topology.\n",
    "\n",
    "> At the moment TATi can only setup multi-layer perceptrons. We'll come to this in the very end.\n",
    "\n",
    "Let's start by importing TATis's `simulation` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TATi.simulation as tati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `simulation` module is an especially *light-weight*, yet powerful interface to TATi.\n",
    "\n",
    "> Although the class name is `Simulation` (inside a module `simulation`), we will refer to it as `tati` here.\n",
    "\n",
    "It relies on a `dict` of **options**. These options control every aspect of TATi: the network, the dataset, how and what files are written, ...\n",
    "\n",
    "A specific command `tati.help()` lists all available options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "averages_file:             CSV file name to write ensemble averages information such as average kinetic, potential, virial\n",
      "batch_data_file_type:      type of the files to read input from\n",
      "batch_data_files:          set of files to read input from\n",
      "batch_size:                The number of samples used to divide sample set into batches in one sampleing step.\n",
      "burn_in_steps:             number of initial steps to drop when computing averages\n",
      "collapse_walkers:          Whether to regularly collapse all dependent walkers to restart from a single position again, maintaining harmonicapproximation for ensemble preconditioning. 0 will not collapse.\n",
      "covariance_after_steps:    Number of steps after which to regularly recompute the covariance matrix. This will require communication between the walkers. 0 will never compute a covariance matrix.\n",
      "covariance_blending:       mixing for preconditioning matrix to gradient update, identity matrix plus this times the covariance matrix obtained from the other walkers, 0 - will never collapse\n",
      "diffusion_map_method:      name of method to use for diffusion map analysis: vanilla, TMDap, pydiffmap\n",
      "directions_file:           CSV file name to parse directions spanning subspace to project trajectories onto\n",
      "do_hessians:               whether to add hessian evaluation nodes to graph, used by optimzier and explorer\n",
      "dropout:                   Keep probability for sampleing dropout, e.g. 0.9\n",
      "every_nth:                 Store only every nth trajectory (and run) point to files, e.g. 10\n",
      "fix_parameters:            string formatted as 'name=value;...' with name of parameter fixed to value\n",
      "friction_constant:         friction to scale the influence of momenta\n",
      "hamiltonian_dynamics_time: time w.r.t step_width for HMC sampler passing between checking acceptance criterion\n",
      "hidden_activation:         Activation function to use for hidden layer: tanh, relu, linear\n",
      "hidden_dimension:          Dimension of each hidden layer, e.g. 8 8 for two hidden layers each with 8 nodes fully connected\n",
      "in_memory_pipeline:        whether to feed the dataset from file in-memory (True) or not (False)\n",
      "input_columns:             Pick a list of the following: (1) x1, (2) x2, (3) x1^2, (4) x2^2, (5) sin(x1), (6) sin(x2).\n",
      "input_dimension:           number of input nodes/number of features\n",
      "inter_ops_threads:         size of thread pool used for independent ops\n",
      "intra_ops_threads:         size of thread pool used for parallelizing an op\n",
      "inverse_temperature:       Inverse temperature that scales the gradients\n",
      "learning_rate:             learning rate used during optimization, see also `step_width`\n",
      "loss:                      Set the loss to be measured during sampling, e.g. mean_squared, log_loss, ...\n",
      "max_steps:                 Number of steps to run sampleer.\n",
      "number_of_eigenvalues:     \n",
      "number_walkers:            number of dependent walkers to activate ensemble preconditioning\n",
      "optimizer:                 Choose the optimizer to use for sampling: GradientDescent\n",
      "output_activation:         Activation function to use for output layer: tanh, relu, linear\n",
      "output_dimension:          number of output nodes/number of classes\n",
      "parse_parameters_file:     parse neural network parameters from this file on network creation\n",
      "parse_steps:               indicates the row (in column 'step') of the parse_parameters_file to use\n",
      "prior_factor:              factor for scaling prior force\n",
      "prior_lower_boundary:      lower boundary for wall-repelling prior force\n",
      "prior_power:               power of distance used in calculating force\n",
      "prior_upper_boundary:      upper boundary for wall-repelling prior force\n",
      "progress:                  activate progress bar to show remaining time\n",
      "restore_model:             Restore model (weights and biases) from a file.\n",
      "run_file:                  CSV run file name to runtime information such as output accuracy and loss values.\n",
      "sampler:                   Choose the sampler to use for sampling: BAOAB,CovarianceControlledAdaptiveLangevin, GeometricLangevinAlgorithm_1stOrder, GeometricLangevinAlgorithm_2ndOrder, HamiltonianMonteCarlo_1stOrder, HamiltonianMonteCarlo_2ndOrder, StochasticGradientLangevinDynamics\n",
      "save_model:                Save model (weights and biases) to a file for later restoring.\n",
      "seed:                      Seed to use for random number generators.\n",
      "sigma:                     Scale of noise injected to momentum per step for CCaDL.\n",
      "sigmaA:                    Scale of noise in convex combination for CCaDL.\n",
      "sql_db:                    Filename of sqlite3 database file to write iteration information to\n",
      "step_width:                step width \\Delta t to use, e.g. 0.01\n",
      "summaries_path:            path to write summaries (for TensorBoard) to\n",
      "trajectory_file:           CSV file name to output trajectories of sampling, i.e. weights and evaluated loss function.\n",
      "use_reweighting:           \n",
      "verbose:                   how much (debugging) information to print\n"
     ]
    }
   ],
   "source": [
    "tati.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not go through all of them, but let's at least take a closer look at one of them: **hidden_dimension**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option name: hidden_dimension\n",
      "Description: Dimension of each hidden layer, e.g. 8 8 for two hidden layers each with 8 nodes fully connected\n",
      "Type       : list of <class 'int'>\n",
      "Default    : []\n"
     ]
    }
   ],
   "source": [
    "tati.help(\"hidden_dimension\")   # mind that the option name needs to be a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have the option's name, a brief description, its type and the default value. Here, **hidden_dimension** has an empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-layer perceptron\n",
    "\n",
    "Perceptrons have the following properties:\n",
    "\n",
    "- input layer dimension\n",
    "- output layer dimension\n",
    "- number of hidden layers and their dimension\n",
    "- additional drop-out layers\n",
    "- activation function per node\n",
    "- loss function\n",
    "\n",
    "Each of these properties can be tuned with one of the options above.\n",
    "\n",
    "The whole network is set up by instantiating `tati` with a given set of options. We can simply pass the options whose default value we want to change, by giving them as *keyword arguments (kwargs)* to the constructor of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Option hidden_dimension needs to be list of type <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6c5aa85b43f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mhidden_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mhidden_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           loss=\"mean_squared\")\n\u001b[0m",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/simulation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m## Options dict controlling all options of the network, sampling, ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/options/pythonoptions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, add_keys, value_dict)\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvalue_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/options/pythonoptions.py\u001b[0m in \u001b[0;36mset_options\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \"\"\"\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/options/pythonoptions.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPythonOptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/options/options.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     raise ValueError(\"Option %s needs to be list of type %s\" \\\n\u001b[0;32m--> 170\u001b[0;31m                                      %  (key, str(self._list_type_map[key])))\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# option has no designated type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Option hidden_dimension needs to be list of type <class 'int'>"
     ]
    }
   ],
   "source": [
    "nn = tati(input_dimension=2,\n",
    "          output_dimension=1,\n",
    "          hidden_dimension=0,\n",
    "          hidden_activation=\"linear\", output_activation=\"relu\",\n",
    "          loss=\"mean_squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ups, we made a mistake! ... rats, what was again the type of **hidden_dimension**?\n",
    "\n",
    "Of course, we knew already that it needs to be a list of ints. Then, let's fix the above instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tati(input_dimension=2,\n",
    "          output_dimension=1,\n",
    "          hidden_dimension=[0],\n",
    "          hidden_activation=\"linear\", output_activation=\"relu\",\n",
    "          loss=\"mean_squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This single-layer perceptron should have three degrees of freedom, let's check using `num_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Neural network has not been constructed, dataset provided?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-41292b6ab176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/simulation.py\u001b[0m in \u001b[0;36mnum_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \"\"\"\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_weight_dof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_total_bias_dof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/simulation.py\u001b[0m in \u001b[0;36m_check_nn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Neural network has not been constructed, dataset provided?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Neural network has not been constructed, dataset provided?"
     ]
    }
   ],
   "source": [
    "print(nn.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dataset* is an essential part of the network. Its dimensions define type and number of input and output nodes. Therefore, the network is internally *constructed first when a dataset is provided*.\n",
    "\n",
    "Let us provide a dummy dataset through the option `dataset` and check again for the number of degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# mind that features, labels need to be lists of lists\n",
    "nn.dataset = [np.asarray([[0,0]], dtype=np.float32), np.asarray([[1]], dtype=np.int32)]  \n",
    "print(nn.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the correct results, two weights and a single bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the the options\n",
    "\n",
    "In case you are curious about the options inside `tati`, you can inspect its private member variable `_options`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'averages_file': None, 'batch_data_file_type': csv, 'batch_data_files': [], 'batch_size': 1, 'burn_in_steps': 0, 'collapse_walkers': False, 'covariance_after_steps': 100, 'covariance_blending': 0.0, 'diffusion_map_method': vanilla, 'dimension': 1, 'directions_file': None, 'do_hessians': False, 'dropout': None, 'every_nth': 1, 'fix_parameters': None, 'friction_constant': 0.0, 'hamiltonian_dynamics_time': 1.0, 'hidden_activation': linear, 'hidden_dimension': [0], 'in_memory_pipeline': True, 'input_columns': [], 'input_dimension': 2, 'inter_ops_threads': 1, 'intra_ops_threads': None, 'inverse_temperature': 1.0, 'learning_rate': 0.03, 'loss': mean_squared, 'max_steps': 1000, 'number_of_eigenvalues': 4, 'number_walkers': 1, 'optimizer': GradientDescent, 'output_activation': relu, 'output_dimension': 1, 'output_type': binary_classification, 'parse_parameters_file': None, 'parse_steps': [], 'prior_factor': 1.0, 'prior_lower_boundary': None, 'prior_power': 1.0, 'prior_upper_boundary': None, 'progress': False, 'restore_model': None, 'run_file': None, 'sampler': GeometricLangevinAlgorithm_1stOrder, 'save_model': None, 'seed': None, 'sigma': 1.0, 'sigmaA': 1.0, 'sql_db': None, 'step_width': 0.1, 'summaries_path': None, 'trajectory_file': None, 'use_reweighting': False, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "print(nn._options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *WARNING:* Do not change `nn._options[\"input_dimension\"]=3` directly, rather use `tati.set_options()`.\n",
    "\n",
    "This is because some options severely affect the network topology to the effect that the network is reinstantiated. `setup_options()` takes this into account ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Changing the network is not yet supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-886c4dbccca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace_Python/ThermodynamicAnalyticsToolkit/src/TATi/simulation.py\u001b[0m in \u001b[0;36mset_options\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"network\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maffected_parts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Changing the network is not yet supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"network\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maffected_parts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_nn_construction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Changing the network is not yet supported."
     ]
    }
   ],
   "source": [
    "nn.set_options(input_dimension=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and properly warns you in case the change is too severe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing degrees of freedom\n",
    "\n",
    "Consider the case where we only want a network with two weights and no bias. The bias is removed if we set it to zero. How can we fix the single bias to this value?\n",
    "\n",
    "> Fixing the bias is essentially changing the network, hence we need to add this parameter at the start. Let's reinstantiate `tati`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "nn = tati(input_dimension=2,\n",
    "          output_dimension=1,\n",
    "          fix_parameters=\"output/biases/Variable:0=0.\",\n",
    "          hidden_dimension=[0],\n",
    "          hidden_activation=\"linear\", output_activation=\"relu\",\n",
    "          loss=\"mean_squared\")\n",
    "\n",
    "# mind that features, labels need to be lists of lists\n",
    "nn.dataset = [np.asarray([[0,0]], dtype=np.float32), np.asarray([[1]], dtype=np.int32)]  \n",
    "print(nn.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the critical change! The bias degree of freedom has been effectively removed.\n",
    "\n",
    "The above string `output/biases/Variable:0=0.` needs some explanation. The string addresses a particular variable inside tensorflow, namely `Variable:0` in the name scopes `biases` and `output`. Moreover, we assign (\"=\") this variable the fixed value of *0.*. In case you want to fix a weight, replace `biases` by `weights`. In case it is the first hidden layer, user `layer1` in place of `output`. If the name cannot be found, you'll get a helpful error message. \n",
    "\n",
    "> There need to be as many values as the variables has components (comma-separated list). Moreover, it is not possible to fix single components. At the moment only all weights of a layer or all biases of a layer can be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "Let us return to `set_options()` and changing *hidden_dimension*.\n",
    "\n",
    "What do you do in case you really want a different network? You need to reinstantiate `tati` with the different set of options. This will automatically reset tensorflow's internal computational graph.\n",
    "\n",
    "> Therefore, you cannot have two instances of `tati` at the same time.\n",
    "\n",
    "Let's add two hidden layers, each with 8 nodes. Moreover, we want to use the *sigmoid* function for activation. Finally, we need to use the cross entropy function with softmax as loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tati(input_dimension=2,\n",
    "          output_dimension=1,\n",
    "          hidden_dimension=[8, 8],\n",
    "          hidden_activation=\"sigmoid\", output_activation=\"relu\",\n",
    "          loss=\"softmax_cross_entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the number of degrees of freedom - note this is less tedious once you see how to pass a dataset easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "# mind that features, labels need to be lists of lists\n",
    "nn.dataset = [np.asarray([[0,0]], dtype=np.float32), np.asarray([[1]], dtype=np.int32)]  \n",
    "print(nn.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us briefly check whether this is true: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "print(2*8+8*8+8*1+8+8+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems correct.\n",
    "\n",
    "This is all about the setting up of the network. Next we will be looking at specifying the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
