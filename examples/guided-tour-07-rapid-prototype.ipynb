{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapid-prototype your own sampler\n",
    "\n",
    "We will now implement a Geometric Langevin Algorithm 2nd order sampler.\n",
    "\n",
    "This will show you how to use the `simulation` interface as a light-weight interface that lends itself well to rapid prototyping for general sampling of loss manifolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TATi.simulation as tati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLA2 sampler consists of the following steps given the current state $(q_, p_n)$: \n",
    "\n",
    "- B: $p^{n+\\frac 1 2} = \\nabla L(q^n )  \\frac{\\Delta t}{2}$\n",
    "- A: $q^{n+\\frac 1 2} = q^n + M^{−1} p^{n+\\frac 1 2} \\Delta t$\n",
    "- B: $\\tilde{p}^{n+1} = p^{n\\frac 1 2} −\\nabla L(q^{n+\\frac 1 2}) \\frac{\\Delta t}{2}$\n",
    "- O: $p^{n+1} = \\alpha_{\\Delta t} \\tilde{p}^{n+1} + \\sqrt{ \\frac{1-\\alpha^2_{\\Delta t}}{\\beta} M} G_n$, where $\\alpha_{\\Delta t} = \\exp(−\\gamma \\Delta t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the actual GLA2 sampler in simple python code acting upon a given set of `parameters` and `momenta` using gradients that are obtained via a `gradients()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def gla2_update_step(nn, momenta, old_gradients, step_width, beta, gamma):\n",
    "    \"\"\"Implementation of GLA2 update step using TATi's simulation interface.\n",
    "    \n",
    "    Note:\n",
    "        Parameters are contained inside nn. For momenta we use\n",
    "        python variables as the evaluation of the loss does not\n",
    "        depend on them.\n",
    "\n",
    "    Args:\n",
    "      nn: ref to tati simulation instance\n",
    "      momenta: numpy array of parameters\n",
    "      old_gradients: gradients evaluated at last step\n",
    "      step_width: step width for sampling step\n",
    "      beta: inverse temperature\n",
    "      gamma: friction constant\n",
    "\n",
    "    Returns:\n",
    "      updated gradients and momenta\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. p_{n+\\tfrac 1 2} = p_n - \\tfrac {\\lambda}{2} \\nabla_x L(x_n)\n",
    "    momenta -= .5*step_width * old_gradients\n",
    "\n",
    "    # 2. x_{n+1} = x_n + \\lambda p_{n+\\tfrac 1 2}\n",
    "    nn.parameters = nn.parameters + step_width * momenta\n",
    "\n",
    "    # \\nabla_x L(x_{n+1})\n",
    "    gradients = nn.gradients()\n",
    "\n",
    "    # 3. \\widehat{p}_{n+1} = p_{n+\\tfrac 1 2} - \\tfrac {\\lambda}{2} \\nabla_x L(x_{n+1})\n",
    "    momenta -= .5*step_width * gradients\n",
    "\n",
    "    # 4. p_{n+1} = \\alpha \\widehat{p}_{n+1} + \\sqrt{\\frac{1-\\alpha^2}{\\beta}} \\cdot \\eta_n\n",
    "    alpha = math.exp(-gamma*step_width)\n",
    "    momenta = alpha * momenta + \\\n",
    "              math.sqrt((1.-math.pow(alpha,2.))/beta) * np.random.standard_normal(momenta.shape)\n",
    "\n",
    "    return gradients, momenta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function `gla2_update_step()` performs the integration steps for the GLA2.\n",
    "\n",
    "Next, we need to instantiate the interface, handing parameter and defining the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tati(\n",
    "    batch_data_files=[\"dataset-twoclusters.csv\"],\n",
    "    output_activation=\"linear\",\n",
    "    batch_size=5,\n",
    "    seed=426,\n",
    ")\n",
    "print(nn.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that we set a `batch_size` that is smaller than the dataset dimension. This is used to illustrate a point later on.\n",
    "\n",
    "Before the iteration loop, we define some parameters needed by the GLA2 sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 10.\n",
    "beta = 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we need temporary storage for the momentum and for the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momenta = np.zeros((nn.num_parameters()))\n",
    "old_gradients = np.zeros((nn.num_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the sampler's a source of random noise we will be using `numpy`'s random number generator. For reproducible runs we fix its seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set the neural network's `parameters` onto the minimum location found during training. Then, we proceedwith the actual sampling iteration that calls `gla2_update_step()` and prints the loss, parameters, and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.parameters = np.array([0.14637233, 0.32722256, -0.045677684])\n",
    "for i in range(10):\n",
    "    old_gradients, momenta = gla2_update_step(\n",
    "        nn, momenta, old_gradients, step_width=1e-1, beta=beta, gamma=gamma)\n",
    "    print(\"Step #\"+str(i)+\": \"+str(nn.loss())+\" at \" \\\n",
    "        +str(nn.parameters)+\", gradients \"+str(old_gradients))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step #0: 0.0137477545 at [0.14637233, 0.32722256, -0.045677684], gradients [0.174619   0.19510058 0.04459391]\n",
    "Step #1: 0.030512488 at [0.14448531, 0.32886487, -0.051827554], gradients [-0.16949219 -0.1935621  -0.05546231]\n",
    "Step #2: 0.016058164 at [0.14997849, 0.33427963, -0.050482124], gradients [0.26730776 0.2960053  0.04670344]\n",
    "Step #3: 0.02950866 at [0.14737183, 0.33205456, -0.051172715], gradients [-0.12346129 -0.14720283 -0.05923696]\n",
    "Step #4: 0.015005986 at [0.14809996, 0.33188266, -0.04882656], gradients [0.23016752 0.25591916 0.04580177]\n",
    "Step #5: 0.03061847 at [0.14211364, 0.3302522, -0.05122257], gradients [-0.17823109 -0.20106533 -0.0523799 ]\n",
    "Step #6: 0.013284249 at [0.13981462, 0.3320442, -0.052415553], gradients [0.15851466 0.17821857 0.03591432]\n",
    "Step #7: 0.03008079 at [0.14024836, 0.3344937, -0.050430734], gradients [-0.16158937 -0.1825893  -0.05113542]\n",
    "Step #8: 0.013428954 at [0.1379662, 0.33418858, -0.051118664], gradients [0.1621386  0.18661498 0.04102132]\n",
    "Step #9: 0.03442114 at [0.1323737, 0.32435444, -0.049534503], gradients [-0.30107805 -0.3218103  -0.03464577]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- how to implement a GLA2 sampler in python\n",
    "- how to use it directly with `Simulation`\n",
    "- how the next batch is triggered through a second evaluation of `gradients()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
