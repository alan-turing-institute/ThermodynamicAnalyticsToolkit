{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding a sampling loop\n",
    "\n",
    "Here, we want to write a sampling loop using the `Model` class. This will take us into the intrinsics of `tensorflow`, with its `Session` object that requires a `feed_dict` to evaluate nodes. Moreover, we will understand that `Model` is essentially just a bit init() function and a dictionary to get at the initiated nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/heber/packages/TATi-unstable/lib/python3.5/site-packages')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's go through this a bit quicker.\n",
    "\n",
    "#### Instantiate options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TATi.options.pythonoptions import PythonOptions\n",
    "\n",
    "options = PythonOptions()\n",
    "options.set_options(\n",
    "    batch_data_files=[\"dataset-twoclusters.csv\"],\n",
    "    fix_parameters=\"output/biases/Variable:0=-0.045677684\",\n",
    "    friction_constant=1.,\n",
    "    inverse_temperature=10.,\n",
    "    output_activation=\"linear\",\n",
    "    loss=\"mean_squared\",\n",
    "    max_steps=100,\n",
    "    parse_parameters_file=\"training.csv\",\n",
    "    parse_steps=[100],\n",
    "    sampler=\"BAOAB\",\n",
    "    seed=426,\n",
    "    step_width=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TATi.model import Model as tati\n",
    "nn = tati(options)\n",
    "nn.init_input_pipeline()\n",
    "nn.init_network(None, setup=\"sample\")\n",
    "\n",
    "# reset dataset to set its \"iterator\" to start\n",
    "nn.reset_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loop\n",
    "\n",
    "Before we create the loop, we need to fill a `feed_dict`. This is a dictionary with values for every `tensorflow.placeholder` node. As the name suggest, these are placeholders for values fed by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {\n",
    "    nn.state.nn[0].placeholder_nodes[\"friction_constant\"]: options.friction_constant,\n",
    "    nn.state.nn[0].placeholder_nodes[\"inverse_temperature\"]: options.inverse_temperature,\n",
    "    nn.state.nn[0].placeholder_nodes[\"step_width\"]: options.step_width,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we gather a list of nodes which we want to evaluate per step. Among them is `sample_step`, that triggers the a single update step as implemented by the *BAOAB* sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_nodes = [nn.state.nn[0].get(item) for item in [\"sample_step\", \"accuracy\", \"global_step\", \"loss\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we loop for `max_steps` steps: In the loop we evaluate a specific node,  For this, however, we need the feed_dict contain the batch's features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 0.021974392\n",
      "102 0.022016129\n",
      "103 0.022010544\n",
      "104 0.022782605\n",
      "105 0.025476754\n",
      "106 0.030133966\n",
      "107 0.033590313\n",
      "108 0.029148709\n",
      "109 0.022416748\n",
      "110 0.031302772\n",
      "111 0.06415214\n",
      "112 0.096364304\n",
      "113 0.113083005\n",
      "114 0.10122893\n",
      "115 0.074376866\n",
      "116 0.045133177\n",
      "117 0.025384735\n",
      "118 0.028010163\n",
      "119 0.045658525\n",
      "120 0.067148186\n",
      "121 0.08312476\n",
      "122 0.09350805\n",
      "123 0.095763855\n",
      "124 0.102452435\n",
      "125 0.09386392\n",
      "126 0.06980161\n",
      "127 0.056621633\n",
      "128 0.053400915\n",
      "129 0.05912881\n",
      "130 0.06566224\n",
      "131 0.05265801\n",
      "132 0.037870813\n",
      "133 0.03325355\n",
      "134 0.045062046\n",
      "135 0.04695704\n",
      "136 0.03966055\n",
      "137 0.03314541\n",
      "138 0.025108367\n",
      "139 0.025417764\n",
      "140 0.033234216\n",
      "141 0.039734192\n",
      "142 0.044298425\n",
      "143 0.06495814\n",
      "144 0.08764149\n",
      "145 0.090220496\n",
      "146 0.06244477\n",
      "147 0.02853256\n",
      "148 0.028489823\n",
      "149 0.06348887\n",
      "150 0.0916963\n",
      "151 0.07728517\n",
      "152 0.043520056\n",
      "153 0.02381428\n",
      "154 0.034122657\n",
      "155 0.09346721\n",
      "156 0.23239116\n",
      "157 0.34423405\n",
      "158 0.3769963\n",
      "159 0.3399719\n",
      "160 0.24748191\n",
      "161 0.19779679\n",
      "162 0.14594564\n",
      "163 0.07958278\n",
      "164 0.05171747\n",
      "165 0.1004404\n",
      "166 0.17538689\n",
      "167 0.20934375\n",
      "168 0.18344817\n",
      "169 0.11509363\n",
      "170 0.06772251\n",
      "171 0.052927904\n",
      "172 0.068773665\n",
      "173 0.10596206\n",
      "174 0.10016763\n",
      "175 0.06790397\n",
      "176 0.049002428\n",
      "177 0.059179626\n",
      "178 0.12569682\n",
      "179 0.2293745\n",
      "180 0.27206162\n",
      "181 0.21493173\n",
      "182 0.11432324\n",
      "183 0.038393803\n",
      "184 0.025899645\n",
      "185 0.06272057\n",
      "186 0.11477244\n",
      "187 0.15262207\n",
      "188 0.16241704\n",
      "189 0.14653924\n",
      "190 0.10349125\n",
      "191 0.058505934\n",
      "192 0.026441967\n",
      "193 0.029412482\n",
      "194 0.044571403\n",
      "195 0.057844926\n",
      "196 0.0791195\n",
      "197 0.09753545\n",
      "198 0.09078185\n",
      "199 0.06713253\n",
      "200 0.043118924\n"
     ]
    }
   ],
   "source": [
    "for i in range(options.max_steps):\n",
    "    # place the batch inside feed_dict\n",
    "    features, labels = nn.state.input_pipeline.next_batch(nn.sess)\n",
    "    feed_dict.update({\n",
    "        nn.state.xinput: features,\n",
    "        nn.state.true_labels: labels\n",
    "    })\n",
    "    \n",
    "    # perform the sampling step\n",
    "    _, acc, step, loss = nn.sess.run(step_nodes, feed_dict=feed_dict)\n",
    "    \n",
    "    # print loss\n",
    "    print(step, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- `Model` captures its state in the `ModelState`. In that state there is an instance of `NeuralNetwork`, one per walker.\n",
    "- sampling is triggered by evaluating a specific node `sample_step`.\n",
    "- evaluation of nodes requires a `Session` object and for each required `tensorflow.placeholder` an entry in a so-called `feed_dict`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
