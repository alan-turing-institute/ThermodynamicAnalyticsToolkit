{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rapid-prototype your own sampler\n",
    "\n",
    "We will now implement a Geometric Langevin Algorithm 2nd order sampler.\n",
    "\n",
    "This will show you how to use the `simulation` interface as a light-weight interface that lends itself well to rapid prototyping for general sampling of loss manifolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/heber/packages/TATi-unstable/lib/python3.5/site-packages')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GLA2 sampler consists of the following steps: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement the actual GLA2 sampler in simple python code acting upon a given set of `parameters` and `momenta` using gradients that are obtaiend via a `gradients()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def gla2_update_step(nn, momenta, old_gradients, step_width, beta, gamma):\n",
    "    \"\"\"Implementation of GLA2 update step using TATi's simulation interface.\n",
    "    \n",
    "    Note:\n",
    "        Parameters are contained inside nn. For momenta we use\n",
    "        python variables as the evaluation of the loss does not\n",
    "        depend on them.\n",
    "\n",
    "    Args:\n",
    "      nn: ref to tati simulation instance\n",
    "      momenta: numpy array of parameters\n",
    "      old_gradients: gradients evaluated at last step\n",
    "      step_width: step width for sampling step\n",
    "      beta: inverse temperature\n",
    "      gamma: friction constant\n",
    "\n",
    "    Returns:\n",
    "      updated gradients and momenta\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. p_{n+\\tfrac 1 2} = p_n - \\tfrac {\\lambda}{2} \\nabla_x L(x_n)\n",
    "    momenta -= .5*step_width * old_gradients\n",
    "\n",
    "    # 2. x_{n+1} = x_n + \\lambda p_{n+\\tfrac 1 2}\n",
    "    nn.parameters = nn.parameters + step_width * momenta\n",
    "\n",
    "    # \\nabla_x L(x_{n+1})\n",
    "    gradients = nn.gradients()\n",
    "\n",
    "    # 3. \\widehat{p}_{n+1} = p_{n+\\tfrac 1 2} - \\tfrac {\\lambda}{2} \\nabla_x L(x_{n+1})\n",
    "    momenta -= .5*step_width * gradients\n",
    "\n",
    "    # 4. p_{n+1} = \\alpha \\widehat{p}_{n+1} + \\sqrt{\\frac{1-\\alpha^2}{\\beta}} \\cdot \\eta_n\n",
    "    alpha = math.exp(-gamma*step_width)\n",
    "    momenta = alpha * momenta + \\\n",
    "              math.sqrt((1.-math.pow(alpha,2.))/beta) * np.random.standard_normal(momenta.shape)\n",
    "\n",
    "    return gradients, momenta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function `gla2_update_step()` performs the integration steps for the GLA2.\n",
    "\n",
    "Next, we need to instantiate the interface, handing parameter and defining the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import TATi.simulation as tati\n",
    "nn = tati(\n",
    "    batch_data_files=[\"dataset-twoclusters.csv\"],\n",
    "    batch_size=5,\n",
    "    seed=426,\n",
    ")\n",
    "print(nn.num_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that we set a `batch_size` that is smaller than the dataset dimension. This is used to illustrate a point later on.\n",
    "\n",
    "Before the iteration loop, we define some parameters needed by the GLA2 sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 10\n",
    "beta = 1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we need temporary storage for the momentum and for the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "momenta = np.zeros((nn.num_parameters()))\n",
    "old_gradients = np.zeros((nn.num_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the sampler's a source of random noise we will be using `numpy`'s random number generator. For reproducible runs we fix its seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(426)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set the neural network's `parameters` onto the minimum location found during training. Then, we proceedwith the actual sampling iteration that calls `gla2_update_step()` and prints the loss, parameters, and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #0: 0.055072468 at [0.14637233, 0.32722256, -0.045677684], gradients [-0.4977227  -0.5074096   0.01946941]\n",
      "Step #1: 0.05215037 at [0.14822862, 0.3330697, -0.051350962], gradients [-0.47487277 -0.48493218  0.01374783]\n",
      "Step #2: 0.046247073 at [0.15635721, 0.3414384, -0.05003026], gradients [-0.43204674 -0.441189    0.01249683]\n",
      "Step #3: 0.044368546 at [0.15870395, 0.34475836, -0.05025943], gradients [-0.4180484  -0.4269408   0.01165654]\n",
      "Step #4: 0.042139463 at [0.16248181, 0.34795862, -0.048004877], gradients [-0.40186507 -0.4101999   0.01238546]\n",
      "Step #5: 0.04139779 at [0.16116455, 0.35154322, -0.04999301], gradients [-0.39624512 -0.40452045  0.01101012]\n",
      "Step #6: 0.039960828 at [0.16130517, 0.35606495, -0.051266517], gradients [-0.38541412 -0.39347222  0.0098615 ]\n",
      "Step #7: 0.03677634 at [0.16559826, 0.36285028, -0.04894313], gradients [-0.3616403  -0.36890057  0.01036192]\n",
      "Step #8: 0.036284395 at [0.16536659, 0.3648386, -0.049754377], gradients [-0.35780987 -0.3650126   0.00977268]\n",
      "Step #9: 0.03844852 at [0.16335149, 0.3590464, -0.047834277], gradients [-0.3748716  -0.3822688   0.01170162]\n"
     ]
    }
   ],
   "source": [
    "nn.parameters = np.array([0.14637233, 0.32722256, -0.045677684])\n",
    "for i in range(10):\n",
    "    old_gradients, momenta = gla2_update_step(\n",
    "        nn, momenta, old_gradients, step_width=1e-1, beta=beta, gamma=gamma)\n",
    "    print(\"Step #\"+str(i)+\": \"+str(nn.loss())+\" at \" \\\n",
    "        +str(nn.parameters)+\", gradients \"+str(old_gradients))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--- Copy&paste space to illustrate change when using nn.gradients() in place of old_gradients ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
